<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LITREILY</title>
  <icon>https://www.gravatar.com/avatar/6ae20d989e9d976faf00ecc6d9bcfe82</icon>
  <subtitle>Stay Hungry, Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.litreily.top/"/>
  <updated>2018-10-16T10:53:21.905Z</updated>
  <id>http://www.litreily.top/</id>
  
  <author>
    <name>litreily</name>
    <email>707922098@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux指令 - tee的实现</title>
    <link href="http://www.litreily.top/2018/09/27/tee/"/>
    <id>http://www.litreily.top/2018/09/27/tee/</id>
    <published>2018-09-27T06:34:30.000Z</published>
    <updated>2018-10-16T10:53:21.905Z</updated>
    
    <content type="html"><![CDATA[<p>近日学习<a href="https://book.douban.com/subject/25809330/" target="_blank" rel="noopener">《Linux/UNIX 系统编程手册》</a>一书，巩固了C语言中一些常用库函数的使用，主要涉及基本IO操作相关的库函数。为了加深理解，手动实现了Linux系统中的<code>tee</code>指令的功能。借此文记述实现过程。</p><h2 id="tee功能简述"><a href="#tee功能简述" class="headerlink" title="tee功能简述"></a>tee功能简述</h2><p><code>tee</code>类似于一个单输入双输出的三通管道，将标准输入的数据输出到指定文件和标准输出中。为实现这个指令，主要考虑以下几点：</p><ol><li>解析<code>tee</code>包含的命令行参数</li><li>读取标准输入数据，并将数据写入标准输出和指定文件</li><li>若未指定文件，则仅将数据输出到标准输出</li></ol><p>下面逐步分析每个要点的实现方法。</p><h2 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h2><p><code>tee</code>包含以下可选项，本文实现仅考虑<code>-a</code>,<code>--version</code>,<code>--help</code></p><pre><code class="sh">Usage: tee [OPTION]... [FILE]...Copy standard input to each FILE, and also to standard output.  -a, --append              append to the given FILEs, do not overwrite  -i, --ignore-interrupts   ignore interrupt signals  -p                        diagnose errors writing to non pipes      --output-error[=MODE]   set behavior on write error.  See MODE below      --help     display this help and exit      --version  output version information and exit</code></pre><p>为解析命令行的<strong>可选项</strong>，需要用到库函数<code>getopt</code>或<code>getopt_long</code>，前者仅支持短格式，后者支持长短格式。下面对使用这两个函数解析参数的方法进行对比。</p><h3 id="getopt"><a href="#getopt" class="headerlink" title="getopt"></a>getopt</h3><p><code>getopt</code>函数声明及相关参数如下：</p><pre><code class="c">#include &lt;unistd.h&gt;int getopt(int argc, char * const argv[],            const char *optstring);extern char *optarg;extern int optind, opterr, optopt;</code></pre><ul><li><code>argc</code>: 与main函数的argc一致，代表参数个数</li><li><code>argv</code>: 与main函数的argv一致，代表参数值</li><li><code>optstring</code>: 可选项字符串，如<code>a:bc:d:</code>，参数后带冒号代表该选项需要给定参数值</li><li><code>optarg</code>: 存储可选项的参数值，如果不带参数则为NULL</li><li><code>optind</code>: 存储下一个可选参数的索引，每执行一次<code>getopt</code>就加1</li><li><code>opterr</code>: 错误提示标志，默认为1，当输入参数无效时，会给出提示</li><li><code>optopt</code>: 是对可选参数字符的一个备份，当输入的参数无效时可用</li></ul><p>下面使用<code>getopt</code>实现对选项<code>-a</code>的解析：</p><pre><code class="c">int main(int argc, char *argv[]){    int opt, fd = -1;    int flag_append = 0;    int flags = O_WRONLY | O_CREAT;    while((opt = getopt(argc, argv, &quot;a&quot;))!=-1) {        switch(opt){            case &#39;a&#39;:                flag_append = 1;                break;            case &#39;?&#39;:            default:                exit(EXIT_FAILURE);                break;        }    }    if(optind &lt; argc){        flags += flag_append ? O_APPEND:O_TRUNC;        fd = open(argv[optind], flags, S_IRUSR | S_IWUSR                    | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);        if(fd == -1){            printf(&quot;invaild file -- \&quot;%s\&quot;\n&quot;, argv[optind]);            exit(EXIT_FAILURE);        }    }    output(fd);    if(fd &gt; 0)        close(fd);    return 0;}</code></pre><p>以上代码结合<code>while</code>、 <code>switch</code>，使用<code>getopt</code>循环获取和分析可选项。当前只对一个可选项<code>a</code>进行了解析，当包含<code>-a</code>可选项时，flag_append设为1，之后根据该标志为文件的打开方式添加<code>O_APPEND</code>标志，后面读写数据时就会以附加的方式在文件尾部开始写入。</p><pre><code class="c">    if(optind &lt; argc){        flags += flag_append ? O_APPEND:O_TRUNC;        fd = open(argv[optind], flags, S_IRUSR | S_IWUSR                | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);        //...</code></pre><blockquote><p>说明：  </p><ol><li>如果不带<code>-a</code>，程序默认会以<code>O_TRUNC</code>方式打开文件，表示截断，会以重写的方式覆盖原文件  </li><li><code>optind</code>小于<code>argc</code>时，说明除了可选参数之外，用户还输入了其它参数，这里对应的是<code>tee</code>指令所需的文件名称。此时argv[optind]刚好对应第一个非可选参数。</li></ol></blockquote><h3 id="getopt-long"><a href="#getopt-long" class="headerlink" title="getopt_long"></a>getopt_long</h3><p><code>getopt_long</code>定义如下：</p><pre><code class="c">#include &lt;getopt.h&gt;int getopt_long(int argc, char * const argv[],            const char *optstring,            const struct option *longopts, int *longindex);</code></pre><p>与<code>getopt</code>相比，多了两个参数<code>longopts</code>, <code>longindex</code></p><ul><li><code>longopts</code>: 长格式可选项，<code>option</code>结构体指针</li></ul><pre><code class="c">struct option {    const char *name;       // 名称，如&quot;help&quot;    int         has_arg;    // 带参标志，0 或 1    int        *flag;       // 常设为NULL，若非NULL,将会把val值存入flag    int         val;        // getopt_long的返回值或将存入flag的值};</code></pre><p>当<code>flag</code>为NULL时，<code>getopt_long</code>返回<code>val</code>，否则返回0,并将<code>val</code>值存入<code>flag</code></p><ul><li><code>longindex</code>: 用于存储当前解析的长选项在<code>longopts</code>中的索引值(0,1,…)，通常设为<code>NULL</code></li></ul><p>下面使用<code>getopt_long</code>实现对选项<code>--help</code>，<code>--version</code>, <code>-a</code>的解析：</p><pre><code class="c">int main(int argc, char *argv[]){    int opt, fd = -1;    int flag_append = 0;    int flags = O_WRONLY | O_CREAT;    struct option opts[] = {        {&quot;append&quot;, 0, NULL, &#39;a&#39;},        {&quot;help&quot;, 0, NULL, &#39;h&#39;},        {&quot;version&quot;, 0, NULL, &#39;v&#39;}    };    while((opt = getopt_long(argc, argv, &quot;:av&quot;,opts, NULL))!=-1) {        switch(opt){            case &#39;a&#39;:                flag_append = 1;                break;            case &#39;h&#39;:                usage();                break;            case &#39;v&#39;:                printf(VERSION&quot;\n&quot;);                exit(EXIT_FAILURE);                break;            case &#39;?&#39;:                printf(&quot;tee: invaild option -- &#39;%c&#39;\n&quot;                    &quot;Try &#39;tee --help&#39; for more infomation.\n&quot;, optopt);            default:                exit(EXIT_FAILURE);                break;        }    }    if(optind &lt; argc){        flags += flag_append ? O_APPEND:O_TRUNC;        fd = open(argv[optind], flags, S_IRUSR | S_IWUSR            | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);        if(fd == -1){            printf(&quot;invaild file -- \&quot;%s\&quot;\n&quot;, argv[optind]);            exit(EXIT_FAILURE);        }    }    output(fd);    if(fd &gt; 0)        close(fd);    return 0;}</code></pre><p>以上代码中，长格式选项数组如下：</p><pre><code class="c">    struct option opts[] = {        {&quot;append&quot;, 0, NULL, &#39;a&#39;},        {&quot;help&quot;, 0, NULL, &#39;h&#39;},        {&quot;version&quot;, 0, NULL, &#39;v&#39;}    };</code></pre><p>可以看到，当用户分别输入<code>--append</code>,<code>--help</code>,<code>--version</code>时，<code>getopt_long</code>分别返回<code>a</code>，<code>h</code>，<code>v</code>三个字符。</p><p>但注意<code>while((opt = getopt_long(argc, argv, &quot;:av&quot;,opts, NULL))!=-1)</code>只包含了<code>av</code>两个参数，所以当用户输入<code>-h</code>时，程序会认为是无效参数，也就是说<code>--help</code>在这里只支持长格式，而其它两个支持长短两种方式。</p><p>这也是指令的某些选项仅支持长格式的实现方法之一了，其它方法可参考函数<code>getopt_long_only</code></p><blockquote><p>说明：<br><code>:av</code>最前面的冒号可以起到<code>opterr=0</code>的效果，就是在参数无效时不给出默认提示</p></blockquote><h3 id="usage"><a href="#usage" class="headerlink" title="usage"></a>usage</h3><p><a href="#getopt_long">getopt_long</a>提到的<code>--help</code>选项是绝大多数指令都会实现的，用于提供帮助信息，下面是<code>tee</code>的<code>--help</code>输出。</p><pre><code class="c">void usage(){    printf(&quot;Usage: tee [OPTION]... [FILE]\n&quot;        &quot;Copy standard input to each FILE, and also to standard output.\n\n&quot;        &quot;  -a, --append   append to the given FILEs, do not overwrite\n&quot;        &quot;  -v, --version  output version information and exit\n&quot;        &quot;      --help     display this help and exit\n&quot;    );    exit(EXIT_FAILURE);}</code></pre><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>关于参数解析，说到底就是<code>getopt</code>或<code>getopt_long</code>的应用。以上提到的可选参数中，其实也就<code>-a</code>会影响后续写入文件的方式，其它两个长格式选项均用于打印信息，之后便直接退出了。</p><p>而非可选项也只考虑了一个待写入文件的文件名，暂不考虑同时多文件写入。</p><h2 id="数据读写"><a href="#数据读写" class="headerlink" title="数据读写"></a>数据读写</h2><p><code>tee</code>数据读写很简单，仅需不断读取标准输入(stdin)数据，然后写入标准输出(stdout)和文件中，直到无数据可读或遇到中断信号为止。</p><pre><code class="c">#define BUF_SIZE 512void output(int fd){    int i = 0;    char buffer[BUF_SIZE] = {0};    char ch;    fflush(stdin);    fflush(stdout);    while(read(STDIN_FILENO, &amp;ch, 1) &gt; 0){ // read from stdin        buffer[i++] = ch;        if(ch = &#39;\n&#39; || i == BUF_SIZE){            write(STDOUT_FILENO, buffer, i); // output to stdout            if(fd &gt; 0) write(fd, buffer, i);            memset(buffer, 0, sizeof(buffer));            i = 0;        }    }}</code></pre><p><code>STDIN_FILENO</code>, <code>STDOUT_FILENO</code>分别对应标准输入和标准输出的文件描述符<code>0</code>, <code>1</code>。这两个加上标准错误输出<code>STDERR_FILENO</code>是所有应用程序默认打开的，所以无需手动<code>open</code>。</p><p>此外，输出函数<code>output</code>会判断传入的文件描述符是否有效，如果无效则不会写入文件，仅将数据输出至标准输出。</p><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><pre><code class="c">#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;fcntl.h&gt;#include &lt;string.h&gt;#include &lt;getopt.h&gt;#define BUF_SIZE 512#define VERSION &quot;litreily 1.0.0&quot;void output(int fd){    int i = 0;    char buffer[BUF_SIZE] = {0};    char ch;    fflush(stdin);    fflush(stdout);    while(read(STDIN_FILENO, &amp;ch, 1) &gt; 0){        buffer[i++] = ch;        if(ch = &#39;\n&#39; || i == BUF_SIZE){            write(STDOUT_FILENO, buffer, i); // output to stdout            if(fd &gt; 0) write(fd, buffer, i);            memset(buffer, 0, sizeof(buffer));            i = 0;        }    }}void usage(){    printf(&quot;Usage: tee [OPTION]... [FILE]\n&quot;        &quot;Copy standard input to each FILE, and also to standard output.\n\n&quot;        &quot;  -a, --append   append to the given FILEs, do not overwrite\n&quot;        &quot;  -v, --version  output version information and exit\n&quot;        &quot;      --help     display this help and exit\n&quot;    );    exit(EXIT_FAILURE);}int main(int argc, char *argv[]){    int opt, fd = -1;    int flag_append = 0;    int flags = O_WRONLY | O_CREAT;    struct option opts[] = {        {&quot;append&quot;, 0, NULL, &#39;a&#39;},        {&quot;help&quot;, 0, NULL, &#39;h&#39;},        {&quot;version&quot;, 0, NULL, &#39;v&#39;}    };    while((opt = getopt_long(argc, argv, &quot;:av&quot;,opts, NULL))!=-1) {        switch(opt){            case &#39;a&#39;:                flag_append = 1;                break;            case &#39;h&#39;:                usage();                break;            case &#39;v&#39;:                printf(VERSION&quot;\n&quot;);                exit(EXIT_FAILURE);                break;            case &#39;?&#39;:                printf(&quot;tee: invaild option -- &#39;%c&#39;\n&quot;                    &quot;Try &#39;tee --help&#39; for more infomation.\n&quot;, optopt);            default:                exit(EXIT_FAILURE);                break;        }    }    if(optind &lt; argc){        flags += flag_append ? O_APPEND:O_TRUNC;        fd = open(argv[optind], flags, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);        if(fd == -1){            printf(&quot;invaild file -- \&quot;%s\&quot;\n&quot;, argv[optind]);            exit(EXIT_FAILURE);        }    }    output(fd);    if(fd &gt; 0)        close(fd);    return 0;}</code></pre><h2 id="指令测试"><a href="#指令测试" class="headerlink" title="指令测试"></a>指令测试</h2><p>使用<code>gcc</code>完成编译，得到<code>tee</code>可执行文件</p><pre><code class="sh">gcc tee.c -o tee</code></pre><p>下面对指令进行测试：</p><pre><code class="sh">$ ./tee --helpUsage: tee [OPTION]... [FILE]Copy standard input to each FILE, and also to standard output.  -a, --append   append to the given FILEs, do not overwrite  -v, --version  output version information and exit      --help     display this help and exit$ ./tee --versionlitreily 1.0.0$ ./tee -htee: invaild option -- &#39;h&#39;Try &#39;tee --help&#39; for more infomation.$ ./tee -vlitreily 1.0.0$ ./tee -a -dtee: invaild option -- &#39;d&#39;Try &#39;tee --help&#39; for more infomation.$ ./tee test.txt112222$ cat test.txt122$ ./tee -a test.txt444444445555555555$ cat test.txt122444455555$ ls | ./tee test.txtmain.cMakefileREADME.mdteetee.ctest.txt$ cat test.txtmain.cMakefileREADME.mdteetee.ctest.txt</code></pre><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><a href="http://man.linuxde.net/tee" target="_blank" rel="noopener">tee命令</a></li><li><a href="http://blog.zhangjikai.com/2016/03/05/%E3%80%90C%E3%80%91%E8%A7%A3%E6%9E%90%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0--getopt%E5%92%8Cgetopt_long/" target="_blank" rel="noopener">解析命令行参数–getopt和getopt_long</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近日学习&lt;a href=&quot;https://book.douban.com/subject/25809330/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Linux/UNIX 系统编程手册》&lt;/a&gt;一书，巩固了C语言中一些常用库函数的使用，主要涉及基本
      
    
    </summary>
    
      <category term="嵌入式" scheme="http://www.litreily.top/categories/%E5%B5%8C%E5%85%A5%E5%BC%8F/"/>
    
    
      <category term="C/C++" scheme="http://www.litreily.top/tags/C-C/"/>
    
      <category term="linux" scheme="http://www.litreily.top/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>OpenWrt中使用gdb分析coredump</title>
    <link href="http://www.litreily.top/2018/09/20/coredump/"/>
    <id>http://www.litreily.top/2018/09/20/coredump/</id>
    <published>2018-09-20T10:32:00.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p>近日调试一个<code>bug</code>，一个守护进程在特定情况下执行一段程序后便会挂掉。为了分析<code>bug</code>产生原因，本人使用了printf, strace追踪，gdb调试等诸多调试工具和测试方法。本文对于在<code>OpenWrt</code>嵌入式系统中启用<code>gdb</code>功能及其使用方法进行详细说明。</p><h2 id="调试背景"><a href="#调试背景" class="headerlink" title="调试背景"></a>调试背景</h2><p>我最先通过<code>strace</code>工具追踪发现进程是在收到<code>SIGABRT</code>信号后被<code>kill</code>的。</p><pre><code class="c">pipe([8, 9])                            = 0fcntl64(8, F_GETFL)                     = 0 (flags O_RDONLY)ioctl(8, SNDCTL_TMR_TIMEBASE or TCGETS, 0xbe90a454) = -1 EINVAL (Invalid argument)rt_sigprocmask(SIG_UNBLOCK, [ABRT], NULL, 8) = 0tgkill(14189, 14189, SIGABRT)             = 0--- SIGABRT (Aborted) @ 0 (0) ---Process 14189 detached</code></pre><p>由以上信息可知，进程在执行某个管道<code>pipe</code>相关操作时被<code>kill</code>，通常是在执行<code>popen</code>函数会出现<code>pipe</code>调用。之后对源码<code>grep -rn popen</code>， 并结合<code>strace</code>打印的出错前的信息可以大致定位到可能出错的位置，然后通过添加<code>printf</code>打印<code>log</code>，根据重现时停止打印<code>log</code>的地方精确定位到源码出错位置。</p><pre><code class="C">// config.cchar * config_get(char *name) {    FILE *fp;    char cmd[128]={0};    snprintf(cmd, sizeof(cmd)-1, &quot;config get %s&quot;, name);    fp = popen(cmd, &quot;r&quot;);   // 出错位置    // ...}</code></pre><p>至此，仅仅能判断出用户态的出错位置，但从此处代码尚无法明确出错的根本原因，还需抓取内核态的出错信息，此时便需要使用<code>GDB</code>对进程出错时系统生成的<code>coredump</code>文件进行分析了。</p><h2 id="编译gdb以及带symbols的程序"><a href="#编译gdb以及带symbols的程序" class="headerlink" title="编译gdb以及带symbols的程序"></a>编译gdb以及带symbols的程序</h2><p>在分析之前，需要被调试进程的二进制文件包含<code>GDB</code>分析所需的<code>symbols</code>，什么是<code>symbols</code>? 粗略的讲，就是一张嵌入待调试进程的二进制文件中的映射表，包含代码中的变量、函数名、行号等信息。详见<a href="https://www.tutorialspoint.com/gnu_debugger/gdb_debugging_symbols.htm" target="_blank" rel="noopener">GDB-Debugging Symbols</a></p><h3 id="配置编译参数"><a href="#配置编译参数" class="headerlink" title="配置编译参数"></a>配置编译参数</h3><p><code>OpenWrt</code>编译参数存于<code>.config</code>文件中，<code>OpenWrt</code>默认并未打开<code>gdb</code>功能以及<code>debug</code>调试功能，我们可以通过<code>make menuconfig</code>选择参数或者手动更改配置文件。</p><pre><code class="sh"># .configCONFIG_DEBUG=y # 使能调试功能，启用后会给集成GDB调试所需的symbolsCONFIG_NO_STRIP=y # 禁用strip，防止程序代码被打乱#CONFIG_USE_SSTRIP=yCONFIG_TOOLCHAINOPTS=y # 使能交叉工具链可选功能，这是编译GDB功能的总开关</code></pre><p>配置完成后重新编译交叉工具链，用以得到<code>gdb</code>工具</p><pre><code class="sh">make toolchain/{compile,install} V=s</code></pre><h3 id="编译单个模块-package"><a href="#编译单个模块-package" class="headerlink" title="编译单个模块(package)"></a>编译单个模块(package)</h3><p>参考<code>OpenWrt</code>官方<a href="https://wiki.openwrt.org/doc/devel/gdb?s[]=gdb" target="_blank" rel="noopener">文档</a>，可以使用以下指令单独为一个模块添加<code>debug symbols</code></p><pre><code class="sh">make package/traffic_meter/{clean,compile,install} V=99 CONFIG_DEBUG=y</code></pre><h3 id="完整编译"><a href="#完整编译" class="headerlink" title="完整编译"></a>完整编译</h3><p>如果将全局<code>debug</code>开启，并进行完整编译，这会导致<code>image</code>过大（&gt;300M）而编译失败。当然啦，编译失败不要紧，因为只是没有生成<code>image</code>文件，但是所需模块和动态链接库都能正常编译完成，并不影响<code>coredump</code>文件的分析。</p><p>不过完整编译太过费时，不推荐，还是对需要调试的单个模块进行编译比较快捷和方便。</p><h2 id="获取coredump"><a href="#获取coredump" class="headerlink" title="获取coredump"></a>获取coredump</h2><p>得到了带有<code>symbols</code>的二进制文件，以及交叉编译得到的<code>gdb</code>调试工具，剩下的就是获取<code>coredump</code>文件</p><h3 id="配置coredump参数"><a href="#配置coredump参数" class="headerlink" title="配置coredump参数"></a>配置coredump参数</h3><pre><code class="sh">$ sudo vi /etc/profile# 在文件末尾添加以下指令，以取消对coredump文件大小的限制ulimit -c unlimited$ source /etc/profile# 设置coredump文件命名格式# e - process name; p - pid; t - time$ echo &quot;core-%e-%p-%t&quot; &gt; /proc/sys/kernel/core_pattern</code></pre><p>关于<code>coredump</code>文件格式的参数说明，可以参考<a href="http://man7.org/linux/man-pages/man5/core.5.html" target="_blank" rel="noopener">core dump file</a></p><h3 id="重现bug并获取coredump文件"><a href="#重现bug并获取coredump文件" class="headerlink" title="重现bug并获取coredump文件"></a>重现bug并获取coredump文件</h3><p>首先重现bug，然后找到<code>coredump</code>文件，并传至编译服务器</p><pre><code class="sh">$ cd /$ find . -name &quot;core-*&quot; |grep traffic_meter./sbin/core-traffic_meter-14189-2895$ cd sbin$ tftp -pl core-traffic_meter-14189-2895 192.168.1.10</code></pre><h2 id="GDB调试"><a href="#GDB调试" class="headerlink" title="GDB调试"></a>GDB调试</h2><h3 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h3><p>下面是常用的几个<code>gdb</code>指令</p><pre><code class="sh">(gdb) help(gdb) where(gdb) bt    # backtrace(gdb) list  # [l] 显示当前调试处的相关代码(gdb) up [num]  # 向上跳转1个或num个bt(gdb) down [num]    # 向下跳转1个或num个bt(gdb) print [variable]  # [p] 打印当前调试处相关变量的值</code></pre><h3 id="调试实例"><a href="#调试实例" class="headerlink" title="调试实例"></a>调试实例</h3><pre><code class="sh">$ cd repo.git$ cd build_dir/target-arm_v7=a_uClibc-0.9.33.2_eabi/root-ipq806x$ ../../toolchain-arm_v7-a_gcc-4.6-linaro_uClibc-0.9.33.2_eabi/gdb-linaro-7.2-2011.03-0/gdb/gdb sbin/traffic_meter ~/core-traffic_meter-14189-28959-2895GNU gdb (Linaro GDB) 7.2-2011.03-0Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;--host=x86_64-linux-gnu --target=arm-openwrt-linux-uclibcgnueabi&quot;.For bug reporting instructions, please see:&lt;http://bugs.launchpad.net/gdb-linaro/&gt;...Reading symbols from /home/litreily/R7500v2-Fortify.git/build_dir/target-arm_v7-a_uClibc-0.9.33.2_eabi/root-ipq806x/sbin/traffic_meter...done.warning: exec file is newer than core file.[New Thread 14189]Reading symbols from /home/litreily/R7500v2-Fortify.git/build_dir/target-arm_v7-a_uClibc-0.9.33.2_eabi/root-ipq806x/lib/libgcc_s.so.1...done.Loaded symbols for /home/litreily/R7500v2-Fortify.git/build_dir/target-arm_v7-a_uClibc-0.9.33.2_eabi/root-ipq806x/lib/libgcc_s.so.1Reading symbols from /home/litreily/R7500v2-Fortify.git/build_dir/target-arm_v7-a_uClibc-0.9.33.2_eabi/root-ipq806x/lib/libc.so.0...done.Loaded symbols for /home/litreily/R7500v2-Fortify.git/build_dir/target-arm_v7-a_uClibc-0.9.33.2_eabi/root-ipq806x/lib/libc.so.0Reading symbols from /home/litreily/R7500v2-Fortify.git/build_dir/target-arm_v7-a_uClibc-0.9.33.2_eabi/root-ipq806x/lib/ld-uClibc.so.0...done.Loaded symbols for /home/litreily/R7500v2-Fortify.git/build_dir/target-arm_v7-a_uClibc-0.9.33.2_eabi/root-ipq806x/lib/ld-uClibc.so.0Core was generated by `traffic_meter -w brwan -p ppp0 -m /dev/mtd15`.Program terminated with signal 6, Aborted.#0  0x402fb4fc in raise (sig=6) at libpthread/nptl/sysdeps/unix/sysv/linux/raise.c:6767        int res = INLINE_SYSCALL (tgkill, 3, pid, selftid, sig);(gdb) bt#0  0x402fb4fc in raise (sig=6) at libpthread/nptl/sysdeps/unix/sysv/linux/raise.c:67#1  0x402f579c in abort () at libc/stdlib/abort.c:89#2  0x402f5060 in __malloc_consolidate (av=0x4030b3e8) at libc/stdlib/malloc-standard/free.c:234#3  __malloc_consolidate (av=0x4030b3e8) at libc/stdlib/malloc-standard/free.c:170#4  0x402f4854 in malloc (bytes=&lt;value optimized out&gt;) at libc/stdlib/malloc-standard/malloc.c:908#5  0x402d6250 in _stdio_fopen (fname_or_mode=&lt;value optimized out&gt;, mode=&lt;value optimized out&gt;, stream=0x8ca0e8, filedes=8) at libc/stdio/_fopen.c:177#6  0x402d4fb4 in popen (command=0x8 &lt;Address 0x8 out of bounds&gt;, modes=0xfc08 &quot;r&quot;) at libc/stdio/popen.c:83#7  0x0000f488 in config_get (name=&lt;value optimized out&gt;) at config.c:11#8  0x0000ed48 in get_bogus_time_region (ct=60744, st=0xbe9f8994, btr=0xbe9f896c) at util.c:184#9  0x0000e8f0 in get_traffic_from_flash (tfm=0xbe9f8738, ct=2894) at spi_flash.c:912#10 0x0000ccb0 in restart_traffic_counter (tfm=0xbe9f8738, ct=52400) at trafficmeter.c:976#11 0x0000a1f4 in main (argc=&lt;value optimized out&gt;, argv=&lt;value optimized out&gt;) at trafficmeter.c:1798(gdb)</code></pre><blockquote><p><strong>BackTrace (bt) 输出</strong>  </p><p>#num memory_addr in function (arg1=val1, arg2=val2,…) at file.c:line<br><code>bt</code>输出前面的编号是进程执行时的压栈顺序，编号越小越底层。编号后面紧跟的是内存地址，从地址大小可以看出哪些是内核调用，哪些是用户调用。  </p><p><strong>注意：</strong>由于编译器优化缘故，某些变量会显示<code>value optimized out</code>，如果想获取真实值，需要在编译时添加<code>-O0</code>，用以禁用编译器优化</p></blockquote><p>从<code>bt</code>结果可以看出，进程是在执行动态内存分配函数<code>malloc</code>时检测到错误，并执行<code>abort</code>函数触发<code>SIGABRT</code>信号后退出的。那就可以确定是内存问题，多半是内存多次释放或是未释放导致的。</p><p>据此线索，检查代码中与内存分配和释放相关的部分，最终调试发现是某处代码引用指针错误，并在之后使用<code>free</code>释放内存，而该指针指向的内存在多处地方被重新分配和释放，导致内存出现不可预料的问题。</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>在分析<code>coredump</code>时，需要注意以下几点：</p><ol><li>交叉编译后的<code>GDB</code>可执行文件位于<code>build_dir/toolchain-arm_v7-a_gcc-4.6-linaro_uClibc-0.9.33.2_eabi/gdb-linaro-7.2-2011.03-0/gdb/gdb</code></li><li>注意当前调试路径最好是在编译完成后的根目录<code>root-ipq806x</code>, 否则<code>GDB</code>可能无法找到动态链接库的位置，从而无法找到库函数的<code>symbols</code>，此时可能出现以下情况</li></ol><pre><code class="sh">warning: exec file is newer than core file.[New Thread 14189]warning: Could not load shared library symbols for 3 libraries, e.g. /lib/libgcc_s.so.1.Use the &quot;info sharedlibrary&quot; command to see the complete listing.Do you need &quot;set solib-search-path&quot; or &quot;set sysroot&quot;?warning: Unable to find dynamic linker breakpoint function.GDB will be unable to debug shared library initializersand track explicitly loaded dynamic code.Core was generated by `traffic_meter -w brwan -p ppp0 -m /dev/mtd15`.Program terminated with signal 6, Aborted.#0  0x402fb4fc in ?? ()Setting up the environment for debugging gdb.Function &quot;internal_error&quot; not defined.Make breakpoint pending on future shared library load? (y or [n]) [answered N; input not from terminal]Function &quot;info_command&quot; not defined.Make breakpoint pending on future shared library load? (y or [n]) [answered N; input not from terminal].gdbinit:8: Error in sourced command file:Argument required (one or more breakpoint numbers).(gdb) info sharedlibraryFrom        To          Syms Read   Shared Object Library                        No          /lib/libgcc_s.so.1                        No          /lib/libc.so.0                        No          /lib/ld-uClibc.so.0(gdb) bt#0  0x402fb4fc in ?? ()#1  0x402f579c in ?? ()#2  0x402f579c in ?? ()Backtrace stopped: previous frame identical to this frame (corrupt stack?)</code></pre><p>若出现以上情况，我们得不到任何有效信息，此时可以通过提示的<code>set solib-search-path</code>或<code>set sysroot</code>手动设置库路径或根目录路径。但我仍建议在调试前<code>cd</code>到根目录。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><a href="https://blog.51cto.com/terrytong914/1905041" target="_blank" rel="noopener">Linux coredump解决流程</a></li><li><a href="https://wiki.openwrt.org/doc/devel/gdb?s[]=gdb" target="_blank" rel="noopener">GNU Debugger</a></li><li><a href="https://www.tutorialspoint.com/gnu_debugger/gdb_debugging_symbols.htm" target="_blank" rel="noopener">GDB - Debugging Symbols</a></li><li><a href="http://man7.org/linux/man-pages/man5/core.5.html" target="_blank" rel="noopener">Core dump file</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近日调试一个&lt;code&gt;bug&lt;/code&gt;，一个守护进程在特定情况下执行一段程序后便会挂掉。为了分析&lt;code&gt;bug&lt;/code&gt;产生原因，本人使用了printf, strace追踪，gdb调试等诸多调试工具和测试方法。本文对于在&lt;code&gt;OpenWrt&lt;/code&gt;
      
    
    </summary>
    
      <category term="嵌入式" scheme="http://www.litreily.top/categories/%E5%B5%8C%E5%85%A5%E5%BC%8F/"/>
    
    
      <category term="C/C++" scheme="http://www.litreily.top/tags/C-C/"/>
    
      <category term="linux" scheme="http://www.litreily.top/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>网页模板pug基本语法</title>
    <link href="http://www.litreily.top/2018/08/31/pug-synax/"/>
    <id>http://www.litreily.top/2018/08/31/pug-synax/</id>
    <published>2018-08-31T12:24:10.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Pug – robust, elegant, feature rich template engine for Node.js</p></blockquote><p><code>pug</code>原名<code>jade</code>,因版权问题更名为<code>pug</code>,即哈巴狗。与<code>hexo</code>默认模块<code>ejs</code>一样，<code>pug</code>也是一个模板引擎，可用于快速的网站开发，当然也可以用于静态博客网站的设计。本站点现时所用主题<code>manupassant</code>也使用了<code>pug</code>。</p><a id="more"></a><p>本文针对<code>Hexo</code>中使用<code>pug</code>的情况为例，说明其基本语法。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><pre><code class="sh"># common installnpm install pug# install for hexo blognpm install hexo-renderer-pug --save</code></pre><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p><code>pug</code>不同于<code>html</code>，前者不需要标签的开和闭，如<code>html</code>的<code>&lt;p&gt;Demo&lt;/p&gt;</code>，在<code>pug</code>使用<code>p Demo</code>即可。</p><h3 id="缩进"><a href="#缩进" class="headerlink" title="缩进"></a>缩进</h3><p><code>pug</code>对空格敏感，有点类似<code>python</code>对制表符<code>tab</code>敏感。<code>pug</code>使用空格作为缩进符，当然用soft tab也可行。同一级标签需保证左对齐。</p><pre><code class="pug">div    p Hello, world!    p Hello, pug.</code></pre><p>渲染结果如下：</p><pre><code class="html">&lt;div&gt;    &lt;p&gt;Hellow, world!&lt;/p&gt;    &lt;p&gt;Hello, pug.&lt;/p&gt;&lt;/div&gt;</code></pre><h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><p><code>pug</code>使用<code>//-</code>或<code>//</code>对代码进行注释，前者注释内容不出现在渲染后的<code>html</code>文件中，后者反之。</p><pre><code class="pug">//- html中不包含此行// html中会包含此行</code></pre><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p><code>pug</code>将标签属性存放于括号<code>()</code>内，多个属性之间以逗号或空格分隔。此外，对于标签的<code>id</code>和<code>class</code>，<code>pug</code>使用<code>#</code>紧跟标签<code>id</code>,使用<code>.</code>紧跟标签<code>class</code>，可以同时设置多个<code>class</code>。</p><pre><code class="pug">h1#title Test titleimg#name.class1.class2(src=&quot;/test.png&quot; alt=&quot;test&quot;)</code></pre><p>↓</p><pre><code class="html">&lt;h1 id=&quot;title&quot;&gt;Test title&lt;/h1&gt;&lt;img id=&quot;name&quot; class=&quot;class1 class2&quot; src=&quot;/test.png&quot; alt=&quot;test&quot;&gt;</code></pre><h3 id="包含"><a href="#包含" class="headerlink" title="包含"></a>包含</h3><p>为了方便代码复用，<code>pug</code>提供了<code>include</code>包含功能，以下代码会将<code>_partial</code>目录下的<code>head.pug</code>文件内容包含到当前调用的位置。有点<code>C/C++</code>中内联函数的意思。</p><pre><code class="pug">doctype htmlhtml(lang=&#39;en&#39;)    include _partial/head.pug</code></pre><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>下面是一个简单的<code>base</code>模板，通过<code>block</code>定义了页面头部<code>head</code>和内容<code>body</code>。块<code>block</code>有点类似<code>C/C++</code>的抽象函数，需要在继承者中完成定义，填充具体内容。</p><pre><code class="pug">//- base.pughtml    head        block title    body        block content</code></pre><p>以下文件使用<code>extends</code>继承以上模板，通过<code>block</code>覆盖或替换原有块<code>block</code>。当然，继承者也可以在原有基础上继续扩展。</p><pre><code class="pug">//- index.pugextends base.pugblock title    title &quot;Test title&quot;block content    h1 Hello world!    block article</code></pre><h3 id="定义变量"><a href="#定义变量" class="headerlink" title="定义变量"></a>定义变量</h3><p><code>pug</code>中通过<code>- var name = value</code>的形式定义变量</p><pre><code class="pug">- var intData = 100- var boolData = false- var stringData = &#39;Test&#39;p.int= intDatap.bool= boolDatap.stringData= stringData</code></pre><blockquote><p>需注意的是，在引用变量时，需要在引用位置加上<code>=</code>号，否则会默认将变量名当成普通字符串使用。</p></blockquote><p>如果想要将变量与其它字符串常量或是变量连接在一起，就不能用等号了，而是应该用<code>#{}</code>，该符号会对大括号内的变量进行求值和转义，最终得到渲染输出的内容。</p><pre><code class="pug">- var girl = &#39;Lily&#39;- var boy = &#39;Jack&#39;p #{girl} is so beautiful!p And #{boy} is handsome.</code></pre><h3 id="条件结构"><a href="#条件结构" class="headerlink" title="条件结构"></a>条件结构</h3><p><code>pug</code>的条件语句与其它语言类似，均是如下这般：</p><pre><code class="pug">- var A = {value: &#39;Test&#39;}- var B = trueif A.value    p= A.valueelse if B    p= Belse    p nothing</code></pre><h3 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h3><p><code>pug</code>中使用<code>each</code>和<code>while</code>实现循环迭代，<code>each</code>可以返回当前所在项的索引值，默认从0开始计数。</p><pre><code class="pug">//- eachol    each item in [&#39;Sun&#39;, &#39;Mon&#39;, &#39;Tus&#39;, &#39;Wen&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;]        li= item//- get index of each- var week = [&#39;Sun&#39;, &#39;Mon&#39;, &#39;Tus&#39;, &#39;Wen&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;]ol    each item, index in week        li= index + &#39;:&#39; + item</code></pre><p>↓</p><pre><code class="html">&lt;ol&gt;  &lt;li&gt;Sun&lt;/li&gt;  &lt;li&gt;Mon&lt;/li&gt;  &lt;li&gt;Tus&lt;/li&gt;  &lt;li&gt;Wen&lt;/li&gt;  &lt;li&gt;Thu&lt;/li&gt;  &lt;li&gt;Fri&lt;/li&gt;  &lt;li&gt;Sat&lt;/li&gt;&lt;/ol&gt;&lt;ol&gt;  &lt;li&gt;0:Sun&lt;/li&gt;  &lt;li&gt;1:Mon&lt;/li&gt;  &lt;li&gt;2:Tus&lt;/li&gt;  &lt;li&gt;3:Wen&lt;/li&gt;  &lt;li&gt;4:Thu&lt;/li&gt;  &lt;li&gt;5:Fri&lt;/li&gt;  &lt;li&gt;6:Sat&lt;/li&gt;&lt;/ol&gt;</code></pre><p><code>while</code>调用方式如下：</p><pre><code class="pug">//- while- var day = 1ul    while day &lt; 7        li= day++</code></pre><h3 id="Minix"><a href="#Minix" class="headerlink" title="Minix"></a>Minix</h3><p><code>mixin</code>名曰<strong>混入</strong>，类似其它编程语言中的<strong>函数</strong>，也是为了代码复用，可带参数或不带参数，定义方式如下：</p><pre><code class="pug">mixin menu-item(href, name)    li        span.dot ●        a(href=href)= name</code></pre><p>其中，<code>menu-item</code>为调用时所用名称，可认为是函数名，<code>href</code>及<code>name</code>是参数。同上<a href="#定义变量">定义变量</a>所说，<code>a(href=href)= name</code>中第二个<code>=</code>是为了将后面的<code>name</code>当作参数来处理，而不是当作字符串”name”来处理。</p><p>调用<code>mixin</code>定义的代码块，需通过<code>+</code>号紧跟<code>mixin</code>名称及参数:</p><pre><code class="pug">+menu-item(&#39;/Archives&#39;,&#39;Archives&#39;)+menu-item(&#39;/About&#39;,&#39;About&#39;)</code></pre><p><code>mixin</code>之所以称为混入，是因为其语法不局限于函数调用，在<code>mixin</code>可以使用块<code>block</code></p><pre><code class="pug">mixin print(post)    if block        block    else        p= post+print(&quot;no block&quot;)+print(&quot;&quot;)    div.box        p this is the content of block</code></pre><p>↓</p><pre><code class="html">&lt;p&gt;no block&lt;/p&gt;&lt;div class=&quot;box&quot;&gt;&lt;p&gt;this is the content of block&lt;/p&gt;&lt;/div&gt;</code></pre><h3 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h3><p>注意以下<code>pug</code>语句中第一行的<code>.</code>号。</p><pre><code class="pug">script(type=&#39;text/javascript&#39;).    var data = &quot;Test&quot;    var enable = true    if enable        console.log(data)    else        console.log(&#39;nothing&#39;)</code></pre><p>↓</p><pre><code class="js">&lt;script type=&#39;text/javascript&#39;&gt;    var data = &quot;Test&quot;    var enable = true    if enable        console.log(data)    else        console.log(&#39;nothing&#39;)&lt;/script&gt;</code></pre><p>对于简单脚本，使用<code>pug</code>尚可，复杂的还是单独写到<code>.js</code>文件中，然后通过<code>pug</code>引用方便一些，引用方式如下：</p><pre><code class="pug">script(type=&#39;text/javascript&#39;, src=&#39;/path/to/js&#39;)//- with hexo function url_forscript(type=&#39;text/javascript&#39;, src=url_for(theme.js) + &#39;/ready.js&#39;)</code></pre><h3 id="hexo-相关"><a href="#hexo-相关" class="headerlink" title="hexo 相关"></a>hexo 相关</h3><p>在<code>hexo</code>主题中使用<code>pug</code>时，可以通过使用<code>hexo</code>提供的全局变量<code>config</code>，<code>theme</code>来分别调用博客根目录下<code>_config.yml</code>文件中的参数以及主题根目录下<code>_config.yml</code>文件中的参数。</p><pre><code class="pug">//- blog configp= config.description//- theme configp= theme.title</code></pre><p>当然，<code>pug</code>中可以直接使用<code>hexo</code>提供的其它全局变量及辅助函数，使用方法详见<code>hexo</code>的<a href="https://hexo.io/zh-cn/docs/variables" target="_blank" rel="noopener">文档</a></p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><pre><code class="pug">//- head.pughead    meta(http-equiv=&#39;content-type&#39;, content=&#39;text/html; charset=utf-8&#39;)    meta(content=&#39;width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0&#39;, name=&#39;viewport&#39;)    meta(content=&#39;yes&#39;, name=&#39;apple-mobile-web-app-capable&#39;)    meta(content=&#39;black-translucent&#39;, name=&#39;apple-mobile-web-app-status-bar-style&#39;)    meta(content=&#39;telephone=no&#39;, name=&#39;format-detection&#39;)    meta(name=&#39;description&#39;, content=config.description)    block title    link(rel=&#39;stylesheet&#39;, type=&#39;text/css&#39;, href=url_for(theme.css) + &#39;/style.css&#39; + &#39;?v=&#39; + theme.version)    link(rel=&#39;Shortcut Icon&#39;, type=&#39;image/x-icon&#39;, href=url_for(&#39;favicon.png&#39;))    script(type=&#39;text/javascript&#39;, src=&#39;//cdn.bootcss.com/jquery/3.3.1/jquery.min.js&#39;)    block more</code></pre><pre><code class="pug">//- base.pugdoctype htmlhtml(lang=&#39;en&#39;)    include _partial/head.pug    block more        link(rel=&#39;stylesheet&#39;, type=&#39;text/css&#39;, href=url_for(theme.plugins) + &#39;/prettify/doxy.css&#39;)        script(type=&#39;text/javascript&#39;, src=url_for(theme.js) + &#39;/ready.js&#39; + &#39;?v=&#39; + theme.version, async)    //- body    body: #container.box        .h-wrapper            include _partial/nav-menu.pug        // article content        block content        include _partial/footer.pug</code></pre><p>其中:</p><ul><li><code>theme.*</code>为主题配置文件<code>_config.yml</code>中的参数</li><li><code>url_for</code>为<code>hexo</code>提供的用于查找资源路径的函数</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>pug</code>提供了包含，继承，Mixin等多种方式用于代码复用，语法简洁易懂，除了初学时需花费一些时间学习各种标点符号的含义外，其它倒也没有太大困难。</p><p>当然啦，<code>pug</code>还有许多其它特性，但就我目前使用情况而言，以上这些便已足够。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://pugjs.org/zh-cn/api/getting-started.html" target="_blank" rel="noopener">https://pugjs.org/zh-cn/api/getting-started.html</a></li><li><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Pug – robust, elegant, feature rich template engine for Node.js&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;pug&lt;/code&gt;原名&lt;code&gt;jade&lt;/code&gt;,因版权问题更名为&lt;code&gt;pug&lt;/code&gt;,即哈巴狗。与&lt;code&gt;hexo&lt;/code&gt;默认模块&lt;code&gt;ejs&lt;/code&gt;一样，&lt;code&gt;pug&lt;/code&gt;也是一个模板引擎，可用于快速的网站开发，当然也可以用于静态博客网站的设计。本站点现时所用主题&lt;code&gt;manupassant&lt;/code&gt;也使用了&lt;code&gt;pug&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Web" scheme="http://www.litreily.top/categories/Web/"/>
    
      <category term="Template" scheme="http://www.litreily.top/categories/Web/Template/"/>
    
    
      <category term="pug" scheme="http://www.litreily.top/tags/pug/"/>
    
  </entry>
  
  <entry>
    <title>Python实现快排及其可视化</title>
    <link href="http://www.litreily.top/2018/07/07/quick-sort/"/>
    <id>http://www.litreily.top/2018/07/07/quick-sort/</id>
    <published>2018-07-07T11:00:00.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p>最近装了个<code>Anaconda</code>，准备学习一下数据可视化。本着三天打鱼两天装死的心态，重新抱起崭新的<strong>算法</strong>书，认真学起了快排算法。学完后用<code>Python</code>实现了一遍基本的快排，然后使用<code>matplotlib</code>进行动态绘图，最后使用<code>imageio</code>生成GIF图片。谨以此文以记之！</p><h2 id="快排基本原理"><a href="#快排基本原理" class="headerlink" title="快排基本原理"></a>快排基本原理</h2><p><strong>快排</strong>采用和归并排序相同的分而治之的思想，将待排序数组分成左右两个子数组，对两部分子数组独立排序。当子数组均有序时，整个数组也就有序了。</p><p>排序步骤如下：</p><ol><li>将原始数组<code>data</code>随机打乱，以消除对输入的依赖（本步可选）</li><li>选择数组的首个元素<code>data[0]</code>作为切分元素<code>v</code></li><li>切分数组<ul><li>从左往右找到第一个大于切分元素<code>v</code>的元素<code>data[i]</code></li><li>从右到左找到第一个小于切分元素<code>v</code>的元素<code>data[j]</code></li><li>交换<code>data[i]</code>与<code>data[j]</code></li><li>重复以上三步直到<code>i&gt;=j</code></li><li>交换<code>data[j]</code>与切分元素<code>data[0]</code></li></ul></li><li>递归调用，对切分后的左侧子数组进行排序</li><li>递归调用，对切分后的右侧子数组进行排序</li></ol><p>文字性的描述总是那么苍白无力，但还好也能说明一些问题。可以看出，快排的关键在于<strong>切分</strong>，切分后的数组应该满足：</p><ol><li>切分元素的位置（设为<code>j</code>）已经固定</li><li><code>data[lo]</code>到<code>data[j-1]</code>区间内的元素均不大于切分元素<code>data[j]</code></li><li><code>data[j+1]</code>到<code>data[hi]</code>区间内的元素均不小于切分元素<code>data[j]</code></li></ol><p>其中<code>data[lo]</code>代表数组或子数组的首个元素，<code>data[hi]</code>代表数组或子数组的末尾元素。</p><p>简单点说，就是先找一个参考点，把小于这个参考点的元素都扔到它的左边，大于这个参考点的数都扔到它的右边。这样一来，参考点的位置就固定了，然后对左边的数据和右边的数据各自再递归的扔几遍，等所有子数组都扔完了，整个数组也就有序了。</p><p>不过需要注意的是，扔的时候不是随便扔，是把从左往右找到的第一个大于参考点的值和从右往左找到的第一个小于参考点的值进行替换。</p><h2 id="基本实现"><a href="#基本实现" class="headerlink" title="基本实现"></a>基本实现</h2><blockquote><p>Talk is cheap, show me the code</p></blockquote><pre><code class="python">def sort(data):    __sort(data, 0, len(data) - 1)def __sort(data, lo, hi):    if lo &gt;= hi:        return    key = __partition(data, lo, hi)    __sort(data, lo, key - 1)    __sort(data, key + 1, hi)def __swap(data, lo, hi):    data[lo], data[hi] = data[hi], data[lo]def __partition(data, lo, hi):    &#39;&#39;&#39;partition array&#39;&#39;&#39;    i = lo    j = hi    v = data[lo] # slicing element    while True:        # find one element that larger than v scan from left to right(→)        i += 1        while data[i] &lt; v:            if i == hi:                break            i += 1        # find one element that smaller than v scan from right to left(←)        while v &lt; data[j]:            if j == lo:                break            j -= 1        if i &gt;= j:            break        __swap(data, i, j)    __swap(data, lo, j)    return j</code></pre><p>以上便是参考<code>Algorithms</code>书上<code>java</code>代码的<code>Python</code>实现。下面是个使用示例：</p><pre><code class="python">import randomdef main():    data = [_ for _ in range(20)]    random.shuffle(data)    print(data)    sort(data)    print(data)if __name__ == &#39;__main__&#39;:    main()</code></pre><p>执行结果如下：</p><pre><code class="bash">$ python quick.py[4, 9, 1, 13, 18, 5, 6, 14, 2, 16, 7, 12, 15, 8, 11, 17, 0, 19, 10, 3][0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</code></pre><h2 id="快排优化"><a href="#快排优化" class="headerlink" title="快排优化"></a>快排优化</h2><p>快排有很多优化算法，目前我只习得一种最简单的，可以将切分函数两个内部<code>while</code>循环中的<code>if</code>语句去掉</p><pre><code class="python"># first oneif i == hi:    break# second oneif j == lo:    break</code></pre><p>这两个判断都是为了防止访问数组越界而设，其实第二个是完全没有必要加的，因为<code>lo</code>对应的就是切分元素本身，自己肯定不会小于自己，所以这个判断完全是多余的；对于第一个，想要去掉的话，只要保证数组最后一个元素最大即可，实现上只要在执行排序函数之前将最大值换至最后即可。</p><pre><code class="py">__swap(data, data.index(max(data)), len(data) - 1)</code></pre><p>本文后续的<code>code</code>会将第二个判断去掉，但第一个的还保留着，毕竟把最大值直接挪到最后总感觉怪怪的，在可视化的时候也会牺牲一点随机性。</p><h2 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h2><p>为了方便代码的阅读和管理,我将快排代码封装成<code>QuickSort</code>类,同时加入变量<code>swap_times</code>用于记录总的数据交换次数。</p><pre><code class="py">class QuickSort(object):    &#39;&#39;&#39;Quick sort algorithm&#39;&#39;&#39;    def sort(self, data):        self.swap_times = 0        # set the largest element to the end        # self.__swap(data, data.index(max(data)), len(data) - 1)        self.__sort(data, 0, len(data) - 1)        return self.swap_times    def __swap(self, data, lo, hi):        data[lo], data[hi] = data[hi], data[lo]        self.swap_times += 1    def __sort(self, data, lo, hi):        if lo &gt;= hi:            return        key = self.__partition(data, lo, hi)        self.__sort(data, lo, key - 1)        self.__sort(data, key + 1, hi)    def __partition(self, data, lo, hi):        &#39;&#39;&#39;partition array&#39;&#39;&#39;        i = lo        j = hi        v = data[lo] # slicing element        while True:            # find one element that larger than v scan from left to right(→)            i += 1            while data[i] &lt; v:                # below judge can dropped if the end element is the largest                if i == hi:                    break                i += 1            # find one element that smaller than v scan from right to left(←)            while v &lt; data[j]:                j -= 1            if i &gt;= j:                break            self.__swap(data, i, j)        self.__swap(data, lo, j)        return j</code></pre><h2 id="打印数据交换记录"><a href="#打印数据交换记录" class="headerlink" title="打印数据交换记录"></a>打印数据交换记录</h2><p>为了了解排序过程中数据交换,可以在<code>__swap</code>函数中打印每一次交换后的数组。</p><pre><code class="py">def __init__(self, debug=False, save_fig=False):    self.debug = debugdef __swap(self, data, lo, hi):    data[lo], data[hi] = data[hi], data[lo]    self.swap_times += 1    if self.debug:        print(&#39;{0} swap({1}, {2})&#39;.format(data, lo, hi))</code></pre><p>示例：</p><pre><code class="py">#main.pydef main():    data = []    random.seed(time.time())    data = [_ for _ in range(20)]    random.shuffle(data)    qs = QuickSort(debug=True)    swap_times, = qs.sort(data)if __name__ == &#39;__main__&#39;:    main()</code></pre><pre><code class="sh">➜  algorithm git:(master) ✗ ./main.py[14, 3, 11, 10, 4, 1, 2, 12, 18, 17, 7, 8, 13, 15, 0, 9, 16, 6, 5, 19] swap(4, 19)[14, 3, 11, 10, 4, 1, 2, 12, 5, 17, 7, 8, 13, 15, 0, 9, 16, 6, 18, 19] swap(8, 18)[14, 3, 11, 10, 4, 1, 2, 12, 5, 6, 7, 8, 13, 15, 0, 9, 16, 17, 18, 19] swap(9, 17)[14, 3, 11, 10, 4, 1, 2, 12, 5, 6, 7, 8, 13, 9, 0, 15, 16, 17, 18, 19] swap(13, 15)[0, 3, 11, 10, 4, 1, 2, 12, 5, 6, 7, 8, 13, 9, 14, 15, 16, 17, 18, 19] swap(0, 14)[0, 3, 11, 10, 4, 1, 2, 12, 5, 6, 7, 8, 13, 9, 14, 15, 16, 17, 18, 19] swap(0, 0)[0, 3, 2, 10, 4, 1, 11, 12, 5, 6, 7, 8, 13, 9, 14, 15, 16, 17, 18, 19] swap(2, 6)[0, 3, 2, 1, 4, 10, 11, 12, 5, 6, 7, 8, 13, 9, 14, 15, 16, 17, 18, 19] swap(3, 5)[0, 1, 2, 3, 4, 10, 11, 12, 5, 6, 7, 8, 13, 9, 14, 15, 16, 17, 18, 19] swap(1, 3)[0, 1, 2, 3, 4, 10, 11, 12, 5, 6, 7, 8, 13, 9, 14, 15, 16, 17, 18, 19] swap(1, 1)[0, 1, 2, 3, 4, 10, 11, 12, 5, 6, 7, 8, 13, 9, 14, 15, 16, 17, 18, 19] swap(4, 4)[0, 1, 2, 3, 4, 10, 9, 12, 5, 6, 7, 8, 13, 11, 14, 15, 16, 17, 18, 19] swap(6, 13)[0, 1, 2, 3, 4, 10, 9, 8, 5, 6, 7, 12, 13, 11, 14, 15, 16, 17, 18, 19] swap(7, 11)[0, 1, 2, 3, 4, 7, 9, 8, 5, 6, 10, 12, 13, 11, 14, 15, 16, 17, 18, 19] swap(5, 10)[0, 1, 2, 3, 4, 7, 6, 8, 5, 9, 10, 12, 13, 11, 14, 15, 16, 17, 18, 19] swap(6, 9)[0, 1, 2, 3, 4, 7, 6, 5, 8, 9, 10, 12, 13, 11, 14, 15, 16, 17, 18, 19] swap(7, 8)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 11, 14, 15, 16, 17, 18, 19] swap(5, 7)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 11, 14, 15, 16, 17, 18, 19] swap(5, 5)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 11, 14, 15, 16, 17, 18, 19] swap(8, 8)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 11, 13, 14, 15, 16, 17, 18, 19] swap(12, 13)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] swap(11, 12)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] swap(15, 15)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] swap(16, 16)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] swap(17, 17)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] swap(18, 18)</code></pre><p>根据打印结果可以逐步分析快排算法的执行过程，明确知晓每一次交换的数据。</p><h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>打印log固然可以分析算法的执行流程，但是不够直观，所以想着用可视化工具进一步处理，以动态图片形式显示快排过程。为此，只需在交换数据的函数<code>__swap</code>中使用<code>matplotlib</code>的柱状图绘制交换完成后的数组即可。</p><pre><code class="python">import matplotlib.pyplot as plt...class QuickSort(object):    def __init__(self, debug=False, save_fig=False):        ...        self.save_fig = save_fig        self.fig, self.ax = plt.subplots()        # open interactive mode of matplot        plt.ion()        if self.save_fig:            self.path = &#39;./images/{0}&#39;.format(time.strftime(&#39;%Y%m%d_%H%M%S&#39;))            os.makedirs(self.path)    def __swap(self, data, lo, hi):        ...        self.__plot_figure(data, lo, hi, show_swap=True)    def __plot_figure(self, data, lo=0, hi=0, show_swap=False):        &#39;&#39;&#39;plot and save figure&#39;&#39;&#39;        self.ax.clear()        self.ax.set_title(&#39;data quicksort&#39;)        self.ax.bar(range(len(data)), data, label=&#39;data&#39;)        if show_swap:            self.ax.bar([lo, hi], [data[lo], data[hi]], color=&#39;red&#39;, label=&#39;swap&#39;)        plt.legend()        plt.pause(0.001)        if self.save_fig:            plt.savefig(&#39;{0}/{1}.png&#39;.format(self.path, self.swap_times))</code></pre><p>需要注意的几点是：</p><ol><li><code>matplotlib</code>具有两种绘图模式，阻塞(<code>block</code>)模式和交互(<code>interactive</code>)模式，阻塞模式必须等待当前绘图窗口关闭方才执行后续程序，而交互模式则无需等待。为了动态显示排序过程，自然选择交互模式，所以初始化时调用<code>ion()</code>函数打开交互模式</li><li>相比于其它样式的图表，使用柱状图<code>bar</code>能够更直观显示数据大小及变化过程</li><li>在每次重绘图表时，需要清空原有图表</li><li>必须调用<code>pause</code>函数予以等待，否则可能出现无法显示图表的情况，等待时长自定</li><li>为了突出显示每次交换的两个数据，可以使用红色图表单独绘制交换数据</li><li>使用<code>matplotlib</code>函数库中的<code>savefig</code>可以将图表为至本地图片文件，为后续生成<code>gif</code>图片做准备</li></ol><p><img src="/assets/algorithm/dataBar.png" alt="data bar"></p><h2 id="生成GIF动图"><a href="#生成GIF动图" class="headerlink" title="生成GIF动图"></a>生成GIF动图</h2><p>有了前面保存好的图片，使用<code>imageio</code>库的<code>append</code>函数及<code>mimsave</code>即可生成<code>gif</code>图片。图片间隔时间由<code>mimsave</code>函数的<code>duration</code>参数决定。</p><pre><code class="python"># main.pyimport imageiofrom quick_sort import QuickSortimport osfrom os.path import joindef save_gif(path, gif_name):    if not os.path.exists(path) or len(os.listdir(path))==0:        return    images = []    for file_name in range(len(os.listdir(path))):        file_path = join(path, &#39;{}.png&#39;.format(file_name))        images.append(imageio.imread(file_path))    imageio.mimsave(join(path, gif_name), images, &#39;GIF&#39;, duration=0.2)</code></pre><p><img src="/assets/algorithm/quickSort.gif" alt="quick sort"></p><p>至此，便完成了快排的算法实现及其可视化。</p><h2 id="完整源码"><a href="#完整源码" class="headerlink" title="完整源码"></a>完整源码</h2><p>代码已上传至<a href="https://github.com/Litreily/Python-demos.git" target="_blank" rel="noopener">github Python-demos</a> <code>algorithm</code>目录</p><ul><li>quick_sort.py</li></ul><pre><code class="python">#!/usr/bin/env python# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport timeimport osclass QuickSort(object):    &#39;&#39;&#39;Quick sort algorithm&#39;&#39;&#39;    def __init__(self, debug=False, save_fig=False):        self.debug = debug        self.save_fig = save_fig        self.fig, self.ax = plt.subplots()        plt.ion()        if self.save_fig:            self.path = &#39;./images/{0}&#39;.format(time.strftime(&#39;%Y%m%d_%H%M%S&#39;))            os.makedirs(self.path)    def sort(self, data):        self.swap_times = 0        self.__plot_figure(data)        # set the largest element to the end        # self.__swap(data, data.index(max(data)), len(data) - 1)        self.__sort(data, 0, len(data) - 1)        return self.swap_times, self.path    def __swap(self, data, lo, hi):        data[lo], data[hi] = data[hi], data[lo]        self.swap_times += 1        if self.debug:            print(&#39;\t{0} swap({1}, {2})&#39;.format(data, lo, hi))        self.__plot_figure(data, lo, hi, show_swap=True)    def __plot_figure(self, data, lo=0, hi=0, show_swap=False):        &#39;&#39;&#39;plot and save figure&#39;&#39;&#39;        self.ax.clear()        self.ax.set_title(&#39;data quicksort&#39;)        self.ax.bar(range(len(data)), data, label=&#39;data&#39;)        if show_swap:            self.ax.bar([lo, hi], [data[lo], data[hi]], color=&#39;red&#39;, label=&#39;swap&#39;)        plt.legend()        plt.pause(0.001)        if self.save_fig:            plt.savefig(&#39;{0}/{1}.png&#39;.format(self.path, self.swap_times))    def __sort(self, data, lo, hi):        if lo &gt;= hi:            return        key = self.__partition(data, lo, hi)        self.__sort(data, lo, key - 1)        self.__sort(data, key + 1, hi)    def __partition(self, data, lo, hi):        &#39;&#39;&#39;partition array&#39;&#39;&#39;        i = lo        j = hi        v = data[lo] # slicing element        while True:            # find one element that larger than v scan from left to right(→)            i += 1            while data[i] &lt; v:                # below judge can dropped if the end element is the largest                if i == hi:                    break                i += 1            # find one element that smaller than v scan from right to left(←)            while v &lt; data[j]:                j -= 1            if i &gt;= j:                break            self.__swap(data, i, j)        self.__swap(data, lo, j)        return j</code></pre><ul><li>main.py</li></ul><pre><code class="python">#!/bin/env python# -*- encoding: utf-8 -*-import timeimport randomimport imageiofrom quick_sort import QuickSortimport osfrom os.path import joindef save_gif(path, gif_name):    if not os.path.exists(path) or len(os.listdir(path))==0:        return    images = []    for file_name in range(len(os.listdir(path))):        file_path = join(path, &#39;{}.png&#39;.format(file_name))        images.append(imageio.imread(file_path))    imageio.mimsave(join(path, gif_name), images, &#39;GIF&#39;, duration=0.2)def main():    data = []    random.seed(time.time())    random.shuffle(data)    print(&#39;source: {0}&#39;.format(data))    start = time.time()    qs = QuickSort(debug=False, save_fig=True)    swap_times, fig_path = qs.sort(data)    save_gif(fig_path, &#39;quick_sort.gif&#39;)    stop = time.time()    print(&#39;result: {0}\n&#39;.format(data))    print(&#39;----------------------------------&#39;)    print(&#39;swap times: {0}&#39;.format(swap_times))    print(&#39;spend time: {0}s&#39;.format(stop - start))    print(&#39;image path: {0}&#39;.format(fig_path))    print(&#39;----------------------------------&#39;)if __name__ == &#39;__main__&#39;:    main()</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近装了个&lt;code&gt;Anaconda&lt;/code&gt;，准备学习一下数据可视化。本着三天打鱼两天装死的心态，重新抱起崭新的&lt;strong&gt;算法&lt;/strong&gt;书，认真学起了快排算法。学完后用&lt;code&gt;Python&lt;/code&gt;实现了一遍基本的快排，然后使用&lt;code&gt;ma
      
    
    </summary>
    
      <category term="算法" scheme="http://www.litreily.top/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="algorithm" scheme="http://www.litreily.top/tags/algorithm/"/>
    
      <category term="sort" scheme="http://www.litreily.top/tags/sort/"/>
    
      <category term="visualization" scheme="http://www.litreily.top/tags/visualization/"/>
    
  </entry>
  
  <entry>
    <title>tmux常用配置说明</title>
    <link href="http://www.litreily.top/2018/06/19/tmux-config/"/>
    <id>http://www.litreily.top/2018/06/19/tmux-config/</id>
    <published>2018-06-19T01:26:00.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p><code>tmux</code>作为<code>Linux</code>三大神器之一，结合<code>zsh</code>和<code>vim</code>堪称无敌，<code>tmux</code>作为分屏利器自有其强大之处，本文就其常用配置予以说明。</p><h2 id="安装tmux"><a href="#安装tmux" class="headerlink" title="安装tmux"></a>安装tmux</h2><pre><code class="zsh"># install in ubuntu$ sudo apt-get install tmux</code></pre><ul><li><code>Ctrl+B</code>: default keys of tmux <code>prefix</code></li><li><code>.tmux.conf</code>: config file of <code>tmux</code></li><li><code>tmux source-file .tmux.conf</code>: command to reload <code>.tmux.conf</code></li></ul><a id="more"></a><h2 id="禁止自动更新窗口名称"><a href="#禁止自动更新窗口名称" class="headerlink" title="禁止自动更新窗口名称"></a>禁止自动更新窗口名称</h2><p>默认情况下，<code>tmux</code>会根据当前目录及相关操作自动更改窗口名称，但通常情况下，我们是不需要其修改的，为此可以在配置文件中添加：</p><pre><code class="yml">setw -g allow_rename &#39;off&#39;</code></pre><h2 id="设置zsh作为默认shell"><a href="#设置zsh作为默认shell" class="headerlink" title="设置zsh作为默认shell"></a>设置zsh作为默认shell</h2><p>如果先安装的<code>tmux</code>, 后安装和配置<code>zsh</code>，那tmux有可能默认打开使用的是<code>bash</code>，此时可以使用以下配置指令予以更改：</p><pre><code class="yml">set-option -g default-shell /bin/zsh</code></pre><h2 id="设置vim作为默认编辑器"><a href="#设置vim作为默认编辑器" class="headerlink" title="设置vim作为默认编辑器"></a>设置vim作为默认编辑器</h2><p>在<code>tmux</code>的搜索模式<code>prefix+[</code>下，默认使用↑↓←→箭头作为移动按键，对于习惯于<code>vim</code>操作的我来讲，这显然很不方便，还好<code>tmux</code>提供了使用<code>vim</code>作为编辑器的选项，启用配置如下：</p><pre><code class="yml">setw -g mode-keys vi</code></pre><h2 id="解决配色问题"><a href="#解决配色问题" class="headerlink" title="解决配色问题"></a>解决配色问题</h2><p>在<code>tmux</code>中，有可能部分应用的配色与纯<code>shell</code>下不一致，尤其是<code>vim</code>，后来发现<code>htop</code>也存在这个问题。要解决很简单，在<code>tmux</code>配置中加上一句：</p><pre><code class="yml">set -g default-terminal &quot;screen-256color&quot;</code></pre><h2 id="存储-恢复tmux工作环境"><a href="#存储-恢复tmux工作环境" class="headerlink" title="存储/恢复tmux工作环境"></a>存储/恢复tmux工作环境</h2><p>如果在服务器上使用<code>tmux</code>，由于服务器基本不关机，所以不用考虑<code>tmux</code>环境会丢失。但是在个人电脑上使用的话，重启电脑后<code>tmux</code>环境就丢失了，如果每次重启都要重新配置一遍环境的话，那未免太浪费时间了。所以这里推荐个插件<a href="https://github.com/tmux-plugins/tmux-resurrect" target="_blank" rel="noopener">tmux-resurrect</a> </p><p><code>tmux-resurrect</code>可以对<code>tmux</code>环境进行保存和恢复，安装及配置如下：</p><pre><code class="zsh">$ mkdir ~/.tmux$ git clone https://github.com/tmux-plugins/tmux-resurrect ~/.tmux/tmux-resurrect# edit .tmux.conf at the bottom$ vi ~/.tmux.confrun-shell ~/.tmux/tmux-resurrect/resurrect.tmux# re source .tmux.conf$ tmux source-file ~/.tmux.conf</code></pre><p><code>tmux-resurrect</code>用法:</p><ul><li><code>prefix + Ctrl-s</code> - save</li><li><code>prefix + Ctrl-r</code> - restore</li></ul><p>插件<code>tmux-resurrect</code>还可以结合插件<code>tmux-continum</code>一起使用，<code>tmux-continum</code>可以自动定时存储环境，在开机后打开<code>tmux</code>时自动恢复环境，这就避免了人为存储和恢复的麻烦以及因忘记存储导致的环境丢失。<code>tmux-continum</code>的安装及配置如下:</p><pre><code class="zsh">$ git clone https://github.com/tmux-plugins/tmux-continuum ~/.tmux/tmux-continum# edit .tmux.conf$ vi ~/.tmux.confset -g @continuum-restore &#39;on&#39;  # restore last saved envset -g @continuum-save-interval &#39;30&#39;  # default is 15 minutesrun-shell ~/.tmux/tmux-continum/continuum.tmux$ tmux source ~/.tmux.conf</code></pre><p><strong>说明</strong>：</p><ol><li>以上两个插件都是手动安装的，其实也可以使用<code>tmux</code>的插件管理器<a href="https://github.com/tmux-plugins/tpm" target="_blank" rel="noopener">tpm</a>进行安装，这里就不详述了。</li><li>以上插件要求<code>tmux</code>版本大等于<code>1.9</code></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://linuxtoy.org/archives/tmux-resurrect-and-continuum.html" target="_blank" rel="noopener">https://linuxtoy.org/archives/tmux-resurrect-and-continuum.html</a></li><li><a href="https://github.com/tmux/tmux" target="_blank" rel="noopener">https://github.com/tmux/tmux</a></li><li><a href="https://github.com/tmux-plugins/tpm" target="_blank" rel="noopener">https://github.com/tmux-plugins/tpm</a></li><li><a href="https://github.com/tmux-plugins/tmux-resurrect" target="_blank" rel="noopener">https://github.com/tmux-plugins/tmux-resurrect</a></li><li><a href="https://github.com/tmux-plugins/tmux-continuum" target="_blank" rel="noopener">https://github.com/tmux-plugins/tmux-continuum</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;tmux&lt;/code&gt;作为&lt;code&gt;Linux&lt;/code&gt;三大神器之一，结合&lt;code&gt;zsh&lt;/code&gt;和&lt;code&gt;vim&lt;/code&gt;堪称无敌，&lt;code&gt;tmux&lt;/code&gt;作为分屏利器自有其强大之处，本文就其常用配置予以说明。&lt;/p&gt;
&lt;h2 id=&quot;安装tmux&quot;&gt;&lt;a href=&quot;#安装tmux&quot; class=&quot;headerlink&quot; title=&quot;安装tmux&quot;&gt;&lt;/a&gt;安装tmux&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;zsh&quot;&gt;# install in ubuntu
$ sudo apt-get install tmux
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Ctrl+B&lt;/code&gt;: default keys of tmux &lt;code&gt;prefix&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.tmux.conf&lt;/code&gt;: config file of &lt;code&gt;tmux&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tmux source-file .tmux.conf&lt;/code&gt;: command to reload &lt;code&gt;.tmux.conf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://www.litreily.top/categories/Linux/"/>
    
    
      <category term="ubuntu" scheme="http://www.litreily.top/tags/ubuntu/"/>
    
      <category term="tmux" scheme="http://www.litreily.top/tags/tmux/"/>
    
  </entry>
  
  <entry>
    <title>Openwrt中添加内核模块</title>
    <link href="http://www.litreily.top/2018/05/30/openwrt-ko/"/>
    <id>http://www.litreily.top/2018/05/30/openwrt-ko/</id>
    <published>2018-05-30T15:23:56.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p>如果想要在<code>Openwrt</code>系统中添加一个内核模块，其实并不难，对着某个已有的内核模块，依葫芦画瓢嘛，不难。本文将介绍一个最最简单的</p><h2 id="sourcecode"><a href="#sourcecode" class="headerlink" title="sourcecode"></a>sourcecode</h2><p>path: <code>git_repo/package/kmod-demo/</code></p><p>下面是源码包含的所有文件，根级<code>Makefile</code>提供给内核编译器使用，<code>src</code>级<code>Makefile</code>用于编译<code>.ko</code>文件。</p><pre><code>$ tree.├── Makefile└── src    ├── demo.c    ├── Kconfig    └── Makefile</code></pre><h3 id="demo-c"><a href="#demo-c" class="headerlink" title="demo.c"></a>demo.c</h3><p>源码非常简单，在装载时打印<code>Hello World</code>，卸载时打印<code>Exit</code>。</p><pre><code class="c">#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;static int __init hello_init(void){    printk(&quot;Hello World\n&quot;);    return 0;}static void __exit hello_exit(void){    printk(&quot;Exit\n&quot;);}module_init(hello_init);module_exit(hello_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Guangtao.wu&quot;); </code></pre><h3 id="Kconfig"><a href="#Kconfig" class="headerlink" title="Kconfig"></a>Kconfig</h3><p>在<code>Kconfig</code>文件中添加一些编译相关的选项</p><pre><code class="json">config DEMO    tristate &quot;This is a Module_DEMO&quot;    default n    help      This is a DEMO, for debugging kernel model.      If unsure, say N.</code></pre><ul><li><code>default</code> : 该选项的默认值<ul><li><code>n</code> 不编译</li><li><code>y</code> 编译到内核，启动时自动加载</li><li><code>m</code> 编译生成<code>.ko</code>模块，不自动加载，需使用<code>insmod</code>加载</li></ul></li></ul><h3 id="Root-Makefile"><a href="#Root-Makefile" class="headerlink" title="Root - Makefile"></a>Root - Makefile</h3><p>根级<code>Makefile</code>仿其它内核模块编写，注意替换名称<code>demo</code>，<code>SUBMENU</code>以及<code>TITLE</code>将显示在<code>make menuconfig</code>的交互界面中。</p><pre><code class="makefile"># # Copyright (C) 2006 OpenWrt.org## This is free software, licensed under the GNU General Public License v2.# See /LICENSE for more information.## $Id: Makefile 6565 2007-03-14 13:42:21Z nbd $include $(TOPDIR)/rules.mkinclude $(INCLUDE_DIR)/kernel.mkPKG_NAME:=demoPKG_RELEASE:=1PKG_BUILD_DIR:=$(KERNEL_BUILD_DIR)/$(PKG_NAME)include $(INCLUDE_DIR)/package.mkdefine KernelPackage/demo  SUBMENU:=Other modules  TITLE:=kernel demo   VERSION:=$(LINUX_VERSION)-$(BOARD)-$(PKG_RELEASE)  FILES:= $(PKG_BUILD_DIR)/demo.$(LINUX_KMOD_SUFFIX)  AUTOLOAD:=$(call AutoLoad,46,demo)endefdefine Build/Prepare    mkdir -p $(PKG_BUILD_DIR)    $(CP) ./src/* $(PKG_BUILD_DIR)endefdefine Build/Compile    $(MAKE) -C &quot;$(LINUX_DIR)&quot; \        CROSS_COMPILE=&quot;$(TARGET_CROSS)&quot; \        ARCH=&quot;$(LINUX_KARCH)&quot; \        SUBDIRS=&quot;$(PKG_BUILD_DIR)&quot; \        EXTRA_CFLAGS=&quot;$(BUILDFLAGS)&quot; \        modulesendefdefine KernelPackage/demo/install    $(INSTALL_DIR) $(1)/lib/network/endef$(eval $(call KernelPackage,demo))</code></pre><h3 id="src-Makefile"><a href="#src-Makefile" class="headerlink" title="src - Makefile"></a>src - Makefile</h3><p>源码的<code>Makefile</code>很简单，其实只要一行就够了，其中<code>CONFIG_&lt;module_name&gt;</code>需要在后续<code>.config</code>文件中配置，或者直接将<code>$(CONFIG_DEMO)</code>改为<code>y</code>或<code>m</code>即可。</p><pre><code class="Makefile">obj ?= .obj-$(CONFIG_DEMO) += demo.o</code></pre><h2 id="更新-config"><a href="#更新-config" class="headerlink" title="更新 .config"></a>更新 .config</h2><p>使用<code>make menuconfig</code>或者直接修改配置文件<code>.config</code>可以启用新添加的内核模块。</p><ol><li><code>make menuconfig</code>: 搜索<code>demo</code>将其选中然后保存退出</li><li>手动启用，修改<code>.config</code>，在<code>Others module</code>配置参数附近添加如下内容</li></ol><pre><code class="c">CONFIG_KERNEL_kmod_demo=y</code></pre><h2 id="make"><a href="#make" class="headerlink" title="make"></a>make</h2><p>当源码和配置选项都准备好后，便可以编译安装了，单独安装需要整个<code>kernel</code>已经编译过一次，以保证交叉编译工具链能够正常使用。</p><pre><code class="sh">$ make package/kmod-demo/compile V=s$ make package/kmod-demo/install V=s</code></pre><h2 id="insmod-rmmod"><a href="#insmod-rmmod" class="headerlink" title="insmod/rmmod"></a>insmod/rmmod</h2><pre><code class="sh"># insmod$ insmod demo.koHello World# rmmod$ rmmod demo.koExit</code></pre><p>look, <code>printk</code>的信息已经成功打印出来了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;如果想要在&lt;code&gt;Openwrt&lt;/code&gt;系统中添加一个内核模块，其实并不难，对着某个已有的内核模块，依葫芦画瓢嘛，不难。本文将介绍一个最最简单的&lt;/p&gt;
&lt;h2 id=&quot;sourcecode&quot;&gt;&lt;a href=&quot;#sourcecode&quot; class=&quot;header
      
    
    </summary>
    
      <category term="嵌入式" scheme="http://www.litreily.top/categories/%E5%B5%8C%E5%85%A5%E5%BC%8F/"/>
    
    
      <category term="C/C++" scheme="http://www.litreily.top/tags/C-C/"/>
    
      <category term="linux" scheme="http://www.litreily.top/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Python网络爬虫4 - scrapy入门</title>
    <link href="http://www.litreily.top/2018/05/27/scrapy-start/"/>
    <id>http://www.litreily.top/2018/05/27/scrapy-start/</id>
    <published>2018-05-27T15:38:28.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p><code>scrapy</code>作为一款强大的爬虫框架，当然要好好学习一番，本文便是本人学习和使用<code>scrapy</code>过后的一个总结，内容比较基础，算是入门笔记吧，主要讲述<code>scrapy</code>的基本概念和使用方法。</p><h2 id="scrapy-framework"><a href="#scrapy-framework" class="headerlink" title="scrapy framework"></a>scrapy framework</h2><p>首先附上<code>scrapy</code>经典图如下：</p><p><img src="/assets/spider/scrapy/scrapy.jpg" alt="scrapy framework"></p><p><code>scrapy</code>框架包含以下几个部分</p><ol><li><code>Scrapy Engine</code> 引擎</li><li><code>Spiders</code> 爬虫</li><li><code>Scheduler</code> 调度器</li><li><code>Downloader</code> 下载器</li><li><code>Item Pipeline</code> 项目管道</li><li><code>Downloader Middlewares</code> 下载器中间件</li><li><code>Spider Middlewares</code> 爬虫中间件</li></ol><h3 id="spider-process"><a href="#spider-process" class="headerlink" title="spider process"></a>spider process</h3><p>其爬取过程简述如下：</p><ol><li>引擎从爬虫获取首个待爬取的链接<code>url</code>，并传递给调度器</li><li>调度器将链接存入队列</li><li>引擎向调度器请求要爬取的链接，并将请求得到的链接经下载器中间件传递给下载器</li><li>下载器从网上下载网页，下载后的网页经下载器中间件传递给引擎</li><li>引擎将网页经爬虫中间件传递给爬虫</li><li>爬虫对网页进行解析，将得到的<code>Items</code>和新的链接经爬虫中间件交给引擎</li><li>引擎将从爬虫得到的<code>Items</code>交给项目管道，将新的链接请求<code>requests</code>交给调度器</li><li>此后循环2~7步，直到没有待爬取的链接为止</li></ol><p>需要说明的是，项目管道(<code>Item Pipeline</code>)主要完成数据清洗，验证，持久化存储等工作；下载器中间件(<code>Downloader Middlewares</code>)作为下载器和引擎之间的的钩子(<code>hook</code>)，用于监听或修改下载请求或已下载的网页，比如修改请求包的头部信息等；爬虫中间件(<code>Spider Middlewares</code>)作为爬虫和引擎之间的钩子(<code>hook</code>)，用于处理爬虫的输入输出，即网页<code>response</code>和爬虫解析网页后得到的<code>Items</code>和<code>requests</code>。</p><h3 id="Items"><a href="#Items" class="headerlink" title="Items"></a>Items</h3><p>至于什么是<code>Items</code>，个人认为就是经爬虫解析后得到的一个数据单元，包含一组数据，比如爬取的是某网站的商品信息，那么每爬取一个网页可能会得到多组商品信息，每组信息包含商品名称，价格，生产日期，商品样式等，那我们便可以定义一组<code>Item</code></p><pre><code class="python">from scrapy.item import Itemfrom scrapy.item import Fieldclass GoodsItem(Item):    name = Field()    price = Field()    date = Field()    types = Field()</code></pre><p><code>Field()</code>实质就是一个字典<code>Dict()</code>类型的扩展，如上代码所示，一组<code>Item</code>对应一个商品信息，单个网页可能包含一个或多个商品，所有<code>Item</code>信息都需要在<code>Spider</code>中赋值，然后经引擎交给<code>Item Pipeline</code>。具体实现在后续博文的实例中会有体现，本文旨在简单记述<code>scrapy</code>的基本概念和使用方法。</p><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><p>with <code>pip</code></p><pre><code class="node">pip install scrapy</code></pre><p>or <code>conda</code></p><pre><code class="node">conda install -c conda-forge scrapy</code></pre><p>基本指令如下：</p><pre><code class="js">D:\WorkSpace&gt;scrapy --helpScrapy 1.5.0 - no active projectUsage:  scrapy &lt;command&gt; [options] [args]Available commands:  bench         Run quick benchmark test  fetch         Fetch a URL using the Scrapy downloader  genspider     Generate new spider using pre-defined templates  runspider     Run a self-contained spider (without creating a project)  settings      Get settings values  shell         Interactive scraping console  startproject  Create new project  version       Print Scrapy version  view          Open URL in browser, as seen by Scrapy  [ more ]      More commands available when run from project directoryUse &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command</code></pre><p>如果需要使用虚拟环境，需要安装<code>virtualenv</code></p><pre><code class="node">pip install virtualenv</code></pre><h2 id="scrapy-startproject"><a href="#scrapy-startproject" class="headerlink" title="scrapy startproject"></a>scrapy startproject</h2><pre><code class="sh">scrapy startproject &lt;project-name&gt; [project-dir]</code></pre><p>使用该指令可以生成一个新的<code>scrapy</code>项目，以<code>demo</code>为例</p><pre><code class="sh">$ scrapy startproject demo...You can start your first spider with:    cd demo    scrapy genspider example example.com$ cd demo$ tree.├── demo│   ├── __init__.py│   ├── items.py│   ├── middlewares.py│   ├── pipelines.py│   ├── __pycache__│   ├── settings.py│   └── spiders│       ├── __init__.py│       └── __pycache__└── scrapy.cfg4 directories, 7 files</code></pre><p>可以看到<code>startproject</code>自动生成了一些文件夹和文件，其中：</p><ol><li><code>scrapy.cfg</code>: 项目配置文件，一般不用修改</li><li><code>items.py</code>: 定义<code>items</code>的文件，例如上述的<code>GoodsItem</code></li><li><code>middlewares.py</code>: 中间件代码，默认包含下载器中间件和爬虫中间件</li><li><code>pipelines.py</code>: 项目管道，用于处理<code>spider</code>返回的<code>items</code>，包括清洗，验证，持久化等</li><li><code>settings.py</code>: 全局配置文件，包含各类全局变量</li><li><code>spiders</code>: 该文件夹用于存储所有的爬虫文件，注意一个项目可以包含多个爬虫</li><li><code>__init__.py</code>: 该文件指示当前文件夹属于一个<code>python</code>模块</li><li><code>__pycache__</code>: 存储解释器生成的<code>.pyc</code>文件（一种跨平台的字节码<code>byte code</code>），在<code>python2</code>中该类文件与<code>.py</code>保存在相同文件夹</li></ol><h2 id="scrapy-genspider"><a href="#scrapy-genspider" class="headerlink" title="scrapy genspider"></a>scrapy genspider</h2><p>项目生成以后，可以使用<code>scrapy genspider</code>指令自动生成一个爬虫文件，比如，如果要爬取<a href="www.huaban.com">花瓣网首页</a>，执行以下指令：</p><pre><code class="sh">$ cd demo$ scrapy genspider huaban www.huaban.com</code></pre><p>默认生成的爬虫文件<code>huaban.py</code>如下：</p><pre><code class="python"># -*- coding: utf-8 -*-import scrapyclass HuabanSpider(scrapy.Spider):    name = &#39;huaban&#39;    allowed_domains = [&#39;www.huaban.com&#39;]    start_urls = [&#39;http://www.huaban.com/&#39;]    def parse(self, response):        pass</code></pre><ul><li>爬虫类继承于<code>scrapy.Spider</code></li><li><code>name</code>是必须存在的参数，用以标识该爬虫</li><li><code>allowed_domains</code>指代允许爬虫爬取的域名，指定域名之外的链接将被丢弃</li><li><code>start_urls</code>存储爬虫的起始链接，该参数是列表类型，所以可以同时存储多个链接</li></ul><p>如果要自定义起始链接，也可以重写<code>scrapy.Spider</code>类的<code>start_requests</code>函数，此处不予细讲。</p><p><code>parse</code>函数是一个默认的回调函数，当下载器下载网页后，会调用该函数进行解析，<code>response</code>就是请求包的响应数据。至于网页内容的解析方法，<code>scrapy</code>内置了几种选择器(<code>Selector</code>)，包括<code>xpath</code>选择器、<code>CSS</code>选择器和正则匹配。下面是一些选择器的使用示例，方便大家更加直观的了解选择器的用法。</p><pre><code class="python"># xpath selectorresponse.xpath(&#39;//a&#39;)response.xpath(&#39;./img&#39;).extract()response.xpath(&#39;//*[@id=&quot;huaban&quot;]&#39;).extract_first()repsonse.xpath(&#39;//*[@id=&quot;Profile&quot;]/div[1]/a[2]/text()&#39;).extract_first()# css selectorresponse.css(&#39;a&#39;).extract()response.css(&#39;#Profile &gt; div.profile-basic&#39;).extract_first()response.css(&#39;a[href=&quot;test.html&quot;]::text&#39;).extract_first()# re selectorresponse.xpath(&#39;.&#39;).re(&#39;id:\s*(\d+)&#39;)response.xpath(&#39;//a/text()&#39;).re_first(&#39;username: \s(.*)&#39;)</code></pre><p>需要说明的是，<code>response</code>不能直接调用<code>re</code>,<code>re_first</code>.</p><h2 id="scrapy-crawl"><a href="#scrapy-crawl" class="headerlink" title="scrapy crawl"></a>scrapy crawl</h2><p>假设爬虫编写完了，那就可以使用<code>scrapy crawl</code>指令开始执行爬取任务了。</p><p>当进入一个创建好的<code>scrapy</code>项目目录时，使用<code>scrapy -h</code>可以获得相比未创建之前更多的帮助信息，其中就包括用于启动爬虫任务的<code>scrapy crawl</code></p><pre><code class="sh">$ scrapy -hScrapy 1.5.0 - project: huabanUsage:  scrapy &lt;command&gt; [options] [args]Available commands:  bench         Run quick benchmark test  check         Check spider contracts  crawl         Run a spider  edit          Edit spider  fetch         Fetch a URL using the Scrapy downloader  genspider     Generate new spider using pre-defined templates  list          List available spiders  parse         Parse URL (using its spider) and print the results  runspider     Run a self-contained spider (without creating a project)  settings      Get settings values  shell         Interactive scraping console  startproject  Create new project  version       Print Scrapy version  view          Open URL in browser, as seen by ScrapyUse &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command</code></pre><pre><code class="sh">$ scrapy crawl -hUsage=====  scrapy crawl [options] &lt;spider&gt;Run a spiderOptions=======--help, -h              show this help message and exit-a NAME=VALUE           set spider argument (may be repeated)--output=FILE, -o FILE  dump scraped items into FILE (use - for stdout)--output-format=FORMAT, -t FORMAT                        format to use for dumping items with -oGlobal Options----------------logfile=FILE          log file. if omitted stderr will be used--loglevel=LEVEL, -L LEVEL                        log level (default: DEBUG)--nolog                 disable logging completely--profile=FILE          write python cProfile stats to FILE--pidfile=FILE          write process ID to FILE--set=NAME=VALUE, -s NAME=VALUE                        set/override setting (may be repeated)--pdb                   enable pdb on failure</code></pre><p>从<code>scrapy crawl</code>的帮助信息可以看出，该指令包含很多可选参数，但必选参数只有一个，就是<code>spider</code>，即要执行的爬虫名称，对应每个爬虫的名称(<code>name</code>)。</p><pre><code class="sh">scrapy crawl huaban</code></pre><p>至此，一个<code>scrapy</code>爬虫任务的创建和执行过程就介绍完了，至于实例，后续博客会陆续介绍。</p><h2 id="scrapy-shell"><a href="#scrapy-shell" class="headerlink" title="scrapy shell"></a>scrapy shell</h2><p>最后简要说明一下指令<code>scrapy shell</code>，这是一个交互式的<code>shell</code>,类似于命令行形式的<code>python</code>，当我们刚开始学习<code>scrapy</code>或者刚开始爬取某个陌生的站点时，可以使用它熟悉各种函数操作或者选择器的使用，用它来不断试错纠错，熟练掌握<code>scrapy</code>各种用法。</p><pre><code class="sh">$ scrapy shell www.huaban.com2018-05-29 23:58:49 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)2018-05-29 23:58:49 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.6.3 (v3.6.3:2c5fed8, Oct  32017, 17:26:49) [MSC v.1900 32 bit (Intel)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.17134-SP02018-05-29 23:58:49 [scrapy.crawler] INFO: Overridden settings: {&#39;DUPEFILTER_CLASS&#39;: &#39;scrapy.dupefilters.BaseDupeFilter&#39;, &#39;LOGSTATS_INTERVAL&#39;: 0}2018-05-29 23:58:49 [scrapy.middleware] INFO: Enabled extensions:[&#39;scrapy.extensions.corestats.CoreStats&#39;, &#39;scrapy.extensions.telnet.TelnetConsole&#39;]2018-05-29 23:58:50 [scrapy.middleware] INFO: Enabled downloader middlewares:[&#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;, &#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;, &#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;, &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;, &#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;, &#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;, &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;, &#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;, &#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;, &#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;, &#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;]2018-05-29 23:58:50 [scrapy.middleware] INFO: Enabled spider middlewares:[&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;, &#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;, &#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;, &#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;, &#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;]2018-05-29 23:58:50 [scrapy.middleware] INFO: Enabled item pipelines:[]2018-05-29 23:58:50 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232018-05-29 23:58:50 [scrapy.core.engine] INFO: Spider opened2018-05-29 23:58:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET http://huaban.com/&gt; from &lt;GET http://www.huaban.com&gt;2018-05-29 23:58:50 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://huaban.com/&gt; (referer: None)[s] Available Scrapy objects:[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x03385CB0&gt;[s]   item       {}[s]   request    &lt;GET http://www.huaban.com&gt;[s]   response   &lt;200 http://huaban.com/&gt;[s]   settings   &lt;scrapy.settings.Settings object at 0x04CC4D10&gt;[s]   spider     &lt;DefaultSpider &#39;default&#39; at 0x4fa6bf0&gt;[s] Useful shortcuts:[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)[s]   fetch(req)                  Fetch a scrapy.Request and update local objects[s]   shelp()           Shell help (print this help)[s]   view(response)    View response in a browserIn [1]: view(response)Out[1]: TrueIn [2]: response.xpath(&#39;//a&#39;)Out[2]:[&lt;Selector xpath=&#39;//a&#39; data=&#39;&lt;a id=&quot;elevator&quot; class=&quot;off&quot; onclick=&quot;re&#39;&gt;, &lt;Selector xpath=&#39;//a&#39; data=&#39;&lt;a class=&quot;plus&quot;&gt;&lt;/a&gt;&#39;&gt;, &lt;Selector xpath=&#39;//a&#39; data=&#39;&lt;a onclick=&quot;app.showUploadDialog();&quot;&gt;添加采&#39;&gt;, &lt;Selector xpath=&#39;//a&#39; data=&#39;&lt;a class=&quot;add-board-item&quot;&gt;添加画板&lt;i class=&quot;&#39;&gt;, &lt;Selector xpath=&#39;//a&#39; data=&#39;&lt;a href=&quot;/about/goodies/&quot;&gt;安装采集工具&lt;i class&#39;&gt;, &lt;Selector xpath=&#39;//a&#39; data=&#39;&lt;a class=&quot;huaban_security_oauth&quot; logo_si&#39;&gt;]In [3]: response.xpath(&#39;//a&#39;).extract()Out[3]:[&#39;&lt;a id=&quot;elevator&quot; class=&quot;off&quot; onclick=&quot;return false;&quot; title=&quot;回到顶部&quot;&gt;&lt;/a&gt;&#39;, &#39;&lt;a class=&quot;plus&quot;&gt;&lt;/a&gt;&#39;, &#39;&lt;a onclick=&quot;app.showUploadDialog();&quot;&gt;添加采集&lt;i class=&quot;upload&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39;, &#39;&lt;a class=&quot;add-board-item&quot;&gt;添加画板&lt;i class=&quot;add-board&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39;, &#39;&lt;a href=&quot;/about/goodies/&quot;&gt;安装采集工具&lt;i class=&quot;goodies&quot;&gt;&lt;/i&gt;&lt;/a&gt;&#39;, &#39;&lt;a class=&quot;huaban_security_oauth&quot; logo_size=&quot;124x47&quot; logo_type=&quot;realname&quot; href=&quot;//www.anquan.org&quot; rel=&quot;nofollow&quot;&gt; &lt;script src=&quot;//static.anquan.org/static/outer/js/aq_auth.js&quot;&gt;&lt;/script&gt; &lt;/a&gt;&#39;]In [4]: response.xpath(&#39;//img&#39;)Out[4]: [&lt;Selector xpath=&#39;//img&#39; data=&#39;&lt;img src=&quot;https://d5nxst8fruw4z.cloudfro&#39;&gt;]In [5]: response.xpath(&#39;//a/text()&#39;)Out[5]:[&lt;Selector xpath=&#39;//a/text()&#39; data=&#39;添加采集&#39;&gt;, &lt;Selector xpath=&#39;//a/text()&#39; data=&#39;添加画板&#39;&gt;, &lt;Selector xpath=&#39;//a/text()&#39; data=&#39;安装采集工具&#39;&gt;, &lt;Selector xpath=&#39;//a/text()&#39; data=&#39; &#39;&gt;, &lt;Selector xpath=&#39;//a/text()&#39; data=&#39; &#39;&gt;]In [6]: response.xpath(&#39;//a/text()&#39;).extract()Out[6]: [&#39;添加采集&#39;, &#39;添加画板&#39;, &#39;安装采集工具&#39;, &#39; &#39;, &#39; &#39;]In [7]: response.xpath(&#39;//a/text()&#39;).extract_first()Out[7]: &#39;添加采集&#39;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;scrapy&lt;/code&gt;作为一款强大的爬虫框架，当然要好好学习一番，本文便是本人学习和使用&lt;code&gt;scrapy&lt;/code&gt;过后的一个总结，内容比较基础，算是入门笔记吧，主要讲述&lt;code&gt;scrapy&lt;/code&gt;的基本概念和使用方法。&lt;/p&gt;
&lt;h2 
      
    
    </summary>
    
      <category term="Python" scheme="http://www.litreily.top/categories/Python/"/>
    
    
      <category term="spider" scheme="http://www.litreily.top/tags/spider/"/>
    
      <category term="scrapy" scheme="http://www.litreily.top/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Python网络爬虫3 - 生产者消费者模型爬取某金融网站数据</title>
    <link href="http://www.litreily.top/2018/04/30/cfachina/"/>
    <id>http://www.litreily.top/2018/04/30/cfachina/</id>
    <published>2018-04-30T09:03:35.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p>应一位金融圈的朋友所托，帮忙写个爬虫，帮他爬取<a href="http://www.cfachina.org/cfainfo/organbaseinfoServlet?all=personinfo#" target="_blank" rel="noopener">中国期货行业协议</a>网站中所有金融机构的从业人员信息。网站数据的获取本身比较简单，但是为了学习一些新的爬虫方法和技巧，即本文要讲述的<strong>生产者消费者模型</strong>，我又学习了一下Python中队列库<code>queue</code>及线程库<code>Thread</code>的使用方法。</p><h2 id="生产者消费者模型"><a href="#生产者消费者模型" class="headerlink" title="生产者消费者模型"></a>生产者消费者模型</h2><p>生产者消费者模型非常简单，相信大部分程序员都知道，就是一方作为生产者不断提供资源，另一方作为消费者不断消费资源。简单点说，就好比餐馆的厨师和顾客，厨师作为生产者不断制作美味的食物，而顾客作为消费者不断食用厨师提供的食物。此外，生产者与消费者之间可以是一对一、一对多、多对一和多对多的关系。</p><p>那么这个模型和爬虫有什么关系呢？其实，爬虫可以认为是一个生产者，它不断从网站爬取数据，爬取到的数据就是食物；而所得数据需要消费者进行数据清洗，把有用的数据吸收掉，把无用的数据丢弃。</p><p>在实践过程中，爬虫爬取和数据清洗分别对应一个<code>Thread</code>，两个线程之间通过顺序队列<code>queue</code>传递数据，数据传递过程就好比餐馆服务员从厨房把食物送到顾客餐桌上的过程。爬取线程负责爬取网站数据，并将原始数据存入队列，清洗线程从队列中按入队顺序读取原始数据并提取出有效数据。</p><p>以上便是对生产者消费者模型的简单介绍了，下面针对本次爬取任务予以详细说明。</p><h2 id="分析站点"><a href="#分析站点" class="headerlink" title="分析站点"></a>分析站点</h2><blockquote><p><a href="http://www.cfachina.org/cfainfo/organbaseinfoServlet?all=personinfo" target="_blank" rel="noopener">http://www.cfachina.org/cfainfo/organbaseinfoServlet?all=personinfo</a></p></blockquote><p><img src="/assets/spider/cfachina/home_page.png" alt="home page"></p><p>我们要爬取的数据是主页显示的表格中所有期货公司的<strong>从业人员信息</strong>，每个公司对应一个<strong>机构编号</strong>(<code>G01001~G01198</code>)。从上图可以看到有主页有分页，共8页。以<code>G01001</code>方正中期期货公司为例，点击该公司名称跳转至对应网页如下:</p><p><img src="/assets/spider/cfachina/personinfo.png" alt="personinfo"></p><p>从网址及网页内容可以提取出以下信息：</p><ol><li>网址<ul><li><a href="http://www.cfachina.org/cfainfo/organbaseinfoOneServlet?organid=+G01001+&amp;currentPage=1&amp;pageSize=20&amp;selectType=personinfo" target="_blank" rel="noopener">http://www.cfachina.org/cfainfo/organbaseinfoOneServlet?organid=+G01001+&amp;currentPage=1&amp;pageSize=20&amp;selectType=personinfo</a><ul><li><code>organid</code>: 机构编号，<code>+G01001+</code> ~ <code>+G01198+</code></li><li><code>currentPage</code>: 该机构从业人员信息当前页面编号</li><li><code>pageSize</code>: 每个页面显示的人员个数，默认20</li><li><code>selectType</code>: 固定为<code>personinfo</code></li></ul></li></ul></li><li>机构名称<code>mechanism_name</code>，在每页表格上方可以看到当前机构名称</li><li>从业人员信息，即每页的表格内容，也是我们要爬取的对象</li><li>该机构从业人员信息总页数<code>page_cnt</code></li></ol><p>我们最终爬取的数据可以按机构名称存储到对应的txt文件或excel文件中。</p><h3 id="获取机构名称"><a href="#获取机构名称" class="headerlink" title="获取机构名称"></a>获取机构名称</h3><p><img src="/assets/spider/cfachina/gst_title.png" alt="get mechanism name"></p><p>获取到某机构的任意从业信息页面后，使用<code>BeautifulSoup</code>可快速提取机构名称。</p><pre><code class="python">mechanism_name = soup.find(&#39;&#39;, {&#39;class&#39;:&#39;gst_title&#39;}).find_all(&#39;a&#39;)[2].get_text()</code></pre><p>那么有人可能会问，既然主页表格都已经包含了所有机构的编号和名称，为何还要多此一举的再获取一次呢？这是因为，我压根就不想爬主页的那些表格，直接根据机构编号的递增规律生成对应的网址即可，所以获取机构名称的任务就放在了爬取每个机构首个信息页面之后。</p><h3 id="获取机构信息对应的网页数量"><a href="#获取机构信息对应的网页数量" class="headerlink" title="获取机构信息对应的网页数量"></a>获取机构信息对应的网页数量</h3><p><img src="/assets/spider/cfachina/page_cnt.png" alt="get count of page"></p><p>每个机构的数据量是不等的，幸好每个页面都包含了当前页面数及总页面数。使用以下代码即可获取页码数。</p><pre><code class="python">url_re = re.compile(&#39;#currentPage.*\+.*\+\&#39;(\d+)\&#39;&#39;)page_cnt = url_re.search(html).group(1)</code></pre><p>从每个机构首页获取页码数后，便可<code>for</code>循环修改网址参数中的<code>currentPage</code>，逐页获取机构信息。</p><h3 id="获取当前页面从业人员信息"><a href="#获取当前页面从业人员信息" class="headerlink" title="获取当前页面从业人员信息"></a>获取当前页面从业人员信息</h3><p><img src="/assets/spider/cfachina/personinfo_table.png" alt="get personinfo"></p><p>针对如上图所示的一个特定信息页时，人员信息被存放于一个表中，除了固定的表头信息外，人员信息均被包含在一个带有<code>id</code>的<code>tr</code>标签中，所以使用<code>BeautifulSoup</code>可以很容易提取出页面内所有人员信息。</p><pre><code class="python">soup.find_all(&#39;tr&#39;, id=True)</code></pre><h2 id="确定爬取方案"><a href="#确定爬取方案" class="headerlink" title="确定爬取方案"></a>确定爬取方案</h2><p>一般的想法当然是逐页爬取主页信息，然后获取每页所有机构对应的网页链接，进而继续爬取每个机构信息。</p><p>但是由于该网站的机构信息网址具有明显的规律，我们根据每个机构的编号便可直接得到每个机构每个信息页面的网址。所以具体爬取方案如下：</p><ol><li>将所有<strong>机构编号</strong>网址存入队列<code>url_queue</code></li><li>新建生产者线程<code>SpiderThread</code>完成抓取任务<ul><li>循环从队列<code>url_queue</code>中读取一个编号，生成机构首页网址，使用<code>requests</code>抓取之</li><li>从抓取结果中获取页码数量，若为0，则返回该线程第1步</li><li>循环爬取当前机构剩余页面</li><li>将页面信息存入队列<code>html_queue</code></li></ul></li><li>新建消费者线程<code>DatamineThread</code>完成数据清洗任务<ul><li>循环从队列<code>html_queue</code>中读取一组页面信息</li><li>使用<code>BeautifulSoup</code>提取页面中的从业人员信息</li><li>将信息以二维数组形式存储，最后交由数据存储类<code>Storage</code>存入本地文件</li></ul></li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="生成者SpiderThread"><a href="#生成者SpiderThread" class="headerlink" title="生成者SpiderThread"></a>生成者<code>SpiderThread</code></h3><p>爬虫线程先从队列获取一个机构编号，生成机构首页网址并进行爬取，接着判断机构页面数量是否为0，如若不为0则继续获取机构名称，并根据页面数循环爬取剩余页面，将原始html数据以如下<code>dict</code>格式存入队列<code>html_queue</code>:</p><pre><code class="json">{    &#39;name&#39;: mechanismId_mechanismName,    &#39;num&#39;: currentPage,    &#39;content&#39;: html}</code></pre><p>爬虫产生的数据队列<code>html_queue</code>将由数据清洗线程进行处理，下面是爬虫线程的主程序，整个线程代码请看后面的<a href="#源码">源码</a>。</p><pre><code class="python">def run(self):    while True:        mechanism_id = &#39;G0&#39; + self.url_queue.get()        # the first page&#39;s url        url = self.__get_url(mechanism_id, 1)        html = self.grab(url)        page_cnt = self.url_re.search(html.text).group(1)        if page_cnt == &#39;0&#39;:            self.url_queue.task_done()            continue        soup = BeautifulSoup(html.text, &#39;html.parser&#39;)        mechanism_name = soup.find(&#39;&#39;, {&#39;class&#39;:&#39;gst_title&#39;}).find_all(&#39;a&#39;)[2].get_text()        print(&#39;\nGrab Thread: get %s - %s with %s pages\n&#39; % (mechanism_id, mechanism_name, page_cnt))        # put data into html_queue        self.html_queue.put({&#39;name&#39;:&#39;%s_%s&#39; % (mechanism_id, mechanism_name), &#39;num&#39;:1, &#39;content&#39;:html})        for i in range(2, int(page_cnt) + 1):            url = self.__get_url(mechanism_id, i)            html = self.grab(url)            self.html_queue.put({&#39;name&#39;:&#39;%s_%s&#39; % (mechanism_id, mechanism_name), &#39;num&#39;:i, &#39;content&#39;:html})        self.url_queue.task_done()</code></pre><h3 id="消费者DatamineThread"><a href="#消费者DatamineThread" class="headerlink" title="消费者DatamineThread"></a>消费者<code>DatamineThread</code></h3><p>数据清洗线程比较简单，就是从生产者提供的数据队列<code>html_queue</code>逐一提取<code>html</code>数据，然后从<code>html</code>数据中提取从业人员信息，以二维数组形式存储，最后交由存储模块<code>Storage</code>完成数据存储工作。</p><pre><code class="python">class DatamineThread(Thread):    &quot;&quot;&quot;Parse data from html&quot;&quot;&quot;    def __init__(self, html_queue, filetype):        Thread.__init__(self)        self.html_queue = html_queue        self.filetype = filetype    def __datamine(self, data):        &#39;&#39;&#39;Get data from html content&#39;&#39;&#39;        soup = BeautifulSoup(data[&#39;content&#39;].text, &#39;html.parser&#39;)        infos = []        for info in soup.find_all(&#39;tr&#39;, id=True):            items = []            for item in info.find_all(&#39;td&#39;):                items.append(item.get_text())            infos.append(items)        return infos    def run(self):        while True:            data = self.html_queue.get()            print(&#39;Datamine Thread: get %s_%d&#39; % (data[&#39;name&#39;], data[&#39;num&#39;]))            store = Storage(data[&#39;name&#39;], self.filetype)            store.save(self.__datamine(data))            self.html_queue.task_done()</code></pre><h3 id="数据存储Storage"><a href="#数据存储Storage" class="headerlink" title="数据存储Storage"></a>数据存储<code>Storage</code></h3><p>我写了两类文件格式的存储函数，<code>write_txt</code>, <code>write_excel</code>，分别对应<code>txt</code>,<code>excel</code>文件。实际存储时由调用方确定文件格式。</p><pre><code class="python">def save(self, data):    {        &#39;.txt&#39;: self.write_txt,        &#39;.xls&#39;: self.write_excel    }.get(self.filetype)(data)</code></pre><h4 id="存入txt文件"><a href="#存入txt文件" class="headerlink" title="存入txt文件"></a>存入txt文件</h4><p>存入<code>txt</code>文件是比较简单的，就是以附加(<code>a</code>)形式打开文件，写入数据，关闭文件。其中，文件名称由调用方提供。写入数据时，每个人员信息占用一行，以制表符<code>\t</code>分隔。</p><pre><code class="python">def write_txt(self, data):    &#39;&#39;&#39;Write data to txt file&#39;&#39;&#39;    fid = open(self.path, &#39;a&#39;, encoding=&#39;utf-8&#39;)    # insert the header of table    if not os.path.getsize(self.path):        fid.write(&#39;\t&#39;.join(self.table_header) + &#39;\n&#39;)    for info in data:        fid.write(&#39;\t&#39;.join(info) + &#39;\n&#39;)    fid.close()</code></pre><h4 id="存入Excel文件"><a href="#存入Excel文件" class="headerlink" title="存入Excel文件"></a>存入Excel文件</h4><p>存入<code>Excel</code>文件还是比较繁琐的，由于经验不多，选用的是<code>xlwt</code>, <code>xlrd</code>和<code>xlutils</code>库。说实话，这3个库真心不大好用，勉强完成任务而已。为什么这么说，且看：</p><ol><li>修改文件麻烦：<code>xlwt</code>只能写,<code>xlrd</code>只能读，需要<code>xlutils</code>的<code>copy</code>函数将<code>xlrd</code>读取的数据复制到内存，再用<code>xlwt</code>修改</li><li>只支持<code>.xls</code>文件：<code>.xlsx</code>经读写也会变成<code>.xls</code>格式</li><li>表格样式易变：只要重新写入文件，表格样式必然重置</li></ol><p>所以后续我肯定会再学学其它的<code>excel</code>库，当然，当前解决方案暂时还用这三个。代码如下：</p><pre><code class="python">def write_excel(self, data):    &#39;&#39;&#39;write data to excel file&#39;&#39;&#39;    if not os.path.exists(self.path):        header_style = xlwt.easyxf(&#39;font:name 楷体, color-index black, bold on&#39;)        wb = xlwt.Workbook(encoding=&#39;utf-8&#39;)        ws = wb.add_sheet(&#39;Data&#39;)        # insert the header of table        for i in range(len(self.table_header)):            ws.write(0, i, self.table_header[i], header_style)    else:        rb = open_workbook(self.path)        wb = copy(rb)        ws = wb.get_sheet(0)    # write data    offset = len(ws.rows)    for i in range(0, len(data)):        for j in range(0, len(data[0])):            ws.write(offset + i, j, data[i][j])    # When use xlutils.copy.copy function to copy data from exist .xls file,    # it will loss the origin style, so we need overwrite the width of column,    # maybe there some other good solution, but I have not found yet.    for i in range(len(self.table_header)):        ws.col(i).width = 256 * (10, 10, 15, 20, 50, 20, 15)[i]    # save to file    while True:        try:            wb.save(self.path)            break        except PermissionError as e:            print(&#39;{0} error: {1}&#39;.format(self.path, e.strerror))            time.sleep(5)        finally:            pass</code></pre><p><strong>说明：</strong></p><ol><li>一个文件对应一个机构的数据，需要多次读取和写入，所以需要计算文件写入时的行数偏移量<code>offset</code>，即当前文件已包含数据的行数</li><li>当被写入文件被人为打开时，会出现<code>PermissionError</code>异常，可以在捕获该异常然后提示错误信息，并定时等待直到文件被关闭。</li></ol><h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><p>主函数用于创建和启动生产者线程和消费者线程，同时为生产者线程提供机构编号队列。</p><pre><code class="python">url_queue = queue.Queue()html_queue = queue.Queue()def main():    for i in range(1001, 1199):        url_queue.put(str(i))    # create and start a spider thread    st = SpiderThread(url_queue, html_queue)    st.setDaemon(True)    st.start()    # create and start a datamine thread    dt = DatamineThread(html_queue, &#39;.xls&#39;)    dt.setDaemon(True)    dt.start()    # wait on the queue until everything has been processed    url_queue.join()    html_queue.join()</code></pre><p>从主函数可以看到，两个队列都调用了<code>join</code>函数，用于阻塞，直到对应队列为空为止。要注意的是，队列操作中，<strong>每个出队操作<code>queue.get()</code>需要对应一个<code>queue.task_done()</code>操作</strong>，否则会出现队列数据已全部处理完，但主线程仍在执行的情况。</p><p>至此，爬虫的主要代码便讲解完了，下面是完整源码。</p><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><pre><code class="python">#!/usr/bin/python3# -*-coding:utf-8-*-import queuefrom threading import Threadimport requestsimport refrom bs4 import BeautifulSoupimport osimport platformimport xlwtfrom xlrd import open_workbookfrom xlutils.copy import copyimport time# url format ↓# http://www.cfachina.org/cfainfo/organbaseinfoOneServlet?organid=+G01001+&amp;currentPage=1&amp;pageSize=20&amp;selectType=personinfo&amp;all=undefined# organid: +G01001+, +G01002+, +G01003+, ...# currentPage: 1, 2, 3, ...# pageSize: 20(default)# # Algorithm design:# 2 threads with 2 queues# Thread-1, get first page url, then get page_num and mechanism_name from first page# Thread-2, parse html file and get data from it, then output data to local file# url_queue data -&gt; &#39;url&#39;  # first url of each mechanism# html_queue data -&gt; {&#39;name&#39;:&#39;mechanism_name&#39;, &#39;html&#39;:data}url_queue = queue.Queue()html_queue = queue.Queue()class SpiderThread(Thread):    &quot;&quot;&quot;Threaded Url Grab&quot;&quot;&quot;    def __init__(self, url_queue, html_queue):        Thread.__init__(self)        self.url_queue = url_queue        self.html_queue = html_queue        self.page_size = 20        self.url_re = re.compile(&#39;#currentPage.*\+.*\+\&#39;(\d+)\&#39;&#39;)        self.headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&#39;}    def __get_url(self, mechanism_id, current_page):        return &#39;http://www.cfachina.org/cfainfo/organbaseinfoOneServlet?organid=+%s+&amp;currentPage=%d&amp;pageSize=%d&amp;selectType=personinfo&amp;all=undefined&#39; \        % (mechanism_id, current_page, self.page_size)    def grab(self, url):        &#39;&#39;&#39;Grab html of url from web&#39;&#39;&#39;        while True:            try:                html = requests.get(url, headers=self.headers, timeout=20)                if html.status_code == 200:                    break            except requests.exceptions.ConnectionError as e:                print(url + &#39; Connection error, try again...&#39;)            except requests.exceptions.ReadTimeout as e:                print(url + &#39; Read timeout, try again...&#39;)            except Exception as e:                print(str(e))            finally:                pass        return html    def run(self):        &#39;&#39;&#39;Grab all htmls of mechanism one by one        Steps:            1. grab first page of each mechanism from url_queue            2. get number of pages and mechanism name from first page            3. grab all html file of each mechanism            4. push all html to html_queue        &#39;&#39;&#39;        while True:            mechanism_id = &#39;G0&#39; + self.url_queue.get()            # the first page&#39;s url            url = self.__get_url(mechanism_id, 1)            html = self.grab(url)            page_cnt = self.url_re.search(html.text).group(1)            if page_cnt == &#39;0&#39;:                self.url_queue.task_done()                continue            soup = BeautifulSoup(html.text, &#39;html.parser&#39;)            mechanism_name = soup.find(&#39;&#39;, {&#39;class&#39;:&#39;gst_title&#39;}).find_all(&#39;a&#39;)[2].get_text()            print(&#39;\nGrab Thread: get %s - %s with %s pages\n&#39; % (mechanism_id, mechanism_name, page_cnt))            # put data into html_queue            self.html_queue.put({&#39;name&#39;:&#39;%s_%s&#39; % (mechanism_id, mechanism_name), &#39;num&#39;:1, &#39;content&#39;:html})            for i in range(2, int(page_cnt) + 1):                url = self.__get_url(mechanism_id, i)                html = self.grab(url)                self.html_queue.put({&#39;name&#39;:&#39;%s_%s&#39; % (mechanism_id, mechanism_name), &#39;num&#39;:i, &#39;content&#39;:html})            self.url_queue.task_done()class DatamineThread(Thread):    &quot;&quot;&quot;Parse data from html&quot;&quot;&quot;    def __init__(self, html_queue, filetype):        Thread.__init__(self)        self.html_queue = html_queue        self.filetype = filetype    def __datamine(self, data):        &#39;&#39;&#39;Get data from html content&#39;&#39;&#39;        soup = BeautifulSoup(data[&#39;content&#39;].text, &#39;html.parser&#39;)        infos = []        for info in soup.find_all(&#39;tr&#39;, id=True):            items = []            for item in info.find_all(&#39;td&#39;):                items.append(item.get_text())            infos.append(items)        return infos    def run(self):        while True:            data = self.html_queue.get()            print(&#39;Datamine Thread: get %s_%d&#39; % (data[&#39;name&#39;], data[&#39;num&#39;]))            store = Storage(data[&#39;name&#39;], self.filetype)            store.save(self.__datamine(data))            self.html_queue.task_done()class Storage():    def __init__(self, filename, filetype):        self.filetype = filetype        self.filename = filename + filetype        self.table_header = (&#39;姓名&#39;, &#39;性别&#39;, &#39;从业资格号&#39;, &#39;投资咨询从业证书号&#39;, &#39;任职部门&#39;, &#39;职务&#39;, &#39;任现职时间&#39;)        self.path = self.__get_path()    def __get_path(self):        path = {            &#39;Windows&#39;: &#39;D:/litreily/Documents/python/cfachina&#39;,            &#39;Linux&#39;: &#39;/mnt/d/litreily/Documents/python/cfachina&#39;        }.get(platform.system())        if not os.path.isdir(path):            os.makedirs(path)        return &#39;%s/%s&#39; % (path, self.filename)    def write_txt(self, data):        &#39;&#39;&#39;Write data to txt file&#39;&#39;&#39;        fid = open(self.path, &#39;a&#39;, encoding=&#39;utf-8&#39;)        # insert the header of table        if not os.path.getsize(self.path):            fid.write(&#39;\t&#39;.join(self.table_header) + &#39;\n&#39;)        for info in data:            fid.write(&#39;\t&#39;.join(info) + &#39;\n&#39;)        fid.close()    def write_excel(self, data):        &#39;&#39;&#39;write data to excel file&#39;&#39;&#39;        if not os.path.exists(self.path):            header_style = xlwt.easyxf(&#39;font:name 楷体, color-index black, bold on&#39;)            wb = xlwt.Workbook(encoding=&#39;utf-8&#39;)            ws = wb.add_sheet(&#39;Data&#39;)            # insert the header of table            for i in range(len(self.table_header)):                ws.write(0, i, self.table_header[i], header_style)        else:            rb = open_workbook(self.path)            wb = copy(rb)            ws = wb.get_sheet(0)        # write data        offset = len(ws.rows)        for i in range(0, len(data)):            for j in range(0, len(data[0])):                ws.write(offset + i, j, data[i][j])        # When use xlutils.copy.copy function to copy data from exist .xls file,        # it will loss the origin style, so we need overwrite the width of column,        # maybe there some other good solution, but I have not found yet.        for i in range(len(self.table_header)):            ws.col(i).width = 256 * (10, 10, 15, 20, 50, 20, 15)[i]        # save to file        while True:            try:                wb.save(self.path)                break            except PermissionError as e:                print(&#39;{0} error: {1}&#39;.format(self.path, e.strerror))                time.sleep(5)            finally:                pass    def save(self, data):        &#39;&#39;&#39;Write data to local file.        According filetype to choose function to save data, filetype can be &#39;.txt&#39;         or &#39;.xls&#39;, but &#39;.txt&#39; type is saved more faster then &#39;.xls&#39; type        Args:            data: a 2d-list array that need be save        &#39;&#39;&#39;        {            &#39;.txt&#39;: self.write_txt,            &#39;.xls&#39;: self.write_excel        }.get(self.filetype)(data)def main():    for i in range(1001, 1199):        url_queue.put(str(i))    # create and start a spider thread    st = SpiderThread(url_queue, html_queue)    st.setDaemon(True)    st.start()    # create and start a datamine thread    dt = DatamineThread(html_queue, &#39;.xls&#39;)    dt.setDaemon(True)    dt.start()    # wait on the queue until everything has been processed    url_queue.join()    html_queue.join()if __name__ == &#39;__main__&#39;:    main()</code></pre><h2 id="爬取测试"><a href="#爬取测试" class="headerlink" title="爬取测试"></a>爬取测试</h2><p><img src="/assets/spider/cfachina/spider.png" alt="spider"></p><p><img src="/assets/spider/cfachina/save_txt.png" alt="save to txt"></p><p><img src="/assets/spider/cfachina/save_xls.png" alt="save to excel"></p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><ul><li>测试发现，写入<code>txt</code>的速度明显高于写入<code>excel</code>的速度</li><li>如果将页面网址中的<code>pageSize</code>修改为<code>1000</code>或更大，则可以一次性获取某机构的所有从业人员信息，而不用逐页爬取，效率可以大大提高。</li><li>该爬虫已托管至<a href="https://github.com/Litreily/Python-demos" target="_blank" rel="noopener">github Python-demos</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;应一位金融圈的朋友所托，帮忙写个爬虫，帮他爬取&lt;a href=&quot;http://www.cfachina.org/cfainfo/organbaseinfoServlet?all=personinfo#&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;中国期货
      
    
    </summary>
    
      <category term="Python" scheme="http://www.litreily.top/categories/Python/"/>
    
    
      <category term="spider" scheme="http://www.litreily.top/tags/spider/"/>
    
      <category term="queue" scheme="http://www.litreily.top/tags/queue/"/>
    
      <category term="xlwt" scheme="http://www.litreily.top/tags/xlwt/"/>
    
  </entry>
  
  <entry>
    <title>Python网络爬虫2 - 爬取新浪微博用户图片</title>
    <link href="http://www.litreily.top/2018/04/10/sina/"/>
    <id>http://www.litreily.top/2018/04/10/sina/</id>
    <published>2018-04-10T11:54:08.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p>其实，新浪微博用户图片爬虫是我学习<code>python</code>以来写的第一个爬虫，只不过当时懒，后来爬完<code>Lofter</code>后觉得有必要总结一下，所以就有了第一篇爬虫博客。现在暂时闲下来了，准备把新浪的这个也补上。</p><p>言归正传，既然选择爬新浪微博，那当然是有需求的，这也是学习的主要动力之一，没错，就是美图。<code>sina</code>用户多数微博都是包含图片的，而且是组图居多，单个图片的较少。</p><p>为了避免侵权，本文以本人微博<a href="https://weibo.com/litreily" target="_blank" rel="noopener">litreily</a>为例说明整个爬取过程，虽然图片较少，质量较低，但爬取方案是绝对ok的，使用时只要换个用户ID就可以了。</p><h2 id="分析sina站点"><a href="#分析sina站点" class="headerlink" title="分析sina站点"></a>分析sina站点</h2><h3 id="获取用户ID"><a href="#获取用户ID" class="headerlink" title="获取用户ID"></a>获取用户ID</h3><p>在爬取前，我们需要知道的是每个用户都有一个用户名，而一个用户名又对应一个唯一的整型数字ID，类似于学生的学号，本人的是<code>2657006573</code>。至于怎么根据用户名去获取ID，有以下两种方法：</p><ol><li>进入待爬取用户主页，在浏览器网址栏中即可看到一串数据，那就是用户ID</li><li><code>Ctrl-U</code>查看待爬取用户的源码，搜索<code>&quot;uid</code>，注意是<strong>双引号</strong></li></ol><p>其实是可以在已知用户名的情况下通过爬虫自动获取到<code>uid</code>的，但是我当时初学<code>python</code>，并没有考虑充分，所以后面的源码是以用户ID作为输入参数的。</p><h3 id="图片存储参数解析"><a href="#图片存储参数解析" class="headerlink" title="图片存储参数解析"></a>图片存储参数解析</h3><p>用户所有的图片都被存放至这样的路径下，真的是<strong>所有图片</strong>哦！！！</p><pre><code class="yml">https://weibo.cn/{uid}/profile?filter={filter_type}&amp;page={page_num}# examplehttps://weibo.cn/2657006573/profile?filter=0&amp;page=1uid: 2657006573filter_type: 0page_num: 1</code></pre><p>注意，是<code>weibo.cn</code>而不是<code>weibo.com</code>，至于我是怎么找到这个页面的，说实话，我也忘了。。。</p><p>链接中包含3个参数，<code>uid</code>, <code>filter_mode</code> 以及 <code>page_num</code>。其中，<code>uid</code>就是前面提及的用户ID，<code>page_num</code>也很好理解，就是分页的当前页数，从1开始增加，那么，这个<code>filter_mode</code>是什么呢？</p><p>不着急，我们先来看看页面↓</p><p><img src="/assets/spider/sina/filter_mode.png" alt="filter mode of pictures"></p><p>可以看到，滤波类型<code>filter_mode</code>指的就是筛选条件，一共三个：</p><ol><li>filter=0 全部微博（包含纯文本微博，转载微博）</li><li>filter=1 原创微博（包含纯文本微博）</li><li>filter=2 图片微博（必须含有图片，包含转载）</li></ol><p>我通常会选择<strong>原创</strong>，因为我并不希望爬取结果中包含转载微博中的图片。当然，大家依照自己的需要选择即可。</p><h3 id="图链解析"><a href="#图链解析" class="headerlink" title="图链解析"></a>图链解析</h3><p>好了，参数来源都知道了，我们回过头看看这个网页。页面是不是感觉就是个空架子？毫无css痕迹，没关系，新浪本来就没打算把这个页面主动呈现给用户。但对于爬虫而言，这却是极好的，为什么这么说？原因如下：</p><ol><li>图片齐全，没有遗漏，就是个可视化的数据库</li><li>样式少，页面简单，省流量，爬取快</li><li>静态网页，分页存储，所见即所得</li><li>源码包含了所有微博的<strong>首图</strong>和<strong>组图链接</strong></li></ol><p>这样的网页用来练手再合适不过。但要注意的是上面第4点，什么是<strong>首图</strong>和<strong>组图链接</strong>呢，很好理解。每篇博客可能包含多张图片，那就是<strong>组图</strong>，但该页面只显示博客的第一张图片，即所谓的<strong>首图</strong>，<strong>组图链接</strong>指向的是存储着该组图所有图片的网址。</p><p>由于本人微博没组图，所以此处以刘亦菲微博为例，说明单图及组图的图链格式</p><p><img src="/assets/spider/sina/pictures.png" alt="pictures"></p><p>图中的上面一篇微博只有一张图片，可以轻易获取到原图链接，注意是<strong>原图</strong>，因为我们在页面能看到的是缩略图，但要爬取的当然是<strong>原图</strong>啦。</p><p>图中下面的微博包含组图，在图片右侧的<code>Chrome</code>开发工具可以看到组图链接。</p><p><a href="https://weibo.cn/mblog/picAll/FCQefgeAr?rl=2" target="_blank" rel="noopener">https://weibo.cn/mblog/picAll/FCQefgeAr?rl=2</a> </p><p>打开组图链接，可以看到图片如下图所示：</p><p><img src="/assets/spider/sina/picture_url.png" alt="picture&#39;s url"></p><p>可以看到缩略图链接以及原图链接，然后我们点击<strong>原图</strong>看一下。</p><p><img src="/assets/spider/sina/picture_source.png" alt="picture&#39;s origin url"></p><p>可以发现，弹出页面的链接与上图显示的不同，但与上图中的缩略图链接极为相似。它们分别是：</p><ol><li>缩略图：<a href="http://ww1.sinaimg.cn/thumb180/c260f7ably1fn4vd7ix0qj20rs1aj1kx.jpg" target="_blank" rel="noopener">http://ww1.sinaimg.cn/thumb180/c260f7ably1fn4vd7ix0qj20rs1aj1kx.jpg</a></li><li>原图： <a href="http://wx1.sinaimg.cn/large/c260f7ably1fn4vd7ix0qj20rs1aj1kx.jpg" target="_blank" rel="noopener">http://wx1.sinaimg.cn/large/c260f7ably1fn4vd7ix0qj20rs1aj1kx.jpg</a></li></ol><p>可以看出，只是一个<code>thumb180</code>和<code>large</code>的区别。既然发现了规律，那就好办多了，我们只要知道缩略图的网址，就可以将域名后的第一级子域名替换成<code>large</code>就可以了，而不用获取<strong>原图</strong>链接再跳转一次。</p><p>而且，多次尝试可以发现组图链接及缩略图链接满足正则表达式：</p><pre><code class="python"># 1. 组图链接：imglist_reg = r&#39;href=&quot;(https://weibo.cn/mblog/picAll/.{9}\?rl=2)&quot;&#39;# 2. 缩略图img_reg = r&#39;src=&quot;(http://w.{2}\.sinaimg.cn/(.{6,8})/.{32,33}.(jpg|gif))&quot;&#39;</code></pre><p>到此，新浪微博的解析过程就结束了，图链的格式以及获取方式也都清楚了。下面就可以设计方案进行爬取了。</p><h2 id="确定爬取方案"><a href="#确定爬取方案" class="headerlink" title="确定爬取方案"></a>确定爬取方案</h2><p>根据解析结果，很容易制定出以下爬取方案：</p><ol><li>给定微博用户名<code>litreily</code></li><li>进入待爬取用户主页，即可从网址中获取<code>uid: 2657006573</code></li><li>获取本人登录微博后的<code>cookies</code>（请求报文需要用到<code>cookies</code>）</li><li>逐一爬取 <a href="https://weibo.cn/2657006573/profile?filter=0&amp;page={1,2,3,...}" target="_blank" rel="noopener">https://weibo.cn/2657006573/profile?filter=0&amp;page={1,2,3,...}</a> </li><li>解析每一页的源码，获取单图链接及组图链接，<ul><li>单图：直接获取该图缩略图链接；</li><li>组图：爬取组图链接，循环获取组图页面所有图片的缩略图链接</li></ul></li><li>循环将第5步获取到的图链替换为原图链接，并下载至本地</li><li>重复第4-6步，直至没有图片</li></ol><h3 id="获取cookies"><a href="#获取cookies" class="headerlink" title="获取cookies"></a>获取cookies</h3><p>针对以上方案，其中有几个重点内容，其一就是<code>cookies</code>的获取，我暂时还没学怎么自动获取<code>cookies</code>，所以目前是登录微博后手动获取的。</p><p><img src="/assets/spider/sina/cookies.png" alt="get cookies"></p><h3 id="下载网页"><a href="#下载网页" class="headerlink" title="下载网页"></a>下载网页</h3><p>下载网页用的是<code>python3</code>自带的<code>urllib</code>库，当时没学<code>requests</code>，以后可能也很少用<code>urllib</code>了。</p><pre><code class="python">def _get_html(url, headers):    try:        req = urllib.request.Request(url, headers = headers)        page = urllib.request.urlopen(req)        html = page.read().decode(&#39;UTF-8&#39;)    except Exception as e:        print(&quot;get %s failed&quot; % url)        return None    return html</code></pre><h3 id="获取存储路径"><a href="#获取存储路径" class="headerlink" title="获取存储路径"></a>获取存储路径</h3><p>由于我是在<code>win10</code>下编写的代码，但是个人比较喜欢用<code>bash</code>，所以图片的存储路径有以下两种格式，<code>_get_path</code>函数会自动判断当前操作系统的类型，然后选择相应的路径。</p><pre><code class="python">def _get_path(uid):    path = {        &#39;Windows&#39;: &#39;D:/litreily/Pictures/python/sina/&#39; + uid,        &#39;Linux&#39;: &#39;/mnt/d/litreily/Pictures/python/sina/&#39; + uid    }.get(platform.system())    if not os.path.isdir(path):        os.makedirs(path)    return path</code></pre><p>幸好<code>windows</code>是兼容<code>linux</code>系统的斜杠符号的，不然程序中的相对路径替换还挺麻烦。</p><h3 id="下载图片"><a href="#下载图片" class="headerlink" title="下载图片"></a>下载图片</h3><p>由于选用的<code>urllib</code>库，所以下载图片就使用<code>urllib.request.urlretrieve</code>了</p><pre><code class="python"># image url of one page is saved in imgurlsfor img in imgurls:    imgurl = img[0].replace(img[1], &#39;large&#39;)    num_imgs += 1    try:        urllib.request.urlretrieve(imgurl, &#39;{}/{}.{}&#39;.format(path, num_imgs, img[2]))        # display the raw url of images        print(&#39;\t%d\t%s&#39; % (num_imgs, imgurl))    except Exception as e:        print(str(e))        print(&#39;\t%d\t%s failed&#39; % (num_imgs, imgurl))</code></pre><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><p>其它细节详见源码</p><pre><code class="python">#!/usr/bin/python3# -*- coding:utf-8 -*-# author: litreily# date: 2018.02.05&quot;&quot;&quot;Capture pictures from sina-weibo with user_id.&quot;&quot;&quot;import reimport osimport platformimport urllibimport urllib.requestfrom bs4 import BeautifulSoupdef _get_path(uid):    path = {        &#39;Windows&#39;: &#39;D:/litreily/Pictures/python/sina/&#39; + uid,        &#39;Linux&#39;: &#39;/mnt/d/litreily/Pictures/python/sina/&#39; + uid    }.get(platform.system())    if not os.path.isdir(path):        os.makedirs(path)    return pathdef _get_html(url, headers):    try:        req = urllib.request.Request(url, headers = headers)        page = urllib.request.urlopen(req)        html = page.read().decode(&#39;UTF-8&#39;)    except Exception as e:        print(&quot;get %s failed&quot; % url)        return None    return htmldef _capture_images(uid, headers, path):    filter_mode = 1      # 0-all 1-original 2-pictures    num_pages = 1    num_blogs = 0    num_imgs = 0    # regular expression of imgList and img    imglist_reg = r&#39;href=&quot;(https://weibo.cn/mblog/picAll/.{9}\?rl=2)&quot;&#39;    imglist_pattern = re.compile(imglist_reg)    img_reg = r&#39;src=&quot;(http://w.{2}\.sinaimg.cn/(.{6,8})/.{32,33}.(jpg|gif))&quot;&#39;    img_pattern = re.compile(img_reg)    print(&#39;start capture picture of uid:&#39; + uid)    while True:        url = &#39;https://weibo.cn/%s/profile?filter=%s&amp;page=%d&#39; % (uid, filter_mode, num_pages)        # 1. get html of each page url        html = _get_html(url, headers)        # 2. parse the html and find all the imgList Url of each page        soup = BeautifulSoup(html, &quot;html.parser&quot;)        # &lt;div class=&quot;c&quot; id=&quot;M_G4gb5pY8t&quot;&gt;&lt;div&gt;        blogs = soup.body.find_all(attrs={&#39;id&#39;:re.compile(r&#39;^M_&#39;)}, recursive=False)        num_blogs += len(blogs)        imgurls = []                for blog in blogs:            blog = str(blog)            imglist_url = imglist_pattern.findall(blog)            if not imglist_url:                # 2.1 get img-url from blog that have only one pic                imgurls += img_pattern.findall(blog)            else:                # 2.2 get img-urls from blog that have group pics                html = _get_html(imglist_url[0], headers)                imgurls += img_pattern.findall(html)        if not imgurls:            print(&#39;capture complete!&#39;)            print(&#39;captured pages:%d, blogs:%d, imgs:%d&#39; % (num_pages, num_blogs, num_imgs))            print(&#39;directory:&#39; + path)            break        # 3. download all the imgs from each imgList        print(&#39;PAGE %d with %d images&#39; % (num_pages, len(imgurls)))        for img in imgurls:            imgurl = img[0].replace(img[1], &#39;large&#39;)            num_imgs += 1            try:                urllib.request.urlretrieve(imgurl, &#39;{}/{}.{}&#39;.format(path, num_imgs, img[2]))                # display the raw url of images                print(&#39;\t%d\t%s&#39; % (num_imgs, imgurl))            except Exception as e:                print(str(e))                print(&#39;\t%d\t%s failed&#39; % (num_imgs, imgurl))        num_pages += 1        print(&#39;&#39;)def main():    # uids = [&#39;2657006573&#39;,&#39;2173752092&#39;,&#39;3261134763&#39;,&#39;2174219060&#39;]    uid = &#39;2657006573&#39;    path = _get_path(uid)    # cookie is form the above url-&gt;network-&gt;request headers    cookies = &#39;&#39;    headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36&#39;,            &#39;Cookie&#39;: cookies}    # capture imgs from sina    _capture_images(uid, headers, path)if __name__ == &#39;__main__&#39;:    main()</code></pre><p>使用时记得修改<code>main</code>函数中的<code>cookies</code>和<code>uid</code>！</p><h2 id="爬取测试"><a href="#爬取测试" class="headerlink" title="爬取测试"></a>爬取测试</h2><p><img src="/assets/spider/sina/capturer_litreily.png" alt="capture litreily"></p><p><img src="/assets/spider/sina/capturer_litreily_end.png" alt="capture litreily end"></p><p><img src="/assets/spider/sina/captured_pictures.png" alt="captured pictures"></p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><ul><li>该爬虫已存放至开源项目<a href="https://github.com/Litreily/cfachina_spider" target="_blank" rel="noopener">capturer</a>，欢迎交流</li><li>由于是首个爬虫，所以许多地方有待改进，相对的<a href="http://www.litreily.top/2018/03/17/lofter/">LOFTER爬虫</a>就更娴熟写了</li><li>目前没有发现新浪微博有明显的反爬措施，但还是按需索取为好</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;其实，新浪微博用户图片爬虫是我学习&lt;code&gt;python&lt;/code&gt;以来写的第一个爬虫，只不过当时懒，后来爬完&lt;code&gt;Lofter&lt;/code&gt;后觉得有必要总结一下，所以就有了第一篇爬虫博客。现在暂时闲下来了，准备把新浪的这个也补上。&lt;/p&gt;
&lt;p&gt;言归正传，既然选
      
    
    </summary>
    
      <category term="Python" scheme="http://www.litreily.top/categories/Python/"/>
    
    
      <category term="tools" scheme="http://www.litreily.top/tags/tools/"/>
    
      <category term="spider" scheme="http://www.litreily.top/tags/spider/"/>
    
      <category term="sina" scheme="http://www.litreily.top/tags/sina/"/>
    
  </entry>
  
  <entry>
    <title>Python网络爬虫1 - 爬取网易LOFTER图片</title>
    <link href="http://www.litreily.top/2018/03/17/lofter/"/>
    <id>http://www.litreily.top/2018/03/17/lofter/</id>
    <published>2018-03-17T03:02:17.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p><code>LOFTER</code>是网易出品的优质轻博客，灵感源于国外的<code>tumblr</code>，但比之更加文艺，更加本地化。本人非常喜欢<code>LOFTER</code>的UI设计，以及其中的优质用户和内容，似乎网易并不擅长推广，所以受众并不广泛。这都是题外话，本文主要记录作者近期学习<code>python3</code>并用之爬取<code>LOFTER</code>用户图片的过程和成果，与大家交流分享。</p><blockquote><p>本文将以本人<a href="http://litreily.lofter.com" target="_blank" rel="noopener">litreily</a>博客为例说明整个爬取过程</p></blockquote><h2 id="分析LOFTER站点"><a href="#分析LOFTER站点" class="headerlink" title="分析LOFTER站点"></a>分析LOFTER站点</h2><p>在爬取站点之前，首先需要分析站点的关键信息有哪些，如果给自己提问，可能会有以下问题：</p><ol><li>用户的主页网址格式是？</li><li>用户博客链接的格式是？</li><li>每篇博客内的图片链接的格式是？</li><li>不同用户的主页模板不同，是否可以按同样方法抓取博客信息？</li><li>用户的博客数量巨大，主页以什么方式分页？</li><li>有没有归档页面方便爬取（大多数网站都有归档页面）？</li></ol><p>当然，这些问题不是一下子就能想出来，可以在探索网页内容的过程逐步展开，并思考下一步该考虑的问题，下面针对各个问题对主页进行探索分析。</p><h3 id="主页信息"><a href="#主页信息" class="headerlink" title="主页信息"></a>主页信息</h3><blockquote><p>主页: http://[username].lofter.com</p></blockquote><p><img src="/assets/spider/lofter/litreily.png" alt="litreily"></p><p>从主页可以看到<a href="http://litreily.lofter.com/view" target="_blank" rel="noopener">归档</a>的链接，暂时不管。不同的用户，其主页所用模板不尽一致，<code>LOFTER</code>为提供了大量精美的主页模板，以满足不同用户的需求：</p><p><img src="/assets/spider/lofter/lofterTemplet.png" alt="lofter templet"></p><p>然而，正是因为所用模板不同，其网页内容格式也不同，这个从不同模板中图片的位置，大小，图片信息等就可以看出来。相同的资源，不同的展示方式，就好像同样一件艺术品，既可以摆放在玻璃框中，也可以悬挂在高空。</p><p>当然这不是本文重点，这里只是为了说明不同用户的主页信息展示不一样，会给爬虫爬取带来一定影响。</p><h3 id="分页信息"><a href="#分页信息" class="headerlink" title="分页信息"></a>分页信息</h3><p>点击主页尾部的下一页，可以跳转至下一页，除首页和末页外，都会有上一页和下一页的链接，这里就给了我们一个提示，我们可以先抓取首页信息，然后从中抓取到<strong>下一页</strong>的链接，然后不断获取<strong>下一页</strong>的博客信息。</p><p><img src="/assets/spider/lofter/pages.png" alt="pages"></p><p>或者更简单点，看网址栏中的网址格式:</p><blockquote><p>分页： <code>http://[username].lofter.com/?page=[pageNumber]&amp;t=[timeStamp]</code></p></blockquote><p>直接使用<code>for</code>循环修改<code>page</code>值，逐页爬取博客信息。这貌似是个不错的想法，好，假设这样可行，那我们来分析每一页的信息。</p><p><img src="/assets/spider/lofter/postLink.png" alt="post link"></p><p>如上图所示，首先找到博文永久链接<code>http://litreily.lofter.com/post/44fbca_1265bb3e</code></p><p>针对含有图片的某一篇博文，<strong>litreily</strong>所用模板中会出现两次博文链接（见图中红框标注的两处），倘若我们使用正则表达式:</p><pre><code class="python">re.findall(r&#39;&quot;http://.*lofter.com/post/[\w_]*&quot;&#39;, html)</code></pre><p>将对每篇博文匹配出两个一样的链接，这可不是我们想要的，那咋整，匹配完再把重复的删了？不至于这么麻烦，细看两处链接前后信息，可以看到两处链接的<code>class</code>属性不一致，好办了，咱改改正则：</p><pre><code class="python">re.findall(r&#39;&lt;a class=&quot;img&quot; href=&quot;(http://.*lofter.com/post/[\w_]*)&quot;&gt;&#39;, html)</code></pre><p>好像可以了，这不就可以按页抓取博客链接，然后接着分析每篇博文信息不就好了么。我原本就想这么干，可是当我查看了不同用户的排版以及相应的链接信息后，整个人都不好了，一千个用户就是一千个哈姆雷特啊。如果你发现有统一解析所有用户模板信息的方法，那肯定是你看的模板不够多。</p><p>所以呢，这条路是走不通了，至少我没再往这条路上走。打道回府，只不过重头再来，路漫漫其修远兮，吾将上下而求索。</p><p>靠，说了半天，原来走不通，那你说个毛线！！！淡定淡定，都是文明人，后面的风景很远，额不是，，，是很美，请耐心等待…</p><h3 id="归档页信息"><a href="#归档页信息" class="headerlink" title="归档页信息"></a>归档页信息</h3><blockquote><p>归档页：<code>http://[username].lofter.com/view</code></p></blockquote><p>好了，还记得前面说的<strong>归档</strong>吧，归档可是个好东西，它把所有博文都按日期归档，最主要的是，所有用户的归档页面都是同一个模板，不管大V小v还是普通老百姓，真的是一视同仁。剩下的问题就是<strong>如何从归档页抓取每篇博客的真实路径</strong>。</p><p><img src="/assets/spider/lofter/archive.png" alt="archive"></p><p>先来看看归档页面的结构吧，博客按月份归档，每篇博客仅显示首张图片缩略图或纯文本。然后<code>F12</code>打开调试工具，如下图所示，每个月份对应一个<code>&lt;div class=&quot;m-filecnt m-filecnt-1&quot;&gt;</code>这样的节点，每个月份节点包含了本月所有博客的入口信息，一篇博客对应一个<code>id</code>号，以及一个博客的<strong>相对路径</strong>。<code>id</code>神马的不用关心，重点就是这个<strong>相对路径</strong>，有了它不就有了博客的绝对路径了么。</p><ul><li>相对路径：”/post/44fbca_1265bb3e”</li><li>绝对路径：”http://[username].lofter.com/post/44fbca_1265bb3e”</li></ul><p><img src="/assets/spider/lofter/archive_html.png" alt="archive structure"></p><p>这样看来，那岂不是只要抓取这一个归档页面就可以抓到所有的博客路径了呢？呵呵，真的这么容易吗？显然不大可能，当我们下拉页面时，归档信息将动态加载刷新，没错，是动态的！！！意料之中的猝不及防</p><p>接着我在Chrome浏览器中<code>Ctrl+U</code>看了看网页的源码（太长这就不放图了），果然不出所料，动态数据在源码中是木有的，只有一个脚本在那静静的躺着，躺着，躺着。。。难道就要放弃了吗，当然不！<strong>只要是网络通信，就必然有请求包和响应包</strong></p><p>那现在的问题就是，动态网页的数据是如果获取到的？动态数据的<strong>真实请求</strong>是什么？抓包看看呗，打开浏览器调试工具中的<code>Network</code>，刷新归档页，看看页面加载过程，找到真实请求，这个很好找，这类请求的后缀一般不会是png,jpg,gif,js,css等，而且多半是<strong>POST</strong>包，并且会出现在一堆图片请求的前面。</p><p><img src="/assets/spider/lofter/post.png" alt="post"></p><p>好了，找到了，就是上面这货。现在归档数据请求的链接有了，确实是<strong>POST</strong>包，同样，请求包的头部信息<code>headers</code>和请求参数<code>request payload</code>也有了。</p><p><img src="/assets/spider/lofter/post_values.png" alt="request values"></p><p>现在的关键问题是，这些请求包中的参数都是干嘛的？经我多方尝试、猜测与观察，总结如下：</p><pre><code class="C#">callCount=1       # 固定scriptSessionId=${scriptSessionId}187   # 固定httpSessionId=    # 固定c0-scriptName=ArchiveBean       # 固定c0-methodName=getArchivePostByTime      # 固定c0-id=0           # 固定c0-param0=number:4520906        # 用户ID，可从用户主页源码获取c0-param1=number:1521342313224  # 时间戳，最最最关键参数！c0-param2=number:50       # 单次请求博客篇数，可以按需求修改c0-param3=boolean:false   # 固定batchId=822456            # 6位随机数，爬取时可以固定</code></pre><p>所以我们模拟请求包的时候就按这个来就可以了，至于时间戳怎么获取，请求包的<code>headers</code>如何确定，后面会有详述。</p><p>下面我们来看看请求后得到的响应包长啥样，look，就下面这个，看到没，<strong><code>permalink</code></strong>, 千呼万唤始出来啊，这不就是我们想要的博客固定路径了么。响应包并不是<code>html</code>文件，而是一组数据，我觉着归档页包含的那个脚本就是根据这个数据文件进而请求首张图片信息或文本信息的，当然这是我的猜测了，有兴趣的可以去看看那个脚本。</p><p><img src="/assets/spider/lofter/post_response.png" alt="post response"></p><p>有了这组数据，咱就可以获取每次请求得到的博客路径列表，进而逐一爬取博客内的图片链接了。</p><p>到此处为止，归档页的信息就分析完了，我们已经知道该发送怎样的请求包去获取归档数据，与此同时，我们也知道了从归档页如何获取每篇博客的真实路径。</p><p>下面就来看看当我们知道博客路径并抓取后，该如何获取每篇博客正文内的图片链接。</p><h3 id="博客页信息"><a href="#博客页信息" class="headerlink" title="博客页信息"></a>博客页信息</h3><blockquote><p>博客： <code>http://[username].lofter.com/post/******_********</code></p></blockquote><p>以上面获取的博客 <a href="http://litreily.lofter.com/post/33a459_1230cb50" target="_blank" rel="noopener">http://litreily.lofter.com/post/33a459_1230cb50</a> 为例，大部分博客内的图片都不止一张，这也是必须访问博客页本身的主要原因，好了照旧查看页面元素。</p><p><img src="/assets/spider/lofter/blog_pic.png" alt="blog pictures"></p><p>可以发现每篇博客内所有图片的大图链接都是上图框选中这样的，都有着同样的属性<code>bigimgsrc</code>，并且是博客页面唯一的。由于每篇博客源码内包含了该篇博客所有的图片链接，所以当我们获取了某篇博客的<code>html</code>文件后，便可以使用正则表达式获取所有图片链接。</p><p>至此，我们已经掌握了爬取<code>lofter</code>单用户博客图片所需的所有信息，是时候确定爬取方案了。</p><h2 id="确定爬取方案"><a href="#确定爬取方案" class="headerlink" title="确定爬取方案"></a>确定爬取方案</h2><p>首先，根据给定的<code>username</code>获取<code>uid</code>作为<code>POST</code>请求包数据中的一分子；然后，循环执行以下步骤直至全部爬取完成</p><ol><li>生成或更新归档页请求数据</li><li>模拟归档页面发送POST请求</li><li>解析响应数据并获取博客链接</li><li>逐一爬取博客内容</li><li>解析博客内容并获取图片链接</li><li>逐一下载图片至本地</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>方案确定好了，那就撸起袖子加油干吧！</p><h3 id="依赖库"><a href="#依赖库" class="headerlink" title="依赖库"></a>依赖库</h3><ul><li>requests</li></ul><p>Only one! 没错，依赖的第三方库就这一个，怎么装咱这就不说了</p><h3 id="获取用户ID"><a href="#获取用户ID" class="headerlink" title="获取用户ID"></a>获取用户ID</h3><p>用户ID，确切的说是用户博客的唯一ID，是归档页请求报文中的参数之一，通过查看主页源码找到了相应的字符串，所以只要用<code>request.get</code>抓取首页然后匹配ID字符串就可以了，代码如下：</p><pre><code class="python">def _get_blogid(username):    try:        html = requests.get(&#39;http://%s.lofter.com&#39; % username)        id_reg = r&#39;src=&quot;http://www.lofter.com/control\?blogId=(.*)&quot;&#39;        blogid = re.search(id_reg, html.text).group(1)        print(&#39;The blogid of %s is: %s&#39; % (username, blogid))        return blogid    except Exception as e:        print(&#39;get blogid from http://%s.lofter.com failed&#39; % username)        print(&#39;please check your username.&#39;)        exit(1)</code></pre><h3 id="生成POST请求数据"><a href="#生成POST请求数据" class="headerlink" title="生成POST请求数据"></a>生成POST请求数据</h3><p>根据前面归档页的分析，我们知道POST请求中除了一些固定参数外，还有用户ID，时间戳<code>timestamp</code>以及单次请求的博客篇数<code>N</code>需要确定，而ID已经在前面已经获取到了；博客篇数可以自定义一个数，如40；最后就剩下时间戳了。</p><p>经过多次尝试发现，这个时间戳<code>timestamp</code>是所有参数中唯一一个需要在每次请求中不断更新的参数。那么它更新的依据是什么呢？每篇博客都对应着一个<code>timestamp</code>，而且是博客的发布时间，每次请求后得到的最后一篇博客的<code>timestamp</code>就可以作为下一次请求的<code>timestamp</code>。为什么呢，因为我多次实验发现，在给定一个<code>timestamp</code>并发送POST请求后，服务器会<strong>以请求参数中的时间戳为起点按时间顺序往前检索出指定篇数(如：40)的博客信息</strong></p><p>响应包的博客信息中包含了每篇博客的时间戳，所以每次获取响应包后，只要解析出响应包中最后一篇博客的时间戳，就可以作为下一次请求中的时间戳。</p><p>根据以上分析，可以写出获取时间戳的函数如下：</p><pre><code class="python"># time_pattern: re.compile(&#39;s%d\.time=(.*);s.*type&#39; % (query_number-1))def _get_timestamp(html, time_pattern):    if not html:        timestamp = round(time.time() * 1000)  # first timestamp(ms)    else:        timestamp = time_pattern.search(html).group(1)    return str(timestamp)</code></pre><blockquote><p>注意，首次请求的时间戳可以直接使用当前系统时间(ms)</p></blockquote><h3 id="发送POST请求包"><a href="#发送POST请求包" class="headerlink" title="发送POST请求包"></a>发送POST请求包</h3><p>POST请求包的<code>url</code>是固定的，<code>data</code>就是前面获取到的所有请求参数，<code>headers</code>如下：</p><pre><code class="python">headers = {        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36&#39;,        &#39;Host&#39;: username + &#39;.lofter.com&#39;,        &#39;Referer&#39;: &#39;http://%s.lofter.com/view&#39; % username,        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;    }</code></pre><p>其中，<code>User-Agent</code>用于模拟浏览器请求，后面三个参数最好都加上，否则可能无法请求成功。POST请求其实就是一条语句<code>requests.post</code>，具体实现如下：</p><pre><code class="python">def _get_html(url, data, headers):    try:        html = requests.post(url, data, headers = headers)    except Exception as e:        print(&quot;get %s failed\n%s&quot; % (url, str(e)))        return None    finally:        pass    return html</code></pre><h3 id="解析POST响应包"><a href="#解析POST响应包" class="headerlink" title="解析POST响应包"></a>解析POST响应包</h3><p>在获取响应包的文本<code>html</code>后，便可从中获取本次请求得到的所有博客的相对路径，然后生成绝对路径，进而逐一抓取博客原文，从原文中抓取所有图链。</p><pre><code class="python"># get urls of blogs: s3.permalink=&quot;44fbca_19a6b1b&quot;new_blogs = blog_url_pattern.findall(html)num_new_blogs = len(new_blogs)num_blogs += num_new_blogs if num_new_blogs != 0:    print(&#39;NewBlogs:%d\tTotalBolgs:%d&#39; % (num_new_blogs, num_blogs))    # get imgurls from new_blogs    imgurls = []    for blog in new_blogs:        imgurls.extend(_get_imgurls(username, blog, headers))    num_imgs += len(imgurls)</code></pre><p>以上代码便是获取POST的响应包<code>html</code>后的解析操作，其中<code>_get_imurls</code>是用于抓取博客原文并解析出所有图链的函数。</p><pre><code class="python">def _get_imgurls(username, blog, headers):    blog_url = &#39;http://%s.lofter.com/post/%s&#39; % (username, blog)    blog_html = requests.get(blog_url, headers = headers).text    imgurls = re.findall(r&#39;bigimgsrc=&quot;(.*?)&quot;&#39;, blog_html)    print(&#39;Blog\t%s\twith %d\tpictures&#39; % (blog_url, len(imgurls)))    return imgurls</code></pre><h3 id="下载图片"><a href="#下载图片" class="headerlink" title="下载图片"></a>下载图片</h3><pre><code class="python">def _capture_images(imgurl, path):    headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36&#39;}    for i in range(1,3):        try:            image_request = requests.get(imgurl, headers = headers, timeout = 20)            if image_request.status_code == 200:                open(path, &#39;wb&#39;).write(image_request.content)                break        except requests.exceptions.ConnectionError as e:            print(&#39;\tGet %s failed\n\terror:%s&#39; % (imgurl, e))            if i == 1:                imgurl = re.sub(&#39;^http://img.*?\.&#39;,&#39;http://img.&#39;,imgurl)                print(&#39;\tRetry &#39; + imgurl)            else:                print(&#39;\tRetry fail&#39;)        except Exception as e:            print(e)        finally:            pass</code></pre><p>有了图链，最后的工作当然是下载图片了，上面这段代码便是用来下载图片的，<code>headers</code>是为了模拟浏览器访问。那为什么要尝试下载两次呢？因为我在抓取过程中，有时候会出现抓取失败的情况，并显示以下错误信息：</p><pre><code class="yaml">&#39;Connection aborted.&#39;, RemoteDisconnected(&#39;Remote end closed connection without response&#39;</code></pre><p>所以在<code>Retry</code>前先将图链对应的<code>host</code>稍加修改,这样可以保证更高的成功率，但并不能完全避免。对于下载失败的情况，可能是：</p><ol><li>被反爬了（极大可能）</li><li>网络通信不畅（可能性低）</li><li>图链失效</li><li>服务器出毛病了</li></ol><p>有时候，同样一个图链，过一段时间去抓就好了，或者换个网络就好了。我猜测是被反爬，但证据不足，所以只能降低爬取频率，比如每发送接收一次POST请求便<code>sleep</code>10s左右，但还是会有失败的情况，如果大家有更好的意见，欢迎交流。目前情况，正常情况100%爬取完全没问题，异常情况90%以上吧。</p><h3 id="主循环"><a href="#主循环" class="headerlink" title="主循环"></a>主循环</h3><p>好了，其它零碎的代码就不多说了，爬虫主循环流程如下，其实就是以上步骤的整合：</p><ol><li>爬取归档页面指定篇数<code>query_number</code>的博文链接<code>new_blogs</code></li><li>逐个爬取博文<code>blog</code>数据，获取每篇<code>blog</code>的所有大图链接<code>imgurls</code></li><li>逐个爬取大图链接<code>imgurls</code>,下载图片至本地目录</li><li>判断是否已爬取完所有博文<ul><li>若已爬完，则显示爬取成果信息，并退出</li><li>若未爬完，则更新请求包中的时间戳<code>timestamp</code>，返回第1步继续爬取新的博文</li></ul></li></ol><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><pre><code class="python">#!/usr/bin/env python# -*- coding:utf-8 -*-# date: 2018.03.07&quot;&quot;&quot;Capture pictures from lofter with username.&quot;&quot;&quot;import reimport osimport platformimport requestsimport timeimport randomdef _get_path(username):    path = {        &#39;Windows&#39;: &#39;D:/litreily/Pictures/python/lofter/&#39; + username,        &#39;Linux&#39;: &#39;/mnt/d/litreily/Pictures/python/lofter/&#39; + username    }.get(platform.system())    if not os.path.isdir(path):        os.makedirs(path)    return pathdef _get_html(url, data, headers):    try:        html = requests.post(url, data, headers = headers)    except Exception as e:        print(&quot;get %s failed\n%s&quot; % (url, str(e)))        return None    finally:        pass    return htmldef _get_blogid(username):    try:        html = requests.get(&#39;http://%s.lofter.com&#39; % username)        id_reg = r&#39;src=&quot;http://www.lofter.com/control\?blogId=(.*)&quot;&#39;        blogid = re.search(id_reg, html.text).group(1)        print(&#39;The blogid of %s is: %s&#39; % (username, blogid))        return blogid    except Exception as e:        print(&#39;get blogid from http://%s.lofter.com failed&#39; % username)        print(&#39;please check your username.&#39;)        exit(1)def _get_timestamp(html, time_pattern):    if not html:        timestamp = round(time.time() * 1000)  # first timestamp(ms)    else:        timestamp = time_pattern.search(html).group(1)    return str(timestamp)def _get_imgurls(username, blog, headers):    blog_url = &#39;http://%s.lofter.com/post/%s&#39; % (username, blog)    blog_html = requests.get(blog_url, headers = headers).text    imgurls = re.findall(r&#39;bigimgsrc=&quot;(.*?)&quot;&#39;, blog_html)    print(&#39;Blog\t%s\twith %d\tpictures&#39; % (blog_url, len(imgurls)))    return imgurlsdef _capture_images(imgurl, path):    headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36&#39;}    for i in range(1,3):        try:            image_request = requests.get(imgurl, headers = headers, timeout = 20)            if image_request.status_code == 200:                open(path, &#39;wb&#39;).write(image_request.content)                break        except requests.exceptions.ConnectionError as e:            print(&#39;\tGet %s failed\n\terror:%s&#39; % (imgurl, e))            if i == 1:                imgurl = re.sub(&#39;^http://img.*?\.&#39;,&#39;http://img.&#39;,imgurl)                print(&#39;\tRetry &#39; + imgurl)            else:                print(&#39;\tRetry fail&#39;)        except Exception as e:            print(e)        finally:            passdef _create_query_data(blogid, timestamp, query_number):    data = {&#39;callCount&#39;:&#39;1&#39;,    &#39;scriptSessionId&#39;:&#39;${scriptSessionId}187&#39;,    &#39;httpSessionId&#39;:&#39;&#39;,    &#39;c0-scriptName&#39;:&#39;ArchiveBean&#39;,    &#39;c0-methodName&#39;:&#39;getArchivePostByTime&#39;,    &#39;c0-id&#39;:&#39;0&#39;,    &#39;c0-param0&#39;:&#39;number:&#39; + blogid,    &#39;c0-param1&#39;:&#39;number:&#39; + timestamp,    &#39;c0-param2&#39;:&#39;number:&#39; + query_number,    &#39;c0-param3&#39;:&#39;boolean:false&#39;,    &#39;batchId&#39;:&#39;123456&#39;}    return datadef main():    # prepare paramters    username = &#39;litreily&#39;    blogid = _get_blogid(username)    query_number = 40    time_pattern = re.compile(&#39;s%d\.time=(.*);s.*type&#39; % (query_number-1))    blog_url_pattern = re.compile(r&#39;s[\d]*\.permalink=&quot;([\w_]*)&quot;&#39;)     # creat path to save imgs    path = _get_path(username)    # parameters of post packet    url = &#39;http://%s.lofter.com/dwr/call/plaincall/ArchiveBean.getArchivePostByTime.dwr&#39; % username    data = _create_query_data(blogid, _get_timestamp(None, time_pattern), str(query_number))    headers = {        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36&#39;,        &#39;Host&#39;: username + &#39;.lofter.com&#39;,        &#39;Referer&#39;: &#39;http://%s.lofter.com/view&#39; % username,        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;    }    num_blogs = 0    num_imgs = 0    index_img = 0    print(&#39;------------------------------- start line ------------------------------&#39;)    while True:        html = _get_html(url, data, headers).text        # get urls of blogs: s3.permalink=&quot;44fbca_19a6b1b&quot;        new_blogs = blog_url_pattern.findall(html)        num_new_blogs = len(new_blogs)        num_blogs += num_new_blogs         if num_new_blogs != 0:            print(&#39;NewBlogs:%d\tTotalBolgs:%d&#39; % (num_new_blogs, num_blogs))            # get imgurls from new_blogs            imgurls = []            for blog in new_blogs:                imgurls.extend(_get_imgurls(username, blog, headers))            num_imgs += len(imgurls)            # download imgs            for imgurl in imgurls:                index_img += 1                paths = &#39;%s/%d.%s&#39; % (path, index_img, re.search(r&#39;(jpg|png|gif)&#39;, imgurl).group(0))                print(&#39;{}\t{}&#39;.format(index_img, paths))                _capture_images(imgurl, paths)        if num_new_blogs != query_number:            print(&#39;------------------------------- stop line -------------------------------&#39;)            print(&#39;capture complete!&#39;)            print(&#39;captured blogs:%d images:%d&#39; % (num_blogs, num_imgs))            print(&#39;download path:&#39; + path)            print(&#39;-------------------------------------------------------------------------&#39;)            break        data[&#39;c0-param1&#39;] = &#39;number:&#39; + _get_timestamp(html, time_pattern)        print(&#39;The next TimeStamp is : %s\n&#39; % data[&#39;c0-param1&#39;].split(&#39;:&#39;)[1])        # wait a few second        time.sleep(random.randint(5,10))if __name__ == &#39;__main__&#39;:    main()</code></pre><h2 id="爬取测试"><a href="#爬取测试" class="headerlink" title="爬取测试"></a>爬取测试</h2><p><img src="/assets/spider/lofter/lofter_spider.png" alt="lofter spider"></p><p><img src="/assets/spider/lofter/pictures.png" alt="pictures"></p><h2 id="说在最后"><a href="#说在最后" class="headerlink" title="说在最后"></a>说在最后</h2><ul><li>Github 源码：<a href="https://github.com/Litreily/capturer" target="_blank" rel="noopener">https://github.com/Litreily/capturer</a></li><li>欢迎交流探讨与STAR</li><li>请节制使用！</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;LOFTER&lt;/code&gt;是网易出品的优质轻博客，灵感源于国外的&lt;code&gt;tumblr&lt;/code&gt;，但比之更加文艺，更加本地化。本人非常喜欢&lt;code&gt;LOFTER&lt;/code&gt;的UI设计，以及其中的优质用户和内容，似乎网易并不擅长推广，所以受众并不广泛。这
      
    
    </summary>
    
      <category term="Python" scheme="http://www.litreily.top/categories/Python/"/>
    
    
      <category term="tools" scheme="http://www.litreily.top/tags/tools/"/>
    
      <category term="spider" scheme="http://www.litreily.top/tags/spider/"/>
    
      <category term="lofter" scheme="http://www.litreily.top/tags/lofter/"/>
    
  </entry>
  
  <entry>
    <title>两款实用的DDos攻击工具</title>
    <link href="http://www.litreily.top/2018/02/22/ddos-attack/"/>
    <id>http://www.litreily.top/2018/02/22/ddos-attack/</id>
    <published>2018-02-22T12:53:38.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p>之前为了重现某个bug，需要对网络设备进行ddos攻击测试，同时也是对设备的网络攻击防护功能进行抗压测试。临阵磨枪，google了两款攻击工具，windows平台的<code>hyenae</code>，以及Linux平台的<code>hping3</code>，在此记录一下两者的用法。</p><h2 id="Hyenae"><a href="#Hyenae" class="headerlink" title="Hyenae"></a>Hyenae</h2><p><code>hyenae</code>是在<code>Windows</code>平台上非常好用的一款<code>ddos</code>攻击工具，可以完成绝大多数的攻击操作。</p><h3 id="download"><a href="#download" class="headerlink" title="download"></a>download</h3><ul><li><a href="https://sourceforge.net/projects/hyenae/" target="_blank" rel="noopener">https://sourceforge.net/projects/hyenae/</a></li></ul><h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><ul><li>ARP-Request flooding</li><li>ARP-Cache poisoning</li><li>PPPoE session initiation flooding</li><li>Blind PPPoE session termination</li><li>ICMP-Echo flooding</li><li>ICMP-Smurf attack</li><li>ICMP based TCP-Connection reset</li><li>TCP-SYN flooding</li><li>TCP-Land attack</li><li>Blind TCP-Connection reset</li><li>UDP flooding</li><li>DNS-Query flooding</li><li>DHCP-Discover flooding</li><li>DHCP starvation attack</li><li>DHCP-Release forcing</li><li>Cisco HSRP active router hijacking</li><li>Pattern based packet address configuration</li><li>Intelligent address and address protocol detection</li><li>Smart wildcard-based randomization</li><li>Daemon for setting up remote attack networks - HyenaeFE QT-Frontend support</li></ul><h3 id="interface"><a href="#interface" class="headerlink" title="interface"></a>interface</h3><p><img src="/assets/network/hyenae-interface.png" alt="hyenae interface"></p><p><code>hyenae</code>的界面比较简单，图中展示的是<code>SYN/ACK</code>洪泛攻击的配置选项。</p><ul><li><code>operation mode</code>中可以选择网卡</li></ul><ul><li><code>Network Protocol</code>中可以选择攻击方式对应的网络协议，如<code>SYN</code>洪泛攻击对应传输层的<code>TCP</code>，<code>IP</code>协议可选<code>IPv4, IPv6</code></li></ul><ul><li>攻击源的<code>IP</code>,<code>MAC</code>地址以及端口号可以非常灵活的设置，按<code>MAC-IP@port</code>的格式书写，如图中的攻击源匹配模式<code>%-172.17.14.158@80</code><ul><li>% 代表任意，在此处代表任意的MAC地址</li><li>172.17.14.158为伪造的攻击源IP，可以修改为任意的合法IP</li><li>80为端口号，80同时也是网络服务器的默认端口</li></ul></li></ul><ul><li>攻击目标的设置方式与攻击源一致，图中的<code>%-172.17.14.10@80</code><ul><li>% 随机生成MAC地址</li><li>172.17.14.10代表被攻击的IP</li><li>80为攻击目标的被攻击端口号</li></ul></li></ul><ul><li>针对<code>TCP</code>协议，右侧给出其对应的<code>5</code>个常见<code>flags</code>: <code>FIN, SYN, RST, PSH, ACK</code><ul><li>可以随意进行单选或多选，以实现不同的攻击方式</li><li>随意的组合可以产生正常通信过程中无法出现的数据包</li></ul></li></ul><ul><li>软件下方可以设置数据包的发送速率，默认为无限速发送，这会暂用大量带宽，导致网络拥塞；当然啦，这个软件本就是为了攻击网络，导致网络瘫痪正是其目的所在</li></ul><ul><li>选择不同的攻击方式，会显示相应不同的配置选项 </li></ul><p>简单说，这幅图的作用是产生<code>MAC</code>地址随机，<code>IP</code>为<code>172.17.14.158</code>，端口为80的伪造源，去攻击目的<code>MAC</code>随机，<code>IP</code>为<code>172.17.14.10</code>的<code>web server</code>。</p><p>由于目的<code>MAC</code>随机，<strong>当MAC地址首字节为奇数时，生成的数据包为广播包</strong>，此时将产生广播风暴，局域网内的所有设备都将收到大量的广播包，当速率很高时，很容易导致局域网瘫痪，这是需要注意的。本人当时年少轻狂，有次测试，使用全速率的广播式<code>SYN/ACK</code>攻击，直接导致部门的局域网瘫痪断网，幸好是晚上，后来找人重置了部门内的网络设备才恢复，想想真是罪过啊。</p><h3 id="Use-cases"><a href="#Use-cases" class="headerlink" title="Use cases"></a>Use cases</h3><ul><li>Land Attack<ul><li>src:  %-172.17.14.94@53</li><li>des: %-172.17.14.94@80</li></ul></li></ul><pre><code class="bash">DoS *** 3118 *** {Land Attack} are suppressed![DoS Attack: Land Attack] from source: 172.17.14.94, port 53,[DoS Attack: Land Attack] from source: 172.17.14.94, port 53,[DoS Attack: Land Attack] from source: 172.17.14.94, port 53,</code></pre><ul><li>SYN/ACK scan (TCP SYN ACK)<ul><li>src: %-172.17.14.8@80</li><li>des: %-172.17.14.94@80</li></ul></li></ul><pre><code class="bash">DoS *** 3896 *** {SYN/ACK Scan} are suppressed![DoS Attack: SYN/ACK Scan] from source: 172.17.14.8, port 80,[DoS Attack: SYN/ACK Scan] from source: 172.17.14.8, port 80,[DoS Attack: SYN/ACK Scan] from source: 172.17.14.8, port 80,</code></pre><ul><li>ping flood (icmp echo)<ul><li>src:  %-172.17.14.8</li><li>des: %-172.17.14.94</li></ul></li></ul><pre><code class="bash">DoS *** 1881 *** {Ping Flood} are suppressed![DoS Attack: Ping Flood] from source: 172.17.14.8,[DoS Attack: Ping Flood] from source: 172.17.14.8,[DoS Attack: Ping Flood] from source: 172.17.14.8,</code></pre><ul><li>ping sweep (icmp echo)<ul><li>src:  %-%</li><li>des: %-%</li></ul></li></ul><pre><code class="bash">DoS *** 1719 *** {Ping Sweep} are suppressed![DoS Attack: Ping Sweep] from source: 188.167.1.1,[DoS Attack: Ping Sweep] from source: 113.172.1.5,[DoS Attack: Ping Sweep] from source: 175.181.2.6,</code></pre><ul><li>RST Scan(TCP RST)<ul><li>src: %-172.17.14.8@80</li><li>des: %-172.17.14.94@80</li></ul></li></ul><pre><code class="bash">DoS *** 4023 *** {RST Scan} are suppressed![DoS Attack: RST Scan] from source: 172.17.14.8, port 80,[DoS Attack: RST Scan] from source: 172.17.14.8, port 80,[DoS Attack: RST Scan] from source: 172.17.14.8, port 80,</code></pre><ul><li>ACK scan (TCP ACK)<ul><li>src: %-172.17.14.8@80</li><li>des: %-172.17.14.94@80</li></ul></li></ul><pre><code class="bash">DoS *** 3989 *** {ACK Scan} are suppressed![DoS Attack: ACK Scan] from source: 172.17.14.8, port 80,[DoS Attack: ACK Scan] from source: 172.17.14.8, port 80,[DoS Attack: ACK Scan] from source: 172.17.14.8, port 80</code></pre><ul><li>FIN scan(TCP FIN)<ul><li>src: %-172.17.14.8@80</li><li>des: %-172.17.14.94@80</li></ul></li></ul><pre><code class="bash">DoS *** 3009 *** {FIN Scan} are suppressed![DoS Attack: FIN Scan] from source: 172.17.14.8, port 80,[DoS Attack: FIN Scan] from source: 172.17.14.8, port 80,[DoS Attack: FIN Scan] from source: 172.17.14.8, port 80,</code></pre><h2 id="hping3"><a href="#hping3" class="headerlink" title="hping3"></a>hping3</h2><blockquote><p>hping3是用于生成和解析TCPIP协议数据包的开源工具</p></blockquote><p><code>hping3</code>同样可用于产生<code>ddos</code>攻击包，但与<code>hyenae</code>不同的是，<code>hping3</code>无法手动设置<code>MAC</code>地址，而是根据<code>IP</code>地址自动获取</p><p>需要注意的是，如果使用搬瓦工购买的<code>vps</code>向公网IP执行<code>hping3</code>攻击的话，最好不要尝试，如果要用也一定记得限速，否则就会被警告并关停,当然你有3次机会重置</p><p><img src="/assets/network/vps-warning.png" alt="vps warning"></p><h3 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h3><pre><code class="bash"># land attack$ sudo hping3 -V -c 10000 -d 120 -S -w 64 --keep -p 80 -s 20000 --flood -a 172.17.14.52 172.17.14.52# syn/ack attack$ sudo hping3 -V -c 10000 -d 120 -S -A -w 64 --keep -p 80 -s 80 --flood -a 172.17.14.192 172.17.14.52# -V verbose# -c packet count# -d data size# -p destPort# -s srcPort# -a srcIP# -S SYN tag# -A ACK tag# -w winsize# -I interface</code></pre><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul><li>网络攻击工具<ul><li>hping3<ul><li><a href="http://man.linuxde.net/hping3" target="_blank" rel="noopener">http://man.linuxde.net/hping3</a></li><li><a href="http://0daysecurity.com/articles/hping3_examples.html" target="_blank" rel="noopener">http://0daysecurity.com/articles/hping3_examples.html</a></li></ul></li><li>LOIC</li><li>hyenae <ul><li><a href="https://sourceforge.net/projects/hyenae/" target="_blank" rel="noopener">https://sourceforge.net/projects/hyenae/</a></li></ul></li><li>免费DDOS攻击测试工具大合集 <a href="http://www.freebuf.com/sectool/36545.html" target="_blank" rel="noopener">http://www.freebuf.com/sectool/36545.html</a></li></ul></li></ul><ul><li>ddos攻击说明<ul><li><a href="https://security.radware.com/ddos-knowledge-center/ddospedia/syn-flood/" target="_blank" rel="noopener">https://security.radware.com/ddos-knowledge-center/ddospedia/syn-flood/</a></li><li>网络攻击：半连接攻击(SYN攻击)、全连接攻击、RST攻击、IP欺骗、DNS欺骗、DOS/DDOS攻击  <a href="http://blog.csdn.net/guowenyan001/article/details/11777361" target="_blank" rel="noopener">http://blog.csdn.net/guowenyan001/article/details/11777361</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前为了重现某个bug，需要对网络设备进行ddos攻击测试，同时也是对设备的网络攻击防护功能进行抗压测试。临阵磨枪，google了两款攻击工具，windows平台的&lt;code&gt;hyenae&lt;/code&gt;，以及Linux平台的&lt;code&gt;hping3&lt;/code&gt;，在此记录一
      
    
    </summary>
    
      <category term="Network" scheme="http://www.litreily.top/categories/Network/"/>
    
    
      <category term="ddos" scheme="http://www.litreily.top/tags/ddos/"/>
    
      <category term="hping3" scheme="http://www.litreily.top/tags/hping3/"/>
    
  </entry>
  
  <entry>
    <title>mySQL基本语法</title>
    <link href="http://www.litreily.top/2018/02/08/mysql-basic/"/>
    <id>http://www.litreily.top/2018/02/08/mysql-basic/</id>
    <published>2018-02-08T01:27:00.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p>最近折腾<code>vps</code>，想要手动搭建一个<code>LNMP</code>环境，那必然少不了要补补课，简单学习一下<code>mysql</code>的基本语法。</p><h2 id="install-mySQL"><a href="#install-mySQL" class="headerlink" title="install mySQL"></a>install mySQL</h2><h3 id="install-on-Centos"><a href="#install-on-Centos" class="headerlink" title="install on Centos"></a>install on Centos</h3><pre><code class="bash">$ sudo yum update$ sudo yum install mysql-server mysql-client$ sudo yum install mysql-devel # install sdk of mysql# /usr/include/mysql/mysql.h</code></pre><a id="more"></a><h3 id="install-on-Ubuntu"><a href="#install-on-Ubuntu" class="headerlink" title="install on Ubuntu"></a>install on Ubuntu</h3><pre><code class="bash">$ sudo apt-get update$ sudo apt-get install mysql-server mysql-client$ sudo apt-get install libmysqlclient15-dev || sudo apt-get install libmysqlclient-dev</code></pre><h2 id="mySQL-cmd"><a href="#mySQL-cmd" class="headerlink" title="mySQL cmd"></a>mySQL cmd</h2><h3 id="login"><a href="#login" class="headerlink" title="login"></a>login</h3><pre><code class="bash">$ mysql -u USERNAME -p$ mysql -u root -pmysql&gt; \h</code></pre><h3 id="add-user"><a href="#add-user" class="headerlink" title="add user"></a>add user</h3><pre><code class="bash">mysql&gt; CREATE USER &#39;username&#39;@&#39;host&#39; IDENTIFIED BY &#39;password&#39;;# e.g.mysql&gt; CREATE USER &#39;test&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;;</code></pre><h3 id="set-password"><a href="#set-password" class="headerlink" title="set password"></a>set password</h3><pre><code class="bash"># 为当前用户设置新的密码mysql&gt; SET PASSWORD = PASSWORD(&quot;newpassword&quot;);# 为其它用户重设密码mysql&gt; SET PASSWORD FOR &#39;username&#39;@&#39;host&#39; = PASSWORD(&#39;newpassword&#39;);</code></pre><h3 id="delete-user"><a href="#delete-user" class="headerlink" title="delete user"></a>delete user</h3><pre><code class="bash">DROP USER &#39;username&#39;@&#39;host&#39;</code></pre><h3 id="grant-privileges"><a href="#grant-privileges" class="headerlink" title="grant privileges"></a>grant privileges</h3><pre><code class="bash"># 为用户授权# privileges: SELECT, INSERT, UPDATE, ALL, ...mysql&gt; GRANT privileges ON databasename.tablename TO &#39;username&#39;@&#39;host&#39;;# 刷新缓存，使授权生效mysql&gt; FLUSH PRIVILEGES;# e.g.mysql&gt; GRANT SELECT, INSERT ON test.user TO &#39;pig&#39;@&#39;%&#39;;mysql&gt; GRANT ALL ON *.* TO &#39;pig&#39;@&#39;%&#39;;</code></pre><h3 id="databases-and-tables"><a href="#databases-and-tables" class="headerlink" title="databases and tables"></a>databases and tables</h3><ul><li>create database or table</li></ul><pre><code class="bash"># create database: vpsmysql&gt; CREATE DATABASE vps;Query OK, 1 row affected (0.00 sec)# show databasesmysql&gt; SHOW DATABASES;+--------------------+| Database           |+--------------------+| information_schema || mysql              || performance_schema || sys                || vps                |+--------------------+5 rows in set (0.00 sec)# use databasemysql&gt; USE vps;# create tablemysql&gt; CREATE TABLE test_tbl(    -&gt; test_id INT NOT NULL AUTO_INCREMENT,    -&gt; test_title VARCHAR(100) NOT NULL,    -&gt; test_author VARCHAR(40) NOT NULL,    -&gt; test_date DATE,    -&gt; PRIMARY KEY ( test_id )    -&gt; )ENGINE=InnoDB DEFAULT CHARSET=utf8;Query OK, 0 rows affected (0.06 sec)# show tablesmysql&gt; SHOW TABLES;+---------------+| Tables_in_vps |+---------------+| test_tbl      |+---------------+1 row in set (0.00 sec)# describe tablemysql&gt; DESC test_tbl;+-------------+--------------+------+-----+---------+----------------+| Field       | Type         | Null | Key | Default | Extra          |+-------------+--------------+------+-----+---------+----------------+| test_id     | int(11)      | NO   | PRI | NULL    | auto_increment || test_title  | varchar(100) | NO   |     | NULL    |                || test_author | varchar(40)  | NO   |     | NULL    |                || test_date   | date         | YES  |     | NULL    |                |+-------------+--------------+------+-----+---------+----------------+4 rows in set (0.03 sec)# drop tablemysql&gt; DROP TABLE runoob_tbl;Query OK, 0 rows affected (0.01 sec)</code></pre><p>in conclusion</p><pre><code class="bash">mysql&gt; CREATE DATABESE dataBaseName;mysql&gt; SHOW DATABASES;mysql&gt; USE dataBaseName;mysql&gt; CREATE TABLE tableName(content);mysql&gt; SHOW TABLES;mysql&gt; DESC tableName;mysql&gt; DROP tableName;</code></pre><ul><li>handle tables</li></ul><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul><li><a href="http://www.runoob.com/mysql/mysql-tutorial.html" target="_blank" rel="noopener">http://www.runoob.com/mysql/mysql-tutorial.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近折腾&lt;code&gt;vps&lt;/code&gt;，想要手动搭建一个&lt;code&gt;LNMP&lt;/code&gt;环境，那必然少不了要补补课，简单学习一下&lt;code&gt;mysql&lt;/code&gt;的基本语法。&lt;/p&gt;
&lt;h2 id=&quot;install-mySQL&quot;&gt;&lt;a href=&quot;#install-mySQL&quot; class=&quot;headerlink&quot; title=&quot;install mySQL&quot;&gt;&lt;/a&gt;install mySQL&lt;/h2&gt;&lt;h3 id=&quot;install-on-Centos&quot;&gt;&lt;a href=&quot;#install-on-Centos&quot; class=&quot;headerlink&quot; title=&quot;install on Centos&quot;&gt;&lt;/a&gt;install on Centos&lt;/h3&gt;&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;$ sudo yum update
$ sudo yum install mysql-server mysql-client
$ sudo yum install mysql-devel # install sdk of mysql
# /usr/include/mysql/mysql.h
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Network" scheme="http://www.litreily.top/categories/Network/"/>
    
    
      <category term="linux" scheme="http://www.litreily.top/tags/linux/"/>
    
      <category term="mysql" scheme="http://www.litreily.top/tags/mysql/"/>
    
      <category term="centos" scheme="http://www.litreily.top/tags/centos/"/>
    
  </entry>
  
  <entry>
    <title>结构体中长度为0的字符数组</title>
    <link href="http://www.litreily.top/2018/01/02/len0-charArray/"/>
    <id>http://www.litreily.top/2018/01/02/len0-charArray/</id>
    <published>2018-01-02T03:24:30.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<p>在C语言的结构体中，有一种特殊用法，在结构体的末尾放置一个长度为0的字符数组，结构体倒数第二个位置放置一个整型变量<code>len</code>。其典型样例如下：</p><pre><code class="c">typedef struct dynamic_value {  int flag;  int len;  char val[0];}s_flv;</code></pre><p>本文针对其特征，用法及适用场合予以简单介绍。</p><h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><ul><li>最后的字符数组<code>val</code>长度为0，不占用额外的内存空间</li><li>倒数第二个元素为一整型变量，用于存储字符数组的真实长度</li><li><code>val</code>实际指向的是结构体<code>s_flv</code>之后的内存空间</li><li>字符数组的大小可以在定义结构体变量时动态指定<code>s_flv.len</code></li><li>结构体元素个数不限，样例为典型模式，其中的<code>flag</code>作为标签用以标识不同的数据</li></ul><h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><ul><li>按<code>s_flv</code>格式写入数据到文件</li></ul><pre><code class="c">#include &lt;stdio.h&gt;static void write_flv(FILE *fp, int flag, char *val){    int len = 0;    if(val == NULL)        val = &quot;&quot;;    else        len = strlen(val);    fwrite(&amp;flag, sizeof(flag), 1, fp); // 写入标识    fwrite(&amp;len, sizeof(len), 1, fp);   // 写入字符数组长度    fwrite(val, len, 1, fp);    // 写入长度为len的字符串}</code></pre><ul><li>读取文件</li></ul><pre><code class="c">#include &lt;stdio.h&gt;static int read_flv(char *file, char *val){    char buf[32];   // 假定字符数组长度小等于32    char *p = buf;    FILE *fp = fopen(file, &quot;r&quot;);    if(!fp)        return -1;    if(fread(buf, 2*sizeof(int), 1, fp) &lt;= 0)   // 读取前两个整型数据，获取字符数组长度        return -1;    s_flv *flv = (s_flv *)p;        // 定义s_flv结构体，存储数据    fread(val, flv-&gt;len, 1, fp);    // 读取字符数组    flose(fp);    return 0;}</code></pre><ul><li><strong>说明</strong><ul><li>每执行一次<code>fread</code>或<code>fwrite</code>函数，文件指针就往后偏移相应的读取长度或写入长度</li><li><code>read_flv</code>定义<code>flv</code>时，从<code>buf</code>中可以获取到数组长度，然后使用<code>fread</code>读取相应长度的数据即可取出字符数组的内容。</li><li>使用以上方法生成的文件内容是二进制文件，许多字符是不可打印字符，所以使用<code>cat</code>指令无法正常显示文件内容</li></ul></li></ul><p>上面描述的方法是将<code>s_flv</code>类型的数据存入文件，但如果不想存入文件，那么该如何为其分配内存呢，下面来看一下：</p><pre><code class="c">#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;// 定义s_flv指针变量int size = 10;s_flv *flv = (s_flv *)malloc(sizeof(s_flv) + size);flv-&gt;len = size;flv-&gt;val = flv + sizeof(s_flv);// 释放指针free(flv);</code></pre><h2 id="适用场合"><a href="#适用场合" class="headerlink" title="适用场合"></a>适用场合</h2><ul><li><code>json</code>文件的读写</li><li>不定长度字符串的读写</li></ul><h2 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h2><ul><li><a href="http://www.cnblogs.com/felove2013/articles/4050226.html" target="_blank" rel="noopener">浅析长度为0的数组</a></li></ul>]]></content>
    
    <summary type="html">
    
      在C语言的结构体中，有一种特殊用法，在结构体的末尾放置一个长度为0的字符数组，结构体倒数第二个位置放置一个整型变量，该变量用于存储紧随该结构体之后的字符数组的长度。
    
    </summary>
    
      <category term="嵌入式" scheme="http://www.litreily.top/categories/%E5%B5%8C%E5%85%A5%E5%BC%8F/"/>
    
    
      <category term="C/C++" scheme="http://www.litreily.top/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>VPS+SS翻越GFW</title>
    <link href="http://www.litreily.top/2017/09/07/ss-config/"/>
    <id>http://www.litreily.top/2017/09/07/ss-config/</id>
    <published>2017-09-07T13:33:02.000Z</published>
    <updated>2018-10-12T13:34:06.413Z</updated>
    
    <content type="html"><![CDATA[<h2 id="购买并配置VPS"><a href="#购买并配置VPS" class="headerlink" title="购买并配置VPS"></a>购买并配置VPS</h2><p>在<a href="http://banwagong.cn/" target="_blank" rel="noopener">搬瓦工</a>选择一个<code>VPS</code>，大概每年<code>$19.99</code>，使用邀请码可优惠<code>6%</code>，可以使用支付宝(Alipay)购买。成功购买<code>VPS</code>后，进入<code>VPS</code>的<a href="https://kiwivm.64clouds.com/main.php" target="_blank" rel="noopener">管理界面</a>，登录账户后便可以查看服务器信息，并进行相关配置了。</p><h2 id="安装SS服务器"><a href="#安装SS服务器" class="headerlink" title="安装SS服务器"></a>安装SS服务器</h2><p>如果VPS选用<code>centos</code>系统，则可以直接在配置界面左下方选择<code>Shadowsocks Server</code>，一键安装就<code>ok</code>了，当然喜欢折腾的也可以自己用<code>yum install</code>下载安装和配置<code>Shadowsocks</code>，此不赘述。</p><h2 id="安装SS客户端"><a href="#安装SS客户端" class="headerlink" title="安装SS客户端"></a>安装SS客户端</h2><h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><p><code>Ubuntu</code>可以选择<code>github</code>中的<a href="https://github.com/shadowsocks/shadowsocks" target="_blank" rel="noopener">shadowsocks/shadowsocks</a>，或<a href="https://github.com/shadowsocks/shadowsocks-qt5" target="_blank" rel="noopener">shadowsocks/shadowsocks-qt5</a>.</p><h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><p><code>Windows</code>选择<a href="https://github.com/shadowsocks/shadowsocks-windows" target="_blank" rel="noopener">shadowsocks/shadowsocks-windows</a>，或<code>shadowsocks/shadowsocks-qt5</code>中的<a href="https://github.com/shadowsocks/shadowsocks-qt5/releases" target="_blank" rel="noopener">ss-qt5-v2.9.0-win64.7z</a>。本人刚开始使用前者，无法科学上网，从<code>log</code>信息中发现可以将数据发送至<code>VPS</code>，但<code>VPS</code>无法解析其头部信息，故而连接失败，后来尝试网上各种方法无果，最终使用<code>qt5</code>版本成功。具体原因不详。</p><h3 id="Android"><a href="#Android" class="headerlink" title="Android"></a>Android</h3><p>安卓端就下载安装<a href="https://github.com/shadowsocks/shadowsocks-android/releases" target="_blank" rel="noopener">shadowsocks/shadowsocks-android</a>内的<code>apk</code>即可。</p><h2 id="配置SS客户端"><a href="#配置SS客户端" class="headerlink" title="配置SS客户端"></a>配置SS客户端</h2><p><code>SS</code>的配置相当简单，参考<code>VPS</code>中<code>Shadowsocks</code>配置界面给的信息即可，必要信息包括：</p><ul><li>服务器地址 (<code>Host IP</code>)</li><li>密码(<code>password</code>)  </li><li>端口号<code>Port(default:443)</code></li><li>加密方式<code>encryption(default:aes-256-cfb)</code></li></ul><h2 id="配置浏览器"><a href="#配置浏览器" class="headerlink" title="配置浏览器"></a>配置浏览器</h2><p>无论是<code>google chrome</code>还是<code>firefox</code>，都有相应的代理插件，最常用的是<code>SwitchyOmega</code>。下载该插件并安装后，需要打开插件的<strong>选项</strong>，进行一定的配置操作：</p><ol><li>新建<strong>情景模式</strong>（<code>profile</code>），命名随意，如：<strong>SS</strong>;</li><li>在默认(<code>default</code>)选项中选择<code>SOCK5</code>，服务器(<code>Server</code>)设为<code>127.0.0.1</code>，端口(<code>port</code>)设为<code>1080</code>;</li><li>点击左侧的应用修改(<code>Apply changes</code>);</li><li>点击<code>auto switch</code>，页面上的条件栏(<code>condition</code>)不用管，点击下面的添加规则，在情景模式（<code>profile</code>）一列中选择之前创建的<code>SS</code>，默认选项分两种情况：<ul><li>普通网络，没有额外代理的情况，默认设为直连(<code>direct</code>);</li><li>如果是公司网设了代理，则默认设为代理<code>proxy</code>，并在代理<code>proxy</code>中设置好公司的代理服务器<code>IP</code>和端口号<code>port</code>.</li></ul></li><li>在下面的规则列表配置中，选择<code>AutoProxy</code>,添加网址<a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt，" target="_blank" rel="noopener">https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt，</a> 然后点击下方按钮更新列表信息，网址无误的话会提示成功更新规则列表；</li><li>点击左侧应用修改<code>Apply changes</code>，在插件中选择<code>auto switch</code>选项即可开启自动切换代理模式。</li></ol><p>经过以上配置后，启动已配置好服务器信息的<code>Shadowsocks</code>客户端，正常情况下就可以访问<code>Google</code>, <code>youtube</code>等国外网站了。</p><h2 id="配置全局代理"><a href="#配置全局代理" class="headerlink" title="配置全局代理"></a>配置全局代理</h2><h3 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h3><p>在<code>Ubuntu</code>中，可以打开 Settings-&gt;Network-&gt;Network Proxy ，在<code>Socks Host</code>一栏中设置<code>IP</code>为<code>127.0.0.1</code>，代理端口为<code>1080</code>。这样便设置好了全局代理。<code>firefox</code>的网络设置选项中有单独一项用于选择系统代理设置，对应的就是这个。</p><h3 id="安装polipo"><a href="#安装polipo" class="headerlink" title="安装polipo"></a>安装polipo</h3><p><code>polipo</code>是一个小型的代理服务器软件，用于二次转发数据包，使用它可以配合系统设置里的全局代理实现真正的全局代理，让终端等应用都可以翻越<code>GFW</code>，这样在<code>shell</code>中下载软件就方便快速多了，亲测效果很是明显。</p><p>下面是安装配置步骤：</p><pre><code class="bash">$ sudo apt-get install polipo$ sudo vim /etc/polipo/config# This file only needs to list configuration variables that deviate# from the default values.  See /usr/share/doc/polipo/examples/config.sample# and &quot;polipo -v&quot; for variables you can tweak and further information.logSyslog = truelogFile = /var/log/polipo/polipo.logproxyAddress = &quot;0.0.0.0&quot;socksParentProxy = &quot;127.0.0.1:1080&quot;socksProxyType = socks5chunkHighMark = 50331648objectHighMark = 16384serverMaxSlots = 64serverSlots = 16serverSlots1 = 32</code></pre><p>修改<code>config</code>文件后需要重启一下<code>polipo</code>，然后测试是否代理是否成功开启。</p><pre><code class="bash">$ sudo /etc/init.d/polipo restart$ export http_proxy=&quot;http://127.0.0.1:8123/&quot; curl ifconfig.me***.***.***.*** # return your VPS-IP_ADDR if success</code></pre><p>到此就可以愉快的使用谷歌了，但是目前晚上的网速略慢，看<code>youtube</code>经常只能到<code>360P</code>。</p>]]></content>
    
    <summary type="html">
    
      Shadowsocks简称SS或$$，这段时间在搬瓦工买了个VPS，配合SS以及各种代理插件成功翻越GFW，期间遇到各种问题，谨以此文记录之。
    
    </summary>
    
      <category term="杂物柜" scheme="http://www.litreily.top/categories/%E6%9D%82%E7%89%A9%E6%9F%9C/"/>
    
    
      <category term="ubuntu" scheme="http://www.litreily.top/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>远程服务器中编译安装tmux</title>
    <link href="http://www.litreily.top/2017/08/23/tmux-install/"/>
    <id>http://www.litreily.top/2017/08/23/tmux-install/</id>
    <published>2017-08-23T00:28:22.000Z</published>
    <updated>2018-10-12T13:34:06.409Z</updated>
    
    <content type="html"><![CDATA[<p>在工作当中，公司出于安全考虑，部署的远程Linux服务器总有一些限制（没网，Shell指令有限），而且员工作为普通用户，无法安装软件到除用户目录以外的目录。当我们需要下载安装一些常用工具时，因为没网，所以无法使用<code>apt-get</code>等下载指令，只能先在本地PC下载软件源码，然后传入服务器进行编译安装。本文以<code>tmux</code>为例说明服务器中编译安装软件的流程。</p><p>安装路径：<code>~/lib/software/tmux</code></p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><p><code>tmux</code>的下载地址 : <a href="https://github.com/tmux/tmux/releases/download/2.5/tmux-2.5.tar.gz" target="_blank" rel="noopener">https://github.com/tmux/tmux/releases/download/2.5/tmux-2.5.tar.gz</a></p><p>依赖库<code>libevent</code>：<a href="https://github.com/libevent/libevent/releases/download/release-2.1.8-stable/libevent-2.1.8-stable.tar.gz" target="_blank" rel="noopener">https://github.com/libevent/libevent/releases/download/release-2.1.8-stable/libevent-2.1.8-stable.tar.gz</a></p><p>下载后的文件通过一定方式(scp，共享服务，…)可以传入服务器中。</p><h2 id="Move-files-optional"><a href="#Move-files-optional" class="headerlink" title="Move files (optional)"></a>Move files (optional)</h2><p>在服务器中执行以下指令，将文件放入<code>~/lib</code>文件夹。</p><pre><code class="bash">cdmkdir libcp sharedir.git/*.tar.gz lib/</code></pre><h2 id="Edit-profile"><a href="#Edit-profile" class="headerlink" title="Edit .profile"></a>Edit .profile</h2><p>编辑<code>.profile</code>，添加全局变量<code>SWDIR</code>（存放用户手动编译安装的软件）；更新依赖库<code>LD_LIBRARY_PATH</code>和环境变量<code>PATH</code>，分别添加<code>tmux</code>的依赖路径和安装路径。</p><pre><code class="bash">$ vim ~/.profile# ...export SWDIR=/home/&lt;username&gt;/lib/softwareexport LD_LIBRARY_PATH=$LD_LIBRARY:$SWDIR/libevent/libPATH=$PATH:${SWDIR}/tmux/:${SWDIR}/tmux/bin/</code></pre><h2 id="Complie-libevent"><a href="#Complie-libevent" class="headerlink" title="Complie libevent"></a>Complie libevent</h2><p>配置好环境变量后，首先编译<code>tmux</code>的依赖库<code>libevent</code></p><pre><code class="bash">cd ~/libtar -xvzf libevent-2.1.8-stable.tar.gzcd libevent-2.1.8-stable./autogen.sh./configure --prefix=$SWDIR/libevent &amp;&amp; makemake install</code></pre><p><strong>说明：</strong>默认安装路径是没有访问权限的，所以需要使用<code>--prefix</code>选项手动指定安装路径至用户目录中的某个文件夹。</p><h2 id="Complie-tmux"><a href="#Complie-tmux" class="headerlink" title="Complie tmux"></a>Complie tmux</h2><pre><code class="bash">cd ~/libtar -xvzf tmux-2.5.tar.gzcd tmux-2.5./configure --prefix=$SWDIR/tmux CFLAGS=&quot;-I$SWDIR/libevent/include&quot; LDFLAGS=&quot;-L$SWDIR/libevent/lib&quot; &amp;&amp; makemake install</code></pre><p><strong>说明：</strong><code>CFLAGS</code>和<code>LDFLAGS</code>用于指定编译<code>tmux</code>所需的依赖库文件。</p><p>至此，<code>tmux</code>便安装完成了。在<code>shell</code>中输入<code>tmux</code>即可启动，通过<code>man tmux</code>可以查看帮助文档。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在工作当中，公司出于安全考虑，部署的远程Linux服务器总有一些限制（没网，Shell指令有限），而且员工作为普通用户，无法安装软件到除用户目录以外的目录。当我们需要下载安装一些常用工具时，因为没网，所以无法使用&lt;code&gt;apt-get&lt;/code&gt;等下载指令，只能先在本
      
    
    </summary>
    
      <category term="Linux" scheme="http://www.litreily.top/categories/Linux/"/>
    
    
      <category term="ubuntu" scheme="http://www.litreily.top/tags/ubuntu/"/>
    
      <category term="tmux" scheme="http://www.litreily.top/tags/tmux/"/>
    
  </entry>
  
  <entry>
    <title>Telnet中使用smtp发送邮件</title>
    <link href="http://www.litreily.top/2017/08/17/telnet-smtp/"/>
    <id>http://www.litreily.top/2017/08/17/telnet-smtp/</id>
    <published>2017-08-17T13:18:44.000Z</published>
    <updated>2018-10-12T13:34:06.409Z</updated>
    
    <content type="html"><![CDATA[<p><code>Telnet</code>是远程登录服务的标准协议和主要方式，工作于<code>TCP/IP</code>协议族的应用层，常用于远程登录<code>web</code>服务器，其中便包括<code>SMTP</code>，<code>POP3</code>等邮件服务器。</p><p>这几天参考书籍《计算机网络-自顶向下方法》学习计算机网络知识，顺便尝试了如何使用<code>Telnet</code>登录<code>smtp</code>发送邮件，下面通过一个简单示例介绍具体的发送过程。</p><h2 id="Telnet-smtp-163-com-25"><a href="#Telnet-smtp-163-com-25" class="headerlink" title="Telnet smtp.163.com 25"></a>Telnet smtp.163.com 25</h2><p><code>smtp</code>对应的端口号为<code>25</code>，以163邮箱为例，下面是邮件发送过程。</p><pre><code class="zsh">→ ~ telnet smtp.163.com 25Trying 220.181.12.15...Connected to smtp.163.com.Escape character is ^].220 163.com Anti-spam GT for Coremail System (163com[20141201])helo 163.com250 OKauth login334 dXNlcm5hbWU6MTgyNjY2MzI4NzJAMTYzLmNvbQ==334 UGFzc3dvcmQ6****************235 Authentication successfulmail from:&lt;182****2872@163.com&gt;250 Mail OKrcpt to:&lt;707***098@qq.com&gt;250 Mail OKdata354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;From: 182****2872@163.comTo: 707***098@qq.comSubject: Test the use of smtp  Hello, litreily.                            This is a message edit with telnet.Best wish to you.Litreily.250 Mail OK queued as smtp11,D8CowAAnjlqKlZVZXa4QAQ--.18342S2 1502975693quitConnection closed by foreign host.→ ~</code></pre><p>下面分开对每个过程进行解析，<code>C</code>代表客户端，即本机，<code>S</code>代表远程服务器端。整个过程分为以下几步：</p><ul><li>打招呼</li><li>账户认证</li><li>声明邮箱地址</li><li>书写邮件内容</li><li>退出<code>Telnet</code></li></ul><h3 id="招呼语"><a href="#招呼语" class="headerlink" title="招呼语"></a>招呼语</h3><pre><code class="bash">S 220 163.com Anti-spam GT for Coremail System (163com[20141201])C helo 163.comS 250 OK</code></pre><p><code>220</code>代表服务器已准备好，客户端首先向服务器打一声招呼<code>helo</code>，服务器接收到后返回一个<code>250</code>应答信号，代表打招呼成功，下面可以进行账户的密码认证。</p><h3 id="账户认证"><a href="#账户认证" class="headerlink" title="账户认证"></a>账户认证</h3><pre><code class="bash">C auth loginS 334 dXNlcm5hbWU6C MTgyNjY2MzI4NzJAMTYzLmNvbQ==S 334 UGFzc3dvcmQ6C ****************S 235 Authentication successful</code></pre><p>客户端发送<code>auth login</code>声明自己将进行邮箱账户的用户名认证和密码认证。服务器端返回一个以<code>334</code>代码开头的提示信息，不必追究其语义，而后用户依次输入通过<code>Base64</code>加密的邮箱名和密码，加密网址见后续参考资料。当输入的账号密码无误时，服务端将返回认证成功的提示语。</p><h3 id="声明邮箱地址"><a href="#声明邮箱地址" class="headerlink" title="声明邮箱地址"></a>声明邮箱地址</h3><pre><code class="bash">C mail from:&lt;182****2872@163.com&gt;S 250 Mail OKC rcpt to:&lt;707***098@qq.com&gt;S 250 Mail OK</code></pre><p>账户密码认证结束后，客户端通过<code>mail from:&lt;emailName&gt;</code>以及<code>rcpt to:&lt;nameEmail&gt;</code>声明邮箱的源地址和目的地址。</p><h3 id="书写邮件内容"><a href="#书写邮件内容" class="headerlink" title="书写邮件内容"></a>书写邮件内容</h3><pre><code class="bash">C dataS 354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;C From: 182****2872@163.comC To: 707***098@qq.comC Subject: Test the use of smtpC C   Hello, litreily.                          C   This is a message edit with telnet.C C Best wish to you.C LitreilyC .S 250 Mail OK queued as smtp11,D8CowAAnjlqKlZVZXa4QAQ--.18342S2 1502975693</code></pre><p>写明邮件地址后，使用指令<code>data</code>进入邮件编辑状态，服务器端会提示当输入<code>&lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;</code>时退出编辑状态。</p><p>邮件内容有一定的格式要求，不按要求随意书写有可能被当成垃圾邮件或不合法邮件而退信。通常需要指定邮件主题<code>Subject</code>，其它相关信息包括时间<code>Date</code>，邮件地址<code>From</code>，<code>To</code>等，然后再书写邮件主体内容，更详细的格式可以参考文档<strong>【RFC 821】</strong>。编辑结束后，以单行的点号<code>.</code>结束。如果一切正常，服务器端将返回以<code>250</code>开头的应答信息，并提示邮件以进入<code>smtp</code>的发送队列。此时便可以查收邮件予以验证了。</p><h3 id="退出Telnet"><a href="#退出Telnet" class="headerlink" title="退出Telnet"></a>退出Telnet</h3><pre><code class="bash">C quitS Connection closed by foreign host.</code></pre><p>当邮件发送结束后，客户端通过<code>quit</code>退出<code>Telnet</code>。</p><h2 id="SMTP-Code"><a href="#SMTP-Code" class="headerlink" title="SMTP Code"></a>SMTP Code</h2><ul><li><code>211</code> 系统状态或系统帮助响应</li><li><code>214</code> 帮助信息</li><li><code>220</code> 服务就绪</li><li><code>221</code> 服务关闭传输信道</li><li><code>235</code> 用户验证成功</li><li><code>250</code> 要求的邮件操作完成</li><li><code>251</code> 用户非本地，将转发向</li><li><code>334</code> 等待用户输入验证信息</li><li><code>354</code> 开始邮件输入，以单行<strong>.</strong>结束</li><li><code>421</code> 服务未就绪，关闭传输信道（当必须关闭时，此应答可以作为对任何命令的响应）</li><li><code>450</code> 要求的邮件操作未完成，邮箱不可用（例如，邮箱忙）</li><li><code>451</code> 放弃要求的操作；处理过程中出错</li><li><code>452</code> 系统存储不足，要求的操作未执行</li><li><code>500</code> 格式错误，命令不可识别（此错误也包括命令行过长）</li><li><code>501</code> 参数格式错误</li><li><code>502</code> 命令不可实现</li><li><code>503</code> 错误的命令序列</li><li><code>504</code> 命令参数不可实现</li><li><code>535</code> 用户验证失败</li><li><code>550</code> 要求的邮件操作未完成，邮箱不可用（例如，邮箱未找到，或不可访问）</li><li><code>551</code> 用户非本地，请尝试</li><li><code>552</code> 过量的存储分配，要求的操作未执行</li><li><code>553</code> 邮箱名不可用，要求的操作未执行（例如邮箱格式错误）</li><li><code>554</code> 操作失败</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><code>SMTP</code>文档(<code>RFC 821</code>) : <a href="https://tools.ietf.org/pdf/rfc821.pdf" target="_blank" rel="noopener">https://tools.ietf.org/pdf/rfc821.pdf</a></li><li><code>SMTP Code</code> : <a href="http://bbs.csdn.net/topics/80275246" target="_blank" rel="noopener">http://bbs.csdn.net/topics/80275246</a></li><li><code>163.com</code> 邮件退信代码 : <a href="http://help.163.com/09/1224/17/5RAJ4LMH00753VB8.html" target="_blank" rel="noopener">http://help.163.com/09/1224/17/5RAJ4LMH00753VB8.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      Telnet是远程登录服务的标准协议和主要方式，工作于TCP/IP协议族的应用层，常用于远程登录web服务器，其中便包括SMTP，POP3等邮件服务器。
    
    </summary>
    
      <category term="Linux" scheme="http://www.litreily.top/categories/Linux/"/>
    
    
      <category term="telnet" scheme="http://www.litreily.top/tags/telnet/"/>
    
      <category term="smtp" scheme="http://www.litreily.top/tags/smtp/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下解决wireshark没有权限的问题</title>
    <link href="http://www.litreily.top/2017/07/28/chmod-wireshark/"/>
    <id>http://www.litreily.top/2017/07/28/chmod-wireshark/</id>
    <published>2017-07-28T03:24:49.000Z</published>
    <updated>2018-10-12T13:34:06.409Z</updated>
    
    <content type="html"><![CDATA[<p><code>wireshark</code>是常用的网络抓包工具，在<code>ubuntu</code>中安装方法如下：</p><pre><code class="bash">sudo aptitude install wireshark</code></pre><p>安装以后，打开软件后，在选择网络接口进行抓包时会提示没有权限，为此，可以通过以下方法解决。</p><pre><code class="bash"># 添加用户组，命名为wiresharksudo groupadd wireshark  # 将dumpcap更改为刚添加的用户组sudo chgrp wireshark /usr/bin/dumpcap  # 为wireshark用户组添加使用dumpcap的root权限sudo chmod 4755 /usr/bin/dumpcap# 将自己的用户(本人litreily)添加到wireshark用户组sudo gpasswd -a litreily wireshark  </code></pre><p>执行完成以后便可以使用<code>wireshark</code>正常抓包了。</p>]]></content>
    
    <summary type="html">
    
      wireshark是常用的抓包工具，在Ubuntu下安装后默认会有权限不够的问题，为此可以通过添加用户组并更改dumpcap使用权限的方法予以解决。
    
    </summary>
    
      <category term="Linux" scheme="http://www.litreily.top/categories/Linux/"/>
    
    
      <category term="ubuntu" scheme="http://www.litreily.top/tags/ubuntu/"/>
    
      <category term="wireshark" scheme="http://www.litreily.top/tags/wireshark/"/>
    
  </entry>
  
  <entry>
    <title>C程序与Shell脚本混合编程</title>
    <link href="http://www.litreily.top/2017/07/27/mixwith-shell/"/>
    <id>http://www.litreily.top/2017/07/27/mixwith-shell/</id>
    <published>2017-07-27T01:21:24.000Z</published>
    <updated>2018-10-12T13:34:06.409Z</updated>
    
    <content type="html"><![CDATA[<p>对于嵌入式软件开发，有时候需要在使用<code>C</code>语言进行开发的同时，嵌入<code>shell</code>脚本指令，以完成一些特定的任务。</p><p>本文结合上一篇博文“初探makefile”的程序代码，加上一个简单的<code>shell</code>脚本，说明两者混合编程的方法。</p><h2 id="编写-shell-脚本"><a href="#编写-shell-脚本" class="headerlink" title="编写 shell 脚本"></a>编写 shell 脚本</h2><pre><code class="bash">vim shell.sh</code></pre><p>使用<code>vim</code>编辑器新建<code>shell.sh</code>脚本文件。</p><pre><code class="bash">#!/bin/zshecho &quot;Hello world!&quot;echo &quot;Please input your strings&quot;read input_stringecho &quot;input_string=$input_string&quot;</code></pre><p>脚本首先输出一个“Hello world!”，然后输出一个字符串输入提示语，等待用户输入后，打印出相应的信息。</p><h2 id="在-main-c-中调用-shell-脚本"><a href="#在-main-c-中调用-shell-脚本" class="headerlink" title="在 main.c 中调用 shell 脚本"></a>在 main.c 中调用 shell 脚本</h2><p>为了在<code>C</code>文件中调用<code>shell</code>脚本，可以使用<code>stdlib.h</code>库中的<code>system</code>函数，调用方式如下：</p><pre><code class="c">#include &lt;stdlib.h&gt;...void func(void){    system(&quot;bash ./&lt;script-filename&gt;.sh&quot;);    // bash为默认shell，也可改用诸如&quot;zsh&quot;类的shell}</code></pre><p>根据该用法修改原有的<code>main.c</code></p><pre><code class="c">#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &quot;main.h&quot;#include &quot;add.h&quot;int main(void){    int a = 10;    int b = 1;    int c = add(a,b);    printf(&quot;%d + %d = %d\n&quot;,a,b,c);    system(&quot;zsh ./shell.sh&quot;);     // 调用 shell 脚本    printf(&quot;Infos:File-%s, Func-%s, Line-%d\n&quot;,__FILE__,__func__,__LINE__);    /*    __FILE__ : 文件名    __func__ : 函数名    __LINE__ : 所在行    */    return 0;}</code></pre><h2 id="编译链接并执行"><a href="#编译链接并执行" class="headerlink" title="编译链接并执行"></a>编译链接并执行</h2><pre><code class="bash">$ make clean                # 清除目标文件rm test main.o add.o$ make                      # 生成可执行文件cc    -c -o main.o main.ccc    -c -o add.o add.ccc -o test main.o add.o$ ./test                    # 执行可执行文件10 + 1 = 11                 # 加法运算结果Hello world!                # shell 脚本欢迎语Please input your strings   # 提示语whatever                    # 手动输入的信息  input_string=whatever       # 显示信息Infos:File-main.c, Func-main, Line-14   # 显示打印程序所在文件、函数及对应行数</code></pre><p><strong>说明</strong>：当使用<code>printf</code>函数时，如果不在输出信息后添加<code>\n</code>换行符的话，其输出信息有可能与<code>shell</code>脚本执行结果发生错位。为避免该情况的发生，应习惯在<code>printf</code>函数调用过程中加入换行符。</p><h2 id="显示脚本指令"><a href="#显示脚本指令" class="headerlink" title="显示脚本指令"></a>显示脚本指令</h2><p>为了追踪每个脚本指令的执行过程，可以修改<code>system</code>函数调用方式，在<code>bash</code>,<code>zsh</code>后添加选项<code>-x</code>.</p><pre><code class="c">system(&quot;zsh -x ./shell.sh&quot;);</code></pre><p>修改后重新执行make，执行<code>./test</code>得到以下结果</p><pre><code class="zsh">$ ./test10 + 1 = 11+/etc/zsh/zshenv:15&gt; [[ -z /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin || /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin == /bin:/usr/bin ]]+./shell.sh:3&gt; echo &#39;Hello world!&#39;Hello world!+./shell.sh:4&gt; echo &#39;Please input your strings&#39;Please input your strings+./shell.sh:5&gt; read input_stringwhat+./shell.sh:6&gt; echo &#39;input_string=what&#39;input_string=whatInfos:File-main.c, Func-main, Line-14</code></pre><p>使用不同的<code>shell</code>，其显示效果不大一致，下面是使用<code>bash</code>执行后的效果。</p><pre><code class="bash">$ ./test10 + 1 = 11+ echo &#39;Hello world!&#39;Hello world!+ echo &#39;Please input your strings&#39;Please input your strings+ read input_stringwhat+ echo input_string=whatinput_string=whatInfos:File-main.c, Func-main, Line-14</code></pre><p><strong>说明</strong>：上述执行过程中显示的空行是为了清楚显示执行过程手动添加的，实际执行过程中并无空行。</p>]]></content>
    
    <summary type="html">
    
      简单的C语言与shell脚本混合使用方法介绍
    
    </summary>
    
      <category term="Linux" scheme="http://www.litreily.top/categories/Linux/"/>
    
    
      <category term="C/C++" scheme="http://www.litreily.top/tags/C-C/"/>
    
      <category term="ubuntu" scheme="http://www.litreily.top/tags/ubuntu/"/>
    
      <category term="makefile" scheme="http://www.litreily.top/tags/makefile/"/>
    
      <category term="shell" scheme="http://www.litreily.top/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>初探makefile</title>
    <link href="http://www.litreily.top/2017/07/25/makefile/"/>
    <id>http://www.litreily.top/2017/07/25/makefile/</id>
    <published>2017-07-25T05:38:00.000Z</published>
    <updated>2018-10-12T13:34:06.409Z</updated>
    
    <content type="html"><![CDATA[<p>嵌入式软件开发少不了使用makefile进行软件编译，写好一个makefile，让其完成所有程序代码的自动化编译链接，可以提高软件开发效率。<code>make</code>是一个命令工具，用于解释并执行makefile中的指令。大多数IDE都有这个命令工具，如<code>Visual C++</code>的<code>nmake</code>，<code>Linux</code>中<code>GNU</code>的<code>make</code>。本文以一个简单例子说明<code>makefile</code>文件的书写规范和执行过程。</p><h2 id="创建待编译的文件"><a href="#创建待编译的文件" class="headerlink" title="创建待编译的文件"></a>创建待编译的文件</h2><pre><code class="bash">mkdir mkfilesvim add.h add.c main.h main.c</code></pre><p>然后逐个编辑头文件和源文件，实现一个简单的加法运算。</p><h3 id="add-h"><a href="#add-h" class="headerlink" title="add.h"></a>add.h</h3><p>加法函数放于<code>add.c</code>中，对应头文件为<code>add.h</code>，用于存放函数声明。</p><pre><code class="c">#ifndef __ADD_H_#define __ADD_H_// declaration functionsint add(int a,int b);#endif // __ADD_H_</code></pre><h3 id="add-c"><a href="#add-c" class="headerlink" title="add.c"></a>add.c</h3><p>在<code>add.c</code>中实现加法函数。</p><pre><code class="c">#include &quot;add.h&quot;int add(int a, int b){  return a+b;}</code></pre><h3 id="main-h"><a href="#main-h" class="headerlink" title="main.h"></a>main.h</h3><p>在主函数对应的头文件<code>main.h</code>中添加依赖的头文件。</p><pre><code class="c">#ifndef __MAIN_H_#define __MAIN_H_#include &lt;math.h&gt;#endif // __MAIN_H_</code></pre><h3 id="main-c"><a href="#main-c" class="headerlink" title="main.c"></a>main.c</h3><p>在<code>main.c</code>中实现主函数，打印出一个加法运算结果。</p><pre><code class="c">#include &lt;stdio.h&gt;#include &quot;main.h&quot;#include &quot;add.h&quot;int main(void){  int a = 10;  int b = 1;  int c = add(a,b);  printf(&quot;%d + %d = %d\n&quot;,a,b,c);  return 0;}</code></pre><h2 id="创建-makefile"><a href="#创建-makefile" class="headerlink" title="创建 makefile"></a>创建 makefile</h2><h3 id="makefile-规则"><a href="#makefile-规则" class="headerlink" title="makefile 规则"></a>makefile 规则</h3><pre><code class="bash">target ... : prerequisites ...&lt;tab&gt;   command......</code></pre><p><code>target</code>: 目标文件，可以是.o（object file）文件，也可以是最终的可执行文件<br><code>prerequisites</code> : 生成目标文件所需的文件或是目标（<em>.h, </em>.c, *.o, …）<br><code>command</code>: <code>make</code>需要执行的编译、链接等指令（所有指令前都以<code>TAB</code>键开头）</p><h3 id="makefile-基础写法"><a href="#makefile-基础写法" class="headerlink" title="makefile 基础写法"></a>makefile 基础写法</h3><pre><code class="makefile">test : main.o add.o    cc -o test main.o add.omain.o : main.c main.h add.h    cc -c main.cadd.o : add.c add.h    cc -c add.c# make clean : use to clean all the object files.PHONY : cleanclean :    rm test main.o add.o</code></pre><p>其中，test, main.o, add.o均是目标文件<code>target</code>；所有源文件及头文件均为<code>prerequisites</code>；</p><p><code>cc</code> : 编译器，与<code>gcc</code>相似，<code>-c</code>代表编译，执行后产生对象文件，即”.o”文件；<code>-o</code>代表链接，用于链接”.o”文件并生成可执行文件；</p><p><code>.PHONY</code> : 用于声明后面跟着的都是伪目标（类似于C语言中的标签(Label)）；</p><p><code>clean</code> ： 一个伪目标，只有在外部执行<code>make clean</code>时方才执行相应的执行，此次用于删除生成的所有目标文件。</p><h3 id="makefile-简化版1"><a href="#makefile-简化版1" class="headerlink" title="makefile 简化版1"></a>makefile 简化版1</h3><p>在<code>makefile</code>中可以通过<code>$</code>符号引用变量，简化文件。变量定义以等号连接，类似于宏定义，将需要多处使用的字符串存入变量中可以方便修改和管理，如本例，可定义以下变量：</p><pre><code class="makefile">objs = main.o add.o  # 引用方法 $(objs)</code></pre><p>简化后的<code>makefile</code>如下：</p><pre><code class="makefile">objs = main.o add.otest : $(objs)    cc -o test $(objs)main.o : main.c main.h add.h    cc -c main.cadd.o : add.c add.h    cc -c add.c# make clean : use to clean all the object files.PHONY : cleanclean :    rm test (objs)</code></pre><p>当项目文件很多时，合理使用变量可以大大简化<code>makefile</code>的编写和修改。</p><h3 id="makefile-简化版2"><a href="#makefile-简化版2" class="headerlink" title="makefile 简化版2"></a>makefile 简化版2</h3><p>要知道，<code>make</code>非常强大，具有<strong>自动推导</strong>的功能，可以自动推导”.o”目标文件下面的指令。如果目标文件为”main.o”，那么指令必然为”cc -c main.c”，依赖于<code>make</code>指令的推导功能，<code>makefile</code>可以省略该指令的编写。</p><p>此外，每个”.o”目标文件的依赖文件必然包含一个文件名相同的”.c”文件，该文件可以通过自动推导得出，所以在<code>makefile</code>中同样可以省略。</p><p>根据以上规则可以对<code>makefile</code>进一步简化：</p><pre><code class="makefile">objs = main.o add.otest : $(objs)    cc -o test $(objs)main.o : main.h add.hadd.o : add.h# make clean : use to clean all the object files.PHONY : cleanclean :    rm test (objs)</code></pre><h2 id="make"><a href="#make" class="headerlink" title="make"></a>make</h2><p>编写好<code>makefile</code>文件后，便可以使用<code>make</code>指令进行编译链接并生成可执行文件了。</p><pre><code class="bash">$ make  # 生成可执行文件cc -c main.ccc -c add.ccc -o test main.o add.o$ lsadd.c  add.h  add.o  main.c  main.h  main.o  makefile  test$ ./test  # 执行可执行文件10 + 1 = 11$ make clean  # 清除所有目标文件rm test main.o add.o$ lsadd.c  add.h  main.c  main.h  makefile</code></pre><p>至此，一个简单的<code>makefile</code>编写过程及执行过程便结束了。</p>]]></content>
    
    <summary type="html">
    
      嵌入式开发少不了使用makefile进行软件编译，写好一个makefile，让其完成所有程序代码的自动化编译，可以提高软件开发效率。
    
    </summary>
    
      <category term="Linux" scheme="http://www.litreily.top/categories/Linux/"/>
    
    
      <category term="C/C++" scheme="http://www.litreily.top/tags/C-C/"/>
    
      <category term="ubuntu" scheme="http://www.litreily.top/tags/ubuntu/"/>
    
      <category term="makefile" scheme="http://www.litreily.top/tags/makefile/"/>
    
  </entry>
  
  <entry>
    <title>安装Ubuntu后的系统精简与美化</title>
    <link href="http://www.litreily.top/2017/06/11/initial-ubuntu/"/>
    <id>http://www.litreily.top/2017/06/11/initial-ubuntu/</id>
    <published>2017-06-11T08:26:39.000Z</published>
    <updated>2018-10-12T13:34:06.409Z</updated>
    
    <content type="html"><![CDATA[<p>安装好Ubuntu系统后，通常需要卸载许多用不上的软件，并安装一些常用以及实用的软件。当然，Ubuntu默认的主题实在不敢恭维，为此有必要换一套主题予以美化。</p><h2 id="系统精简"><a href="#系统精简" class="headerlink" title="系统精简"></a>系统精简</h2><p>删除不必要的软件</p><h3 id="删除libreoffice"><a href="#删除libreoffice" class="headerlink" title="删除libreoffice"></a>删除libreoffice</h3><pre><code class="bash">sudo apt-get remove libreoffice-common</code></pre><h3 id="删除Amazon链接"><a href="#删除Amazon链接" class="headerlink" title="删除Amazon链接"></a>删除Amazon链接</h3><pre><code class="bash">sudo apt-get remove unity-webapps-common</code></pre><h3 id="删除其它极少用的软件"><a href="#删除其它极少用的软件" class="headerlink" title="删除其它极少用的软件"></a>删除其它极少用的软件</h3><pre><code class="bash">sudo apt-get remove thunderbird totem rhythmbox empathy brasero simple-scan gnome-mahjongg aisleriot gnome-mines cheese transmission-common gnome-orca webbrowser-app gnome-sudokusudo apt-get remove onboard deja-dup</code></pre><h2 id="常用软件安装"><a href="#常用软件安装" class="headerlink" title="常用软件安装"></a>常用软件安装</h2><h3 id="常用工具"><a href="#常用工具" class="headerlink" title="常用工具"></a>常用工具</h3><pre><code class="bash">sudo apt-get install vim    # Vimsudo apt-get install git    # Git</code></pre><p>安装<code>Git</code>之后需要配置以下全局变量，并生成<code>ssh-key</code></p><pre><code class="bash">git config --global user.email &quot;emailname@email.com&quot;git config --global user.name &quot;username&quot;ssh-keygen -t rsa -C &quot;emailname@email.com&quot;cd ~/.sshgedit id_rsa.pub    # 将文件内容添加至github或其它代码托管平台</code></pre><h3 id="系统指示器Syspeek"><a href="#系统指示器Syspeek" class="headerlink" title="系统指示器Syspeek"></a>系统指示器Syspeek</h3><pre><code class="bash">sudo add-apt-repository ppa:nilarimogard/webupd8    sudo apt-get update    sudo apt-get install syspeek  </code></pre><h3 id="Hexo-amp-amp-Gitbook"><a href="#Hexo-amp-amp-Gitbook" class="headerlink" title="Hexo &amp;&amp; Gitbook"></a>Hexo &amp;&amp; Gitbook</h3><p>首先安装<code>node.js</code>与<code>npm</code>,然后使用<code>npm</code>安装<code>hexo</code>与<code>gitbook</code></p><pre><code class="bash">npm install hexo-cli -gnpm install -g gitbook-cligitbook -V</code></pre><h3 id="必备软件"><a href="#必备软件" class="headerlink" title="必备软件"></a>必备软件</h3><ul><li>汉字输入法： 搜狗拼音</li><li>音乐播放器： 网易云音乐</li></ul><pre><code class="bash">sudo dpkg -i netease-cloud-music_1.0.0_amd64_ubuntu16.04.deb   # 下载安装包后执行该指令sudo apt-get -f install</code></pre><ul><li>视频播放器： VLC（自带software 管理软件中下载安装）</li><li>网络浏览器： Google Chrome</li></ul><pre><code class="bash">sudo dpkg -i google-chrome-stable_current_amd64.deb   # 下载安装包后执行该指令sudo apt-get -f install</code></pre><ul><li>截图软件： Shutter</li><li>社交软件： <a href="https://github.com/geeeeeeeeek/electronic-wechat.git" target="_blank" rel="noopener">微信</a></li><li>办公软件： WPS</li></ul><h2 id="系统美化"><a href="#系统美化" class="headerlink" title="系统美化"></a>系统美化</h2><p>参考资料：<a href="http://www.jcodecraeer.com/plus/view.php?aid=3502" target="_blank" rel="noopener">http://www.jcodecraeer.com/plus/view.php?aid=3502</a></p><h3 id="安装Flatabulous主题"><a href="#安装Flatabulous主题" class="headerlink" title="安装Flatabulous主题"></a>安装Flatabulous主题</h3><ul><li>首先安装 Unity-tweak-tool软件（从Ubuntu软件商店可以找到）</li><li>然后在<code>~</code>目录下创建系统主题文件夹<code>.themes</code></li></ul><pre><code class="bash">mkdir ~/.themes</code></pre><ul><li>在<a href="https://github.com/anmoljagetia/Flatabulous/archive/master.zip" target="_blank" rel="noopener">https://github.com/anmoljagetia/Flatabulous/archive/master.zip</a> 下载 Flatabulous 主题</li><li>解压主题后移动到<code>~/.themes</code>文件夹下</li><li>打开tweak软件，在theme面板中选择Flatabulous主题</li></ul><h3 id="安装ultra-flat-icons图标主题"><a href="#安装ultra-flat-icons图标主题" class="headerlink" title="安装ultra-flat-icons图标主题"></a>安装ultra-flat-icons图标主题</h3><ul><li>安装图标主题</li></ul><pre><code class="bash">sudo add-apt-repository ppa:noobslab/icons  sudo apt-get update  sudo apt-get install ultra-flat-icons</code></pre><ul><li>在tweak软件中的icon面板中选择Ultra-flat</li></ul>]]></content>
    
    <summary type="html">
    
      安装好Ubuntu系统后，通常需要卸载许多用不上的软件，并安装一些常用以及实用的软件。当然，Ubuntu默认的主题实在不敢恭维，为此有必要换一套主题予以美化。
    
    </summary>
    
      <category term="Linux" scheme="http://www.litreily.top/categories/Linux/"/>
    
    
      <category term="ubuntu" scheme="http://www.litreily.top/tags/ubuntu/"/>
    
  </entry>
  
</feed>
